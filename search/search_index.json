{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to rompy's documentation!","text":"<p>Taking the pain out of ocean model setup</p> <p>This library is in early prototype stage and the interfaces are likely to change</p> <p>This library takes an opinionated approach to combining the functionality of the cookie-cutter library (https://github.com/cookiecutter/cookiecutter) with the XArray ecosystem (http://xarray.pydata.org/en/stable/related-projects.html) and intake (https://github.com/intake/intake) for data to aid in the configuration and evaluation of coastal scale numerical models.</p> <p>There are two base classes BaseModel and BaseGrid. BaseModel implements the cookie-cutter code and model configuration packaging. BaseGrid defines a loose definition of the grid as two arrays of x, y points that establish the models geographic extents, bounding box and convex hull.</p> <p>At present only one example model has been implemented - the SwanModel (http://swanmodel.sourceforge.net/). An example cookie-cutter template for swan is provided in the <code>rompy/templates</code> folder.</p> <p>A model implementation will generally consist of the following components:</p> <ol> <li>A model class that inherits from BaseModel and implements the minimal interface. At present only a private <code>_get_grid()</code> method.</li> <li>A grid class that inherits from BaseGrid and implements the minimal interface of either loading the grid from file or a model specific grid specification string</li> <li>An XArray accessor that has methods that translate an XArray dataset into a model specific input file format (usually some bespoke text file format). This allows convenient namespacing of methods from an XArray dataset e.g.:</li> </ol> <pre><code>ds.swan.to_inpgrid(filename)\n</code></pre> <p>The final main component of the library is an intake driver that builds on the intake-xarray.DataSourceMixin and allows for the stacking of multiple model forecast datasets that are typically published in netCDF format on THREDDS/OpenDAP servers. The unique feature of the driver include:</p> <ol> <li>The ability to use format strings in the urlpath and pass a dictionary of values for the format keys. The product of the dictionary values is expanded to a set of URLs that are scanned checked for existence using the fsspec library. This allows for scanning of both local filesystems and http servers in a targetted fashion, for example a specific date range of interest.</li> <li>The subset of urls identified are opened with XArray with a preprocessing function that takes a dictionary of filters for common operations that are applied during pre-processing - allowing this to be parameterised in the intake catalog yaml entry for a specific dataset.</li> <li>The result is either a stack of model forcasts normalised to an initialisation and lead time (hindcast=false), or a pseudo-reanalysis that selects the shortest lead-time for each time point in the stack.</li> </ol>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Home</li> <li>Quickstart</li> <li>Core Concepts</li> <li>Formatting and Logging</li> <li>CLI</li> <li>Backends</li> <li>Backend Reference</li> <li>Models</li> <li>Demo</li> <li>API</li> <li>Developer</li> </ul>"},{"location":"#indices-and-tables","title":"Indices and tables","text":"<ul> <li>General Index</li> <li>Module Index</li> <li>Search</li> </ul>"},{"location":"api/","title":"API","text":""},{"location":"api/#rompy","title":"rompy","text":""},{"location":"api/#rompy-attributes","title":"Attributes","text":""},{"location":"api/#rompy.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = get_logger(__name__)\n</code></pre>"},{"location":"api/#rompy.ROOT_DIR","title":"ROOT_DIR  <code>module-attribute</code>","text":"<pre><code>ROOT_DIR = resolve()\n</code></pre>"},{"location":"api/#rompy.TEMPLATES_DIR","title":"TEMPLATES_DIR  <code>module-attribute</code>","text":"<pre><code>TEMPLATES_DIR = ROOT_DIR / 'templates'\n</code></pre>"},{"location":"api/#rompy-classes","title":"Classes","text":""},{"location":"api/#rompy-functions","title":"Functions","text":""},{"location":"backends/","title":"Backend Systems","text":"<p>ROMPY's backend system provides flexible, type-safe execution environments for wave models. Whether you're running simple local simulations or complex containerized workflows, the backend system handles execution, resource management, and result processing.</p>"},{"location":"backends/#overview","title":"Overview","text":"<p>Backend systems in ROMPY control how and where your models execute. They provide:</p> <ul> <li>Execution Environments: Local system, Docker containers, HPC clusters</li> <li>Resource Management: CPU, memory, and timeout controls</li> <li>Type Safety: Pydantic-based configurations with validation</li> <li>Reproducibility: Consistent execution across different environments</li> <li>Scalability: From development to production deployments</li> </ul>"},{"location":"backends/#key-components","title":"Key Components","text":"<p>Backend Configurations : Type-safe Pydantic models that define execution parameters. See <code>rompy.backends.config.BaseBackendConfig</code> and its subclasses.</p> <p>Run Backends : Execution engines that handle the actual model runs. See <code>rompy.run</code> for implementations.</p> <p>Postprocessors : Components that handle results after model execution. See <code>rompy.backends.postprocessors</code> for available processors.</p> <p>CLI Integration : Command-line tools for configuration management and execution. See cli for details.</p>"},{"location":"backends/#quick-start","title":"Quick Start","text":"<p>Get started with backends in three simple steps:</p> <ol> <li>Create a Backend Configuration</li> </ol> <pre><code># my_backend.yml\ntype: local\ntimeout: 3600\ncommand: \"python run_model.py\"\nenv_vars:\n  OMP_NUM_THREADS: \"4\"\n</code></pre> <ol> <li>Validate Your Configuration</li> </ol> <pre><code>rompy backends validate my_backend.yml\n</code></pre> <ol> <li>Run Your Model</li> </ol> <pre><code>from rompy.model import ModelRun\nfrom rompy.backends import LocalConfig\n\n# Load and run\nmodel = ModelRun.from_file(\"model_config.yml\")\nconfig = LocalConfig.from_file(\"my_backend.yml\")\nsuccess = model.run(backend=config)\n</code></pre>"},{"location":"backends/#configuration-types","title":"Configuration Types","text":"<p>All backend configurations inherit from <code>rompy.backends.config.BaseBackendConfig</code> and provide type-safe, validated execution parameters.</p>"},{"location":"backends/#localconfig-local-system-execution","title":"LocalConfig - Local System Execution","text":"<p>Execute models directly on your local system using <code>rompy.backends.config.LocalConfig</code>.</p> <p>Basic Usage:</p> <pre><code>from rompy.backends import LocalConfig\n\nconfig = LocalConfig(\n    timeout=3600,\n    command=\"python run_simulation.py\"\n)\n</code></pre> <p>Advanced Configuration:</p> <pre><code># local_advanced.yml\ntype: local\ntimeout: 7200  # 2 hours\ncommand: \"python run_simulation.py --verbose\"\nshell: true\ncapture_output: true\nenv_vars:\n  OMP_NUM_THREADS: \"8\"\n  PYTHONPATH: \"/custom/path\"\n  MODEL_DEBUG: \"true\"\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>timeout</code>: Maximum execution time in seconds (60-86400)</li> <li><code>command</code>: Shell command to execute (optional)</li> <li><code>shell</code>: Execute through shell (default: True)</li> <li><code>capture_output</code>: Capture stdout/stderr (default: True)</li> <li><code>env_vars</code>: Environment variables to set</li> <li><code>working_dir</code>: Working directory for execution</li> </ul> <p>For complete parameter documentation, see <code>rompy.backends.config.LocalConfig</code>.</p>"},{"location":"backends/#dockerconfig-container-execution","title":"DockerConfig - Container Execution","text":"<p>Execute models inside Docker containers for reproducible, isolated environments using <code>rompy.backends.config.DockerConfig</code>.</p> <p>Basic Usage:</p> <pre><code>from rompy.backends import DockerConfig\n\nconfig = DockerConfig(\n    image=\"swan:latest\",\n    cpu=4,\n    memory=\"2g\",\n    timeout=3600\n)\n</code></pre> <p>Advanced Configuration:</p> <pre><code># docker_advanced.yml\ntype: docker\nimage: \"swan:latest\"\ntimeout: 10800  # 3 hours\ncpu: 8\nmemory: \"4g\"\nmpiexec: \"mpirun -np 8\"\nuser: \"modeluser\"\nvolumes:\n  - \"/data/input:/app/input:ro\"\n  - \"/data/output:/app/output:rw\"\nenv_vars:\n  MODEL_THREADS: \"8\"\n  DATA_DIR: \"/app/input\"\n</code></pre> <p>Building from Dockerfile:</p> <pre><code># docker_build.yml\ntype: docker\ndockerfile: \"./docker/Dockerfile\"\nbuild_context: \"./docker\"\nbuild_args:\n  MODEL_VERSION: \"2.1.0\"\ncpu: 4\nmemory: \"2g\"\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>image</code>: Docker image to use (mutually exclusive with dockerfile)</li> <li><code>dockerfile</code>: Path to Dockerfile to build (mutually exclusive with image)</li> <li><code>cpu</code>: Number of CPU cores (1-128)</li> <li><code>memory</code>: Memory limit (e.g., \"2g\", \"512m\")</li> <li><code>timeout</code>: Maximum execution time in seconds</li> <li><code>volumes</code>: Volume mounts in \"host:container:mode\" format</li> <li><code>env_vars</code>: Environment variables to set</li> <li><code>executable</code>: Path to executable inside container</li> <li><code>mpiexec</code>: MPI execution command for parallel runs</li> </ul> <p>For complete parameter documentation, see <code>rompy.backends.config.DockerConfig</code>.</p>"},{"location":"backends/#using-backend-configurations","title":"Using Backend Configurations","text":""},{"location":"backends/#with-modelrun","title":"With ModelRun","text":"<p>Backend configurations integrate directly with ROMPY's model execution via <code>rompy.model.ModelRun.run</code>:</p> <pre><code>from rompy.model import ModelRun\nfrom rompy.backends import LocalConfig, DockerConfig\n\n# Load your model\nmodel_run = ModelRun.from_file(\"model_config.yml\")\n\n# Execute locally\nlocal_config = LocalConfig(timeout=3600)\nsuccess = model_run.run(backend=local_config)\n\n# Execute in Docker\ndocker_config = DockerConfig(\n    image=\"rompy/swan:latest\",\n    cpu=4,\n    memory=\"4g\"\n)\nsuccess = model_run.run(backend=docker_config)\n</code></pre>"},{"location":"backends/#from-configuration-files","title":"From Configuration Files","text":"<p>Load configurations from YAML or JSON files:</p> <pre><code>import yaml\nfrom rompy.backends import LocalConfig, DockerConfig\n\n# Load configuration\nwith open(\"backend_config.yml\") as f:\n    config_data = yaml.safe_load(f)\n    backend_type = config_data.pop(\"type\")\n\n    if backend_type == \"local\":\n        config = LocalConfig(**config_data)\n    elif backend_type == \"docker\":\n        config = DockerConfig(**config_data)\n\n# Use configuration\nsuccess = model_run.run(backend=config)\n</code></pre>"},{"location":"backends/#command-line-interface","title":"Command Line Interface","text":"<p>The CLI provides comprehensive backend management capabilities. See cli for complete details.</p>"},{"location":"backends/#configuration-management","title":"Configuration Management","text":"<pre><code># Validate configuration\nrompy backends validate my_config.yml\n\n# List available backends\nrompy backends list\n\n# Show configuration schema\nrompy backends schema --backend-type docker\n\n# Create configuration template\nrompy backends create --backend-type local --output template.yml\n</code></pre>"},{"location":"backends/#model-execution","title":"Model Execution","text":"<pre><code># Run model with backend configuration\nrompy run model_config.yml --backend-config my_backend.yml\n\n# Run pipeline with configuration\nrompy pipeline --config pipeline_config.yml\n</code></pre>"},{"location":"backends/#configuration-examples","title":"Configuration Examples","text":""},{"location":"backends/#environment-specific-configurations","title":"Environment-Specific Configurations","text":"<p>Development Environment:</p> <pre><code># dev_backend.yml\ntype: local\ntimeout: 1800  # 30 minutes\ncommand: \"python run_model.py --debug\"\nenv_vars:\n  ENV: \"development\"\n  LOG_LEVEL: \"DEBUG\"\n  PYTHONUNBUFFERED: \"1\"\n</code></pre> <p>Production Environment:</p> <pre><code># prod_backend.yml\ntype: docker\nimage: \"mymodel:production\"\ntimeout: 14400  # 4 hours\ncpu: 16\nmemory: \"32g\"\nmpiexec: \"mpirun -np 16\"\nvolumes:\n  - \"/data/production:/app/data:ro\"\n  - \"/results/production:/app/results:rw\"\nenv_vars:\n  ENV: \"production\"\n  LOG_LEVEL: \"INFO\"\n</code></pre>"},{"location":"backends/#resource-based-configurations","title":"Resource-Based Configurations","text":"<p>Small Models:</p> <pre><code>type: local\ntimeout: 3600\nenv_vars:\n  OMP_NUM_THREADS: \"4\"\n</code></pre> <p>Large Models:</p> <pre><code>type: docker\nimage: \"swan:hpc\"\ntimeout: 86400  # 24 hours\ncpu: 32\nmemory: \"64g\"\nmpiexec: \"mpirun -np 32\"\nenv_vars:\n  OMP_NUM_THREADS: \"1\"\n  MODEL_PRECISION: \"double\"\n</code></pre>"},{"location":"backends/#validation-and-error-handling","title":"Validation and Error Handling","text":""},{"location":"backends/#type-safety","title":"Type Safety","text":"<p>Pydantic provides comprehensive validation with clear error messages:</p> <pre><code>from rompy.backends import LocalConfig, DockerConfig\nfrom pydantic import ValidationError\n\ntry:\n    # Invalid timeout (too short)\n    config = LocalConfig(timeout=30)\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n\ntry:\n    # Missing required image/dockerfile\n    config = DockerConfig()\nexcept ValidationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"backends/#configuration-validation","title":"Configuration Validation","text":"<p>Each configuration class validates fields according to execution environment requirements:</p> <p>LocalConfig Validation: * Working directory must exist if specified * Environment variables must be string key-value pairs * Timeout must be between 60 and 86400 seconds</p> <p>DockerConfig Validation: * Either <code>image</code> or <code>dockerfile</code> must be provided (not both) * CPU count must be between 1 and 128 * Memory format must match pattern (e.g., \"2g\", \"512m\") * Volume mounts must use \"host:container:mode\" format * Docker image names must follow valid naming conventions</p>"},{"location":"backends/#best-practices","title":"Best Practices","text":""},{"location":"backends/#configuration-management_1","title":"Configuration Management","text":"<ol> <li> <p>Version Control: Keep configuration files in version control</p> </li> <li> <p>Environment Variables: Use environment variables for sensitive data:</p> </li> </ol> <pre><code>config = LocalConfig(\n    env_vars={\"API_KEY\": os.environ[\"API_KEY\"]}\n)\n</code></pre> <ol> <li>Validation: Always validate configurations before production use:</li> </ol> <pre><code>rompy backends validate my_config.yml\n</code></pre> <ol> <li>Documentation: Document your configurations with comments:</li> </ol> <pre><code># Production SWAN model configuration\ntype: docker\nimage: \"swan:2.1.0\"  # Pin specific version\ntimeout: 14400  # 4 hours for typical runs\ncpu: 16  # Match server capabilities\n</code></pre>"},{"location":"backends/#resource-planning","title":"Resource Planning","text":"<ol> <li>Start Small: Begin with conservative resource allocations</li> <li>Monitor Usage: Track actual resource consumption</li> <li>Scale Gradually: Increase resources based on measured needs</li> <li>Set Realistic Timeouts: Base timeouts on model complexity</li> </ol>"},{"location":"backends/#security-considerations","title":"Security Considerations","text":"<ol> <li>Container Security: Use appropriate user permissions:</li> </ol> <pre><code>config = DockerConfig(\n    image=\"myapp:latest\",\n    user=\"appuser\",  # Don't run as root\n    volumes=[\"/data:/app/data:ro\"]  # Read-only when possible\n)\n</code></pre> <ol> <li> <p>Environment Variables: Never hardcode sensitive data in configuration files</p> </li> <li> <p>Volume Mounts: Use read-only mounts when possible</p> </li> </ol>"},{"location":"backends/#troubleshooting","title":"Troubleshooting","text":""},{"location":"backends/#common-issues","title":"Common Issues","text":"<p>Configuration Validation Errors</p> <pre><code># Check configuration syntax\nrompy backends validate my_config.yml\n\n# Show configuration schema\nrompy backends schema --backend-type local\n</code></pre> <p>Docker Issues</p> <pre><code># Verify Docker image exists\ndocker images | grep myimage\n\n# Test Docker configuration\nrompy backends validate --backend-type docker docker_config.yml\n</code></pre> <p>Timeout Issues</p> <pre><code># Increase timeout for long-running models\ntype: local\ntimeout: 21600  # 6 hours\n</code></pre> <p>Memory Issues</p> <pre><code># Increase memory allocation\ntype: docker\nimage: \"swan:latest\"\nmemory: \"8g\"\n</code></pre>"},{"location":"backends/#debugging-configuration-issues","title":"Debugging Configuration Issues","text":"<ol> <li>Check Validation Errors:</li> </ol> <pre><code>from pydantic import ValidationError\n\ntry:\n    config = DockerConfig(**config_data)\nexcept ValidationError as e:\n    for error in e.errors():\n        print(f\"Field {error['loc']}: {error['msg']}\")\n</code></pre> <ol> <li>Verify Configuration Serialization:</li> </ol> <pre><code>config = LocalConfig(timeout=3600)\nprint(config.model_dump_json(indent=2))\n</code></pre> <ol> <li>Test with Simple Examples:</li> </ol> <pre><code># Start with minimal configuration\nconfig = LocalConfig(timeout=3600)\n# Add complexity gradually\n</code></pre>"},{"location":"backends/#getting-help","title":"Getting Help","text":"<pre><code># General help\nrompy backends --help\n\n# Command-specific help\nrompy backends validate --help\n\n# Show examples\nrompy backends create --backend-type local --with-examples\n</code></pre>"},{"location":"backends/#advanced-usage","title":"Advanced Usage","text":""},{"location":"backends/#custom-backend-configurations","title":"Custom Backend Configurations","text":"<p>Extend the system with custom backend types by inheriting from <code>rompy.backends.config.BaseBackendConfig</code>:</p> <pre><code>from rompy.backends import BaseBackendConfig\nfrom pydantic import Field\n\nclass HPCConfig(BaseBackendConfig):\n    \"\"\"Configuration for HPC cluster execution.\"\"\"\n\n    queue: str = Field(..., description=\"SLURM queue name\")\n    nodes: int = Field(1, ge=1, le=100)\n    partition: str = Field(\"compute\")\n\n    def get_backend_class(self):\n        from mypackage.backends import HPCRunBackend\n        return HPCRunBackend\n</code></pre> <p>For detailed implementation guidance, see backend_reference.</p>"},{"location":"backends/#postprocessors","title":"Postprocessors","text":"<p>Handle results after model execution using postprocessor classes:</p> <pre><code># Basic postprocessing\nresults = model_run.postprocess(processor=\"archive\")\n\n# Custom postprocessing\nresults = model_run.postprocess(\n    processor=\"custom_analyzer\",\n    output_format=\"netcdf\",\n    compress=True\n)\n</code></pre> <p>For available postprocessors, see <code>rompy.backends.postprocessors</code>.</p>"},{"location":"backends/#schema-generation","title":"Schema Generation","text":"<p>Generate configuration schemas for external tools:</p> <pre><code>from rompy.backends import LocalConfig, DockerConfig\nimport json\n\n# Generate JSON schema\nlocal_schema = LocalConfig.model_json_schema()\ndocker_schema = DockerConfig.model_json_schema()\n\n# Save schema for external validation\nwith open(\"local_schema.json\", \"w\") as f:\n    json.dump(local_schema, f, indent=2)\n</code></pre>"},{"location":"backends/#integration-examples","title":"Integration Examples","text":""},{"location":"backends/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>from rompy.model import ModelRun\nfrom rompy.backends import DockerConfig\n\n# Load model\nmodel = ModelRun.from_file(\"swan_model.yml\")\n\n# Configure backend\nbackend = DockerConfig(\n    image=\"swan:latest\",\n    cpu=8,\n    memory=\"8g\",\n    timeout=7200,\n    volumes=[\n        \"/data/bathymetry:/app/bathy:ro\",\n        \"/data/forcing:/app/forcing:ro\",\n        \"/results:/app/results:rw\"\n    ]\n)\n\n# Execute\nsuccess = model.run(backend=backend)\n\nif success:\n    print(\"Model execution completed successfully\")\nelse:\n    print(\"Model execution failed\")\n</code></pre>"},{"location":"backends/#pipeline-integration","title":"Pipeline Integration","text":"<pre><code>from rompy.pipeline import Pipeline\nfrom rompy.backends import LocalConfig, DockerConfig\n\n# Create pipeline with different backends for different stages\npipeline = Pipeline([\n    {\n        \"name\": \"preprocessing\",\n        \"backend\": LocalConfig(timeout=1800),\n        \"command\": \"python preprocess.py\"\n    },\n    {\n        \"name\": \"simulation\",\n        \"backend\": DockerConfig(\n            image=\"swan:latest\",\n            cpu=16,\n            memory=\"32g\",\n            timeout=14400\n        )\n    },\n    {\n        \"name\": \"postprocessing\",\n        \"backend\": LocalConfig(timeout=3600),\n        \"command\": \"python postprocess.py\"\n    }\n])\n\n# Execute pipeline\nresults = pipeline.run()\n</code></pre>"},{"location":"backends/#api-reference","title":"API Reference","text":"<p>For complete API documentation, see:</p> <ul> <li><code>rompy.backends.config.BaseBackendConfig</code> - Base configuration class</li> <li><code>rompy.backends.config.LocalConfig</code> - Local execution configuration</li> <li><code>rompy.backends.config.DockeConfig</code> - Docker execution configuration</li> <li><code>rompy.run</code> - Run backend implementations</li> <li><code>rompy.backends.postprocessors</code> - Postprocessor implementations</li> <li>backend_reference - Comprehensive technical reference</li> </ul> <p>The backend system provides a robust, type-safe foundation for model execution while maintaining flexibility for different deployment scenarios. From simple local development to complex containerized production environments, the backend system adapts to your needs while ensuring consistent, reproducible results.</p>"},{"location":"cli/","title":"Command Line Interface","text":"<p>The ROMPY Command Line Interface (CLI) provides a comprehensive set of tools for generating, running, and processing ocean, wave, and hydrodynamic model configurations with support for multiple execution backends.</p>"},{"location":"cli/#overview","title":"Overview","text":"<p>ROMPY CLI provides a comprehensive command structure:</p> <pre><code>rompy &lt;command&gt; [options] [arguments]\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#run","title":"run","text":"<p>Execute a model configuration using a Pydantic backend configuration.</p> <pre><code>rompy run [&lt;config-file&gt;] --backend-config &lt;backend-config-file&gt; [OPTIONS]\n</code></pre> <p>Options:</p> <p><code>--backend-config PATH</code> : Required. YAML/JSON file with backend configuration</p> <p><code>--dry-run</code> : Generate inputs only, don't run the model</p> <p><code>--config-from-env</code> : Load configuration from ROMPY_CONFIG environment variable instead of file</p> <p>Examples:</p> <pre><code># Run with local backend configuration\nrompy run model.yml --backend-config local_backend.yml\n\n# Run with Docker backend configuration\nrompy run model.yml --backend-config docker_backend.yml\n\n# Dry run (generate only, don't execute)\nrompy run model.yml --backend-config local_backend.yml --dry-run\n\n# Load configuration from environment variable\nexport ROMPY_CONFIG=\"$(cat model.yml)\"\nrompy run --config-from-env --backend-config local_backend.yml\n</code></pre>"},{"location":"cli/#backends","title":"backends","text":"<p>Manage execution backend configurations.</p> <pre><code>rompy backends &lt;subcommand&gt; [OPTIONS]\n</code></pre> <p>Subcommands:</p> <p>list : List available backends and configuration types</p> <p>validate : Validate a backend configuration file</p> <p>schema : Generate JSON schema for backend configurations</p> <p>create : Create template backend configuration files</p> <p>Examples:</p> <pre><code># List available backends\nrompy backends list\n\n# Validate configuration\nrompy backends validate my_config.yml --backend-type local\n\n# Generate schema\nrompy backends schema --backend-type docker --format json\n\n# Create template\nrompy backends create --backend-type local --output local_template.yml\n</code></pre>"},{"location":"cli/#pipeline","title":"pipeline","text":"<p>Run the complete model pipeline: generate \u2192 run \u2192 postprocess.</p> <pre><code>rompy pipeline [&lt;config-file&gt;] [OPTIONS]\n</code></pre> <p>Options:</p> <p><code>--run-backend TEXT</code> : Execution backend for run stage (default: local)</p> <p><code>--processor TEXT</code> : Postprocessor to use (default: noop)</p> <p><code>--cleanup-on-failure, --no-cleanup</code> : Clean up outputs on pipeline failure (default: False)</p> <p><code>--validate-stages, --no-validate</code> : Validate each stage before proceeding (default: True)</p> <p><code>--config-from-env</code> : Load configuration from ROMPY_CONFIG environment variable instead of file</p> <p>Example:</p> <pre><code>rompy pipeline config.yaml --run-backend docker --processor analysis\n</code></pre>"},{"location":"cli/#generate","title":"generate","text":"<p>Generate model input files without running the model.</p> <pre><code>rompy generate [&lt;config-file&gt;] [OPTIONS]\n</code></pre> <p>Options:</p> <p><code>--output-dir PATH</code> : Override output directory from configuration</p> <p><code>--config-from-env</code> : Load configuration from ROMPY_CONFIG environment variable instead of file</p> <p>Example:</p> <pre><code>rompy generate config.yaml --output-dir ./test_inputs\n</code></pre>"},{"location":"cli/#validate","title":"validate","text":"<p>Validate model configuration without execution.</p> <pre><code>rompy validate [&lt;config-file&gt;]\n</code></pre> <p>Options:</p> <p><code>--config-from-env</code> : Load configuration from ROMPY_CONFIG environment variable instead of file</p> <p>Examples:</p> <pre><code>rompy validate config.yaml\n\n# Validate configuration from environment variable\nexport ROMPY_CONFIG=\"$(cat config.yaml)\"\nrompy validate --config-from-env\n</code></pre>"},{"location":"cli/#configuration-sources","title":"Configuration Sources","text":"<p>ROMPY CLI commands support loading configuration from two sources:</p> <p>File-based Configuration:</p> <pre><code>rompy validate config.yaml\nrompy run config.yaml --backend-config local.yml\n</code></pre> <p>Environment Variable Configuration:</p> <pre><code>export ROMPY_CONFIG=\"$(cat config.yaml)\"\nrompy validate --config-from-env\nrompy run --config-from-env --backend-config local.yml\n</code></pre> <p>Important Notes:</p> <ul> <li>You cannot specify both a config file and <code>--config-from-env</code> simultaneously</li> <li>The <code>ROMPY_CONFIG</code> environment variable must contain valid JSON or YAML</li> <li>Environment variable configuration is ideal for containers, CI/CD, and cloud deployments</li> <li>All commands that accept configuration support both methods</li> </ul>"},{"location":"cli/#schema","title":"schema","text":"<p>Show configuration schema information.</p> <pre><code>rompy schema [OPTIONS]\n</code></pre> <p>Options:</p> <p><code>--model-type TEXT</code> : Show schema for specific model type</p> <p>Example:</p> <pre><code>rompy schema --model-type swan\n</code></pre>"},{"location":"cli/#backend-configuration-files","title":"Backend Configuration Files","text":"<p>Backend configurations are defined in YAML or JSON files with a <code>type</code> field indicating the backend type:</p> <p>Local Backend Configuration:</p> <pre><code>type: local\ntimeout: 3600\nenv_vars:\n  OMP_NUM_THREADS: \"4\"\n  MODEL_DEBUG: \"true\"\ncommand: \"python run_model.py --verbose\"\nshell: true\ncapture_output: true\n</code></pre> <p>Docker Backend Configuration:</p> <pre><code>type: docker\nimage: \"swan:latest\"\ncpu: 4\nmemory: \"2g\"\ntimeout: 7200\nenv_vars:\n  SWAN_THREADS: \"4\"\nvolumes:\n  - \"/data/input:/app/input:ro\"\n  - \"/data/output:/app/output:rw\"\nexecutable: \"/usr/local/bin/swan\"\n</code></pre> <p>For complete configuration options, see backend_reference.</p>"},{"location":"cli/#global-options","title":"Global Options","text":"<p>All commands support these common options:</p> <p><code>-v, --verbose</code> : Increase verbosity (can be used multiple times: -v, -vv)</p> <p><code>--log-dir PATH</code> : Directory to save log files</p> <p><code>--show-warnings, --hide-warnings</code> : Show or hide Python warnings (default: hide)</p> <p><code>--ascii-only, --unicode</code> : Use ASCII-only characters in output (default: unicode)</p> <p><code>--simple-logs, --detailed-logs</code> : Use simple log format without timestamps and module names (default: detailed)</p> <p><code>--config-from-env</code> : Load configuration from ROMPY_CONFIG environment variable instead of file</p> <p><code>--version</code> : Show version information and exit</p>"},{"location":"cli/#backend-types","title":"Backend Types","text":"<p>ROMPY supports multiple execution backends through its plugin architecture:</p>"},{"location":"cli/#run-backends","title":"Run Backends","text":"<p>Execute models in different environments:</p> <ul> <li>local: Execute directly on the local system</li> <li>docker: Execute inside Docker containers</li> <li>slurm: Execute via SLURM job scheduler (if available)</li> <li>kubernetes: Execute on Kubernetes clusters (if available)</li> </ul>"},{"location":"cli/#postprocessors","title":"Postprocessors","text":"<p>Handle model output analysis and transformation:</p> <ul> <li>noop: No-operation processor (validation only)</li> <li>analysis: Statistical analysis and metrics calculation</li> <li>visualization: Generate plots and animations</li> <li>netcdf: NetCDF output processing and compression</li> </ul>"},{"location":"cli/#pipeline-backends","title":"Pipeline Backends","text":"<p>Orchestrate complete workflows:</p> <ul> <li>local: Execute all stages locally</li> <li>hpc: HPC-optimized pipeline execution</li> <li>cloud: Cloud-native pipeline execution</li> </ul>"},{"location":"cli/#examples","title":"Examples","text":""},{"location":"cli/#modern-workflow-examples","title":"Modern Workflow Examples","text":"<p>Execute a SWAN model with typed backend configuration:</p> <pre><code>rompy run swan_config.yaml --backend-config local_backend.yml\n</code></pre> <p>Complete pipeline with analysis:</p> <pre><code>rompy pipeline ocean_model.yaml \\\n    --run-backend local \\\n    --processor analysis \\\n    --validate-stages\n</code></pre> <p>Development workflow:</p> <pre><code># Validate configuration\nrompy validate config.yaml\n\n# Generate inputs only\nrompy generate config.yaml --output-dir ./test_run\n\n# Test run with dry-run\nrompy run config.yaml --backend-config local.yml --dry-run\n</code></pre>"},{"location":"cli/#configuration-files","title":"Configuration Files","text":""},{"location":"cli/#enhanced-configuration-structure","title":"Enhanced Configuration Structure","text":"<p>The modern CLI supports enhanced configuration files with run and pipeline settings:</p> <pre><code># Basic model configuration\nrun_id: my_ocean_model\nperiod:\n  start: 20230101T00\n  end: 20230102T00\n  interval: 3600\noutput_dir: ./outputs\n\nconfig:\n  model_type: schism\n  # ... model-specific configuration\n\n# Run configuration (optional)\nrun:\n  backend: local\n  local:\n    env_vars:\n      OMP_NUM_THREADS: \"4\"\n    timeout: 3600\n\n# Pipeline configuration (optional)\npipeline:\n  backend: local\n  local:\n    run_backend: docker\n    processor: analysis\n    cleanup_on_failure: false\n</code></pre>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":""},{"location":"cli/#configuration-loading","title":"Configuration Loading","text":"<p>Load configuration from environment variables instead of files:</p> <pre><code># Set configuration in environment variable (JSON format)\nexport ROMPY_CONFIG='{\n  \"run_id\": \"env_example\",\n  \"output_dir\": \"./output\",\n  \"period\": {\"start\": \"20230101T000000\", \"duration\": \"1d\"},\n  \"config\": {\"model_type\": \"swanconfig\"}\n}'\n\n# Use environment variable configuration\nrompy validate --config-from-env\nrompy generate --config-from-env\nrompy run --config-from-env --backend-config local.yml\n\n# YAML format is also supported\nexport ROMPY_CONFIG=\"\nrun_id: env_yaml_example\noutput_dir: ./output\nperiod:\n  start: 20230101T000000\n  duration: 1d\nconfig:\n  model_type: swanconfig\n\"\n</code></pre>"},{"location":"cli/#global-settings","title":"Global Settings","text":"<p>Set default values using environment variables:</p> <pre><code>export ROMPY_LOG_DIR=\"./logs\"\nexport ROMPY_ASCII_ONLY=\"1\"\nexport ROMPY_SIMPLE_LOGS=\"1\"\n\nrompy run config.yaml --backend-config local.yml  # Uses environment settings\n</code></pre>"},{"location":"cli/#container-and-cicd-usage","title":"Container and CI/CD Usage","text":"<p>Environment variable configuration is particularly useful for containerized deployments:</p> <pre><code># Docker usage\ndocker run -e ROMPY_CONFIG=\"$(cat config.yml)\" rompy:latest \\\n  rompy validate --config-from-env\n\n# Kubernetes ConfigMap\nkubectl create configmap rompy-config --from-file=ROMPY_CONFIG=config.yml\n# Then reference in pod spec as environment variable\n\n# CI/CD Pipeline\nexport ROMPY_CONFIG=\"$(envsubst &lt; config_template.yml)\"\nrompy pipeline --config-from-env --run-backend docker\n</code></pre>"},{"location":"cli/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"cli/#verbose-output","title":"Verbose Output","text":"<p>Use multiple -v flags for increased verbosity:</p> <pre><code>rompy run config.yaml --backend-config local.yml -v      # INFO level\nrompy run config.yaml --backend-config local.yml -vv     # DEBUG level\n</code></pre>"},{"location":"cli/#log-files","title":"Log Files","text":"<p>Save detailed logs to files:</p> <pre><code>rompy pipeline config.yaml --log-dir ./logs --verbose\n</code></pre>"},{"location":"cli/#backend-information","title":"Backend Information","text":"<p>Inspect available backends:</p> <pre><code>rompy backends list\n</code></pre>"},{"location":"cli/#validation-and-testing","title":"Validation and Testing","text":"<p>Validate configurations before running:</p> <pre><code>rompy validate config.yaml\nrompy backends validate backend_config.yml --backend-type local\n</code></pre>"},{"location":"cli/#exit-codes","title":"Exit Codes","text":"<p>The CLI uses standard exit codes:</p> <ul> <li><code>0</code>: Success</li> <li><code>1</code>: Execution error</li> <li><code>2</code>: Configuration or argument error</li> </ul>"},{"location":"cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/#common-issues","title":"Common Issues","text":"<p>Configuration Validation Errors:</p> <pre><code>rompy validate config.yaml\nrompy backends validate backend_config.yml --backend-type local\n\n# Test environment variable configuration\nexport ROMPY_CONFIG=\"$(cat config.yaml)\"\nrompy validate --config-from-env\n</code></pre> <p>Backend Not Available:</p> <pre><code>rompy backends list\n</code></pre> <p>Execution Failures:</p> <pre><code>rompy run config.yaml --backend-config local.yml --verbose --log-dir ./debug_logs\n</code></pre> <p>Docker Issues:</p> <pre><code># Check if Docker backend is available\nrompy backends list | grep docker\n\n# Test with local backend first\nrompy run config.yaml --backend-config local_backend.yml\n</code></pre>"},{"location":"cli/#getting-help","title":"Getting Help","text":"<ul> <li>Use <code>--help</code> with any command for detailed options</li> <li>Check the developer documentation for architectural details</li> <li>Use verbose logging for debugging execution issues</li> <li>Validate configurations before running production jobs</li> </ul> <pre><code>rompy --help\nrompy run --help\nrompy pipeline --help\nrompy backends --help\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! Here are the guidelines for contributing to rompy:</p>"},{"location":"contributing/#setup-for-development","title":"Setup for Development","text":"<ol> <li>Fork the repository</li> <li>Clone your fork</li> <li>Create a virtual environment</li> <li>Install in editable mode: <code>pip install -e .[dev]</code></li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a feature branch</li> <li>Add your changes</li> <li>Add tests if applicable</li> <li>Update documentation</li> <li>Submit a pull request</li> </ol>"},{"location":"core_concepts/","title":"Core Concepts","text":"<p>[!NOTE] For information about Rompy's formatting and logging system, see formatting_and_logging.</p> <p>For details on using the command line interface, see cli.</p> <p>Rompy is a Python library for generating ocean model control files and required input data ready for ingestion into the model. The framework is separated into two broad concepts:</p> <p>There is information about each of these in the documentation of each object, but at a high level, ModelRun is the high level framework that renders the config object and controls the period of which the model is run, and the config object is responsible for producing model configuration. </p> <p>If we consider a very simple case using the <code>BaseConfig</code> class. This is not intended to do anything except provide a base class on which to implement a specific model, however, is is functional and can be used to demonstrate core concepts.</p>"},{"location":"core_concepts/#rompy.model.ModelRun","title":"ModelRun","text":"<p>               Bases: <code>RompyBaseModel</code></p> <p>A model run.</p> <p>It is intented to be model agnostic. It deals primarily with how the model is to be run, i.e. the period of the run and where the output is going. The actual configuration of the run is provided by the config object.</p> <p>Further explanation is given in the rompy.core.Baseconfig docstring.</p> Source code in <code>rompy/model.py</code> <pre><code>class ModelRun(RompyBaseModel):\n    \"\"\"A model run.\n\n    It is intented to be model agnostic.\n    It deals primarily with how the model is to be run, i.e. the period of the run\n    and where the output is going. The actual configuration of the run is\n    provided by the config object.\n\n    Further explanation is given in the rompy.core.Baseconfig docstring.\n    \"\"\"\n\n    # Initialize formatting variables in __init__\n\n    model_type: Literal[\"modelrun\"] = Field(\"modelrun\", description=\"The model type.\")\n    run_id: str = Field(\"run_id\", description=\"The run id\")\n    period: TimeRange = Field(\n        TimeRange(\n            start=datetime(2020, 2, 21, 4),\n            end=datetime(2020, 2, 24, 4),\n            interval=\"15M\",\n        ),\n        description=\"The time period to run the model\",\n    )\n    output_dir: Path = Field(\"./simulations\", description=\"The output directory\")\n    config: Union[CONFIG_TYPES] = Field(\n        default_factory=BaseConfig,\n        description=\"The configuration object\",\n        discriminator=\"model_type\",\n    )\n    delete_existing: bool = Field(False, description=\"Delete existing output directory\")\n    run_id_subdir: bool = Field(\n        True, description=\"Use run_id subdirectory in the output directory\"\n    )\n    _datefmt: str = \"%Y%m%d.%H%M%S\"\n    _staging_dir: Path = None\n\n    @property\n    def staging_dir(self):\n        \"\"\"The directory where the model is staged for execution\n\n        returns\n        -------\n        staging_dir : str\n        \"\"\"\n\n        if self._staging_dir is None:\n            self._staging_dir = self._create_staging_dir()\n        return self._staging_dir\n\n    def _create_staging_dir(self):\n        if self.run_id_subdir:\n            odir = Path(self.output_dir) / self.run_id\n        else:\n            odir = Path(self.output_dir)\n        if self.delete_existing and odir.exists():\n            shutil.rmtree(odir)\n        odir.mkdir(parents=True, exist_ok=True)\n        return odir\n\n    @property\n    def _generation_medatadata(self):\n        return dict(\n            _generated_at=str(datetime.now(timezone.utc)),\n            _generated_by=os.environ.get(\"USER\"),\n            _generated_on=platform.node(),\n        )\n\n    def generate(self) -&gt; str:\n        \"\"\"Generate the model input files\n\n        returns\n        -------\n        staging_dir : str\n\n        \"\"\"\n        # Import formatting utilities\n        from rompy.formatting import format_table_row, log_box\n\n        # Format model settings in a structured way\n        config_type = type(self.config).__name__\n        duration = self.period.end - self.period.start\n        formatted_duration = self.period.format_duration(duration)\n\n        # Create table rows for the model run info\n        rows = [\n            format_table_row(\"Run ID\", str(self.run_id)),\n            format_table_row(\"Model Type\", config_type),\n            format_table_row(\"Start Time\", self.period.start.isoformat()),\n            format_table_row(\"End Time\", self.period.end.isoformat()),\n            format_table_row(\"Duration\", formatted_duration),\n            format_table_row(\"Time Interval\", str(self.period.interval)),\n            format_table_row(\"Output Directory\", str(self.output_dir)),\n        ]\n\n        # Add description if available\n        if hasattr(self.config, \"description\") and self.config.description:\n            rows.append(format_table_row(\"Description\", self.config.description))\n\n        # Create a formatted table with proper alignment\n        formatted_rows = []\n        key_lengths = []\n\n        # First pass: collect all valid rows and calculate max key length\n        for row in rows:\n            try:\n                # Split the row by the box-drawing vertical line character\n                parts = [p.strip() for p in row.split(\"\u2503\") if p.strip()]\n                if len(parts) &gt;= 2:  # We expect at least key and value parts\n                    key = parts[0].strip()\n                    value = parts[1].strip() if len(parts) &gt; 1 else \"\"\n                    key_lengths.append(len(key))\n                    formatted_rows.append((key, value))\n            except Exception as e:\n                logger.warning(f\"Error processing row '{row}': {e}\")\n\n        if not formatted_rows:\n            logger.warning(\"No valid rows found for model run configuration table\")\n            return self._staging_dir\n\n        max_key_len = max(key_lengths) if key_lengths else 0\n\n        # Format the rows with proper alignment\n        aligned_rows = []\n        for key, value in formatted_rows:\n            aligned_row = f\"{key:&gt;{max_key_len}} : {value}\"\n            aligned_rows.append(aligned_row)\n\n        # Log the box with the model run info\n        log_box(title=\"MODEL RUN CONFIGURATION\", logger=logger, add_empty_line=False)\n\n        # Log each row of the content with proper indentation\n        for row in aligned_rows:\n            logger.info(f\"  {row}\")\n\n        # Log the bottom of the box\n        log_box(\n            title=None,\n            logger=logger,\n            add_empty_line=True,  # Just the bottom border\n        )\n\n        # Display detailed configuration info using the new formatting framework\n        from rompy.formatting import log_box\n\n        # Create a box with the configuration type as title\n        log_box(f\"MODEL CONFIGURATION ({config_type})\")\n\n        # Use the model's string representation which now uses the new formatting\n        try:\n            # The __str__ method of RompyBaseModel already handles the formatting\n            config_str = str(self.config)\n            for line in config_str.split(\"\\n\"):\n                logger.info(line)\n        except Exception as e:\n            # If anything goes wrong with config formatting, log the error and minimal info\n            logger.info(f\"Using {type(self.config).__name__} configuration\")\n            logger.debug(f\"Configuration string formatting error: {str(e)}\")\n\n        logger.info(\"\")\n        log_box(\n            title=\"STARTING MODEL GENERATION\",\n            logger=logger,\n            add_empty_line=False,\n        )\n        logger.info(f\"Preparing input files in {self.output_dir}\")\n\n        # Collect context data\n        cc_full = {}\n        cc_full[\"runtime\"] = self.model_dump()\n        cc_full[\"runtime\"][\"staging_dir\"] = self.staging_dir\n        cc_full[\"runtime\"].update(self._generation_medatadata)\n        cc_full[\"runtime\"].update({\"_datefmt\": self._datefmt})\n\n        # Process configuration\n        logger.info(\"Processing model configuration...\")\n        if callable(self.config):\n            # Run the __call__() method of the config object if it is callable passing\n            # the runtime instance, and fill in the context with what is returned\n            logger.info(\"Running configuration callable...\")\n            cc_full[\"config\"] = self.config(self)\n        else:\n            # Otherwise just fill in the context with the config instance itself\n            logger.info(\"Using static configuration...\")\n            cc_full[\"config\"] = self.config\n\n        # Render templates\n        logger.info(f\"Rendering model templates to {self.output_dir}/{self.run_id}...\")\n        staging_dir = render(\n            cc_full, self.config.template, self.output_dir, self.config.checkout\n        )\n\n        logger.info(\"\")\n        # Use the log_box utility function\n        from rompy.formatting import log_box\n\n        log_box(\n            title=\"MODEL GENERATION COMPLETE\",\n            logger=logger,\n            add_empty_line=False,\n        )\n        logger.info(f\"Model files generated at: {staging_dir}\")\n        return staging_dir\n\n    def zip(self) -&gt; str:\n        \"\"\"Zip the input files for the model run\n\n        This function zips the input files for the model run and returns the\n        name of the zip file. It also cleans up the staging directory leaving\n        only the settings.json file that can be used to reproduce the run.\n\n        returns\n        -------\n        zip_fn : str\n        \"\"\"\n        # Use the log_box utility function\n        from rompy.formatting import log_box\n\n        log_box(\n            title=\"ARCHIVING MODEL FILES\",\n            logger=logger,\n        )\n\n        # Always remove previous zips\n        zip_fn = Path(str(self.staging_dir) + \".zip\")\n        if zip_fn.exists():\n            logger.info(f\"Removing existing archive at {zip_fn}\")\n            zip_fn.unlink()\n\n        # Count files to be archived\n        file_count = sum([len(fn) for _, _, fn in os.walk(self.staging_dir)])\n        logger.info(f\"Archiving {file_count} files from {self.staging_dir}\")\n\n        # Create zip archive\n        with zf.ZipFile(zip_fn, mode=\"w\", compression=zf.ZIP_DEFLATED) as z:\n            for dp, dn, fn in os.walk(self.staging_dir):\n                for filename in fn:\n                    source_path = os.path.join(dp, filename)\n                    rel_path = os.path.relpath(source_path, self.staging_dir)\n                    z.write(source_path, rel_path)\n\n        # Clean up staging directory\n        logger.info(f\"Cleaning up staging directory {self.staging_dir}\")\n        shutil.rmtree(self.staging_dir)\n\n        from rompy.formatting import log_box\n\n        log_box(\n            f\"\u2713 Archive created successfully: {zip_fn}\",\n            logger=logger,\n            add_empty_line=False,\n        )\n        return zip_fn\n\n    def __call__(self):\n        return self.generate()\n\n    def run(self, backend: BackendConfig, workspace_dir: Optional[str] = None) -&gt; bool:\n        \"\"\"\n        Run the model using the specified backend configuration.\n\n        This method uses Pydantic configuration objects that provide type safety\n        and validation for all backend parameters.\n\n        Args:\n            backend: Pydantic configuration object (LocalConfig, DockerConfig, etc.)\n            workspace_dir: Path to generated workspace directory (optional)\n\n        Returns:\n            True if execution was successful, False otherwise\n\n        Raises:\n            TypeError: If backend is not a BackendConfig instance\n\n        Examples:\n            from rompy.backends import LocalConfig, DockerConfig\n\n            # Local execution\n            model.run(LocalConfig(timeout=3600, command=\"python run.py\"))\n\n            # Docker execution\n            model.run(DockerConfig(image=\"swan:latest\", cpu=4, memory=\"2g\"))\n        \"\"\"\n        if not isinstance(backend, BaseBackendConfig):\n            raise TypeError(\n                f\"Backend must be a subclass of BaseBackendConfig, \"\n                f\"got {type(backend).__name__}\"\n            )\n\n        logger.debug(f\"Using backend config: {type(backend).__name__}\")\n\n        # Get the backend class directly from the configuration\n        backend_class = backend.get_backend_class()\n        backend_instance = backend_class()\n\n        # Pass the config object and workspace_dir to the backend\n        return backend_instance.run(self, config=backend, workspace_dir=workspace_dir)\n\n    def postprocess(self, processor: str = \"noop\", **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        Postprocess the model outputs using the specified processor.\n\n        This method uses entry points to load and execute the appropriate postprocessor.\n        Available processors are automatically discovered from the rompy.postprocess entry point group.\n\n        Built-in processors:\n        - \"noop\": A placeholder processor that does nothing but returns success\n\n        Args:\n            processor: Name of the postprocessor to use (default: \"noop\")\n            **kwargs: Additional processor-specific parameters\n\n        Returns:\n            Dictionary with results from the postprocessing\n\n        Raises:\n            ValueError: If the specified processor is not available\n        \"\"\"\n        # Get the requested postprocessor class from entry points\n        if processor not in POSTPROCESSORS:\n            available = list(POSTPROCESSORS.keys())\n            raise ValueError(\n                f\"Unknown postprocessor: {processor}. \"\n                f\"Available processors: {', '.join(available)}\"\n            )\n\n        # Create an instance and process the outputs\n        processor_class = POSTPROCESSORS[processor]\n        processor_instance = processor_class()\n        return processor_instance.process(self, **kwargs)\n\n    def pipeline(self, pipeline_backend: str = \"local\", **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        Run the complete model pipeline (generate, run, postprocess) using the specified pipeline backend.\n\n        This method executes the entire model workflow from input generation through running\n        the model to postprocessing outputs. It uses entry points to load and execute the\n        appropriate pipeline backend from the rompy.pipeline entry point group.\n\n        Built-in pipeline backends:\n        - \"local\": Execute the complete pipeline locally using the existing ModelRun methods\n\n        Args:\n            pipeline_backend: Name of the pipeline backend to use (default: \"local\")\n            **kwargs: Additional backend-specific parameters. Common parameters include:\n                - run_backend: Backend to use for the run stage (for local pipeline)\n                - processor: Processor to use for postprocessing (for local pipeline)\n                - run_kwargs: Additional parameters for the run stage\n                - process_kwargs: Additional parameters for postprocessing\n\n        Returns:\n            Dictionary with results from the pipeline execution\n\n        Raises:\n            ValueError: If the specified pipeline backend is not available\n        \"\"\"\n        # Get the requested pipeline backend class from entry points\n        if pipeline_backend not in PIPELINE_BACKENDS:\n            available = list(PIPELINE_BACKENDS.keys())\n            raise ValueError(\n                f\"Unknown pipeline backend: {pipeline_backend}. \"\n                f\"Available backends: {', '.join(available)}\"\n            )\n\n        # Create an instance and execute the pipeline\n        backend_class = PIPELINE_BACKENDS[pipeline_backend]\n        backend_instance = backend_class()\n        return backend_instance.execute(self, **kwargs)\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.model.ModelRun.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['modelrun'] = Field('modelrun', description='The model type.')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.run_id","title":"run_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>run_id: str = Field('run_id', description='The run id')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.period","title":"period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>period: TimeRange = Field(TimeRange(start=datetime(2020, 2, 21, 4), end=datetime(2020, 2, 24, 4), interval='15M'), description='The time period to run the model')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.output_dir","title":"output_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_dir: Path = Field('./simulations', description='The output directory')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: Union[CONFIG_TYPES] = Field(default_factory=BaseConfig, description='The configuration object', discriminator='model_type')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.delete_existing","title":"delete_existing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>delete_existing: bool = Field(False, description='Delete existing output directory')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.run_id_subdir","title":"run_id_subdir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>run_id_subdir: bool = Field(True, description='Use run_id subdirectory in the output directory')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.staging_dir","title":"staging_dir  <code>property</code>","text":"<pre><code>staging_dir\n</code></pre> <p>The directory where the model is staged for execution</p>"},{"location":"core_concepts/#rompy.model.ModelRun.staging_dir--returns","title":"returns","text":"<p>staging_dir : str</p>"},{"location":"core_concepts/#rompy.model.ModelRun-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.model.ModelRun.generate","title":"generate","text":"<pre><code>generate() -&gt; str\n</code></pre> <p>Generate the model input files</p>"},{"location":"core_concepts/#rompy.model.ModelRun.generate--returns","title":"returns","text":"<p>staging_dir : str</p> Source code in <code>rompy/model.py</code> <pre><code>def generate(self) -&gt; str:\n    \"\"\"Generate the model input files\n\n    returns\n    -------\n    staging_dir : str\n\n    \"\"\"\n    # Import formatting utilities\n    from rompy.formatting import format_table_row, log_box\n\n    # Format model settings in a structured way\n    config_type = type(self.config).__name__\n    duration = self.period.end - self.period.start\n    formatted_duration = self.period.format_duration(duration)\n\n    # Create table rows for the model run info\n    rows = [\n        format_table_row(\"Run ID\", str(self.run_id)),\n        format_table_row(\"Model Type\", config_type),\n        format_table_row(\"Start Time\", self.period.start.isoformat()),\n        format_table_row(\"End Time\", self.period.end.isoformat()),\n        format_table_row(\"Duration\", formatted_duration),\n        format_table_row(\"Time Interval\", str(self.period.interval)),\n        format_table_row(\"Output Directory\", str(self.output_dir)),\n    ]\n\n    # Add description if available\n    if hasattr(self.config, \"description\") and self.config.description:\n        rows.append(format_table_row(\"Description\", self.config.description))\n\n    # Create a formatted table with proper alignment\n    formatted_rows = []\n    key_lengths = []\n\n    # First pass: collect all valid rows and calculate max key length\n    for row in rows:\n        try:\n            # Split the row by the box-drawing vertical line character\n            parts = [p.strip() for p in row.split(\"\u2503\") if p.strip()]\n            if len(parts) &gt;= 2:  # We expect at least key and value parts\n                key = parts[0].strip()\n                value = parts[1].strip() if len(parts) &gt; 1 else \"\"\n                key_lengths.append(len(key))\n                formatted_rows.append((key, value))\n        except Exception as e:\n            logger.warning(f\"Error processing row '{row}': {e}\")\n\n    if not formatted_rows:\n        logger.warning(\"No valid rows found for model run configuration table\")\n        return self._staging_dir\n\n    max_key_len = max(key_lengths) if key_lengths else 0\n\n    # Format the rows with proper alignment\n    aligned_rows = []\n    for key, value in formatted_rows:\n        aligned_row = f\"{key:&gt;{max_key_len}} : {value}\"\n        aligned_rows.append(aligned_row)\n\n    # Log the box with the model run info\n    log_box(title=\"MODEL RUN CONFIGURATION\", logger=logger, add_empty_line=False)\n\n    # Log each row of the content with proper indentation\n    for row in aligned_rows:\n        logger.info(f\"  {row}\")\n\n    # Log the bottom of the box\n    log_box(\n        title=None,\n        logger=logger,\n        add_empty_line=True,  # Just the bottom border\n    )\n\n    # Display detailed configuration info using the new formatting framework\n    from rompy.formatting import log_box\n\n    # Create a box with the configuration type as title\n    log_box(f\"MODEL CONFIGURATION ({config_type})\")\n\n    # Use the model's string representation which now uses the new formatting\n    try:\n        # The __str__ method of RompyBaseModel already handles the formatting\n        config_str = str(self.config)\n        for line in config_str.split(\"\\n\"):\n            logger.info(line)\n    except Exception as e:\n        # If anything goes wrong with config formatting, log the error and minimal info\n        logger.info(f\"Using {type(self.config).__name__} configuration\")\n        logger.debug(f\"Configuration string formatting error: {str(e)}\")\n\n    logger.info(\"\")\n    log_box(\n        title=\"STARTING MODEL GENERATION\",\n        logger=logger,\n        add_empty_line=False,\n    )\n    logger.info(f\"Preparing input files in {self.output_dir}\")\n\n    # Collect context data\n    cc_full = {}\n    cc_full[\"runtime\"] = self.model_dump()\n    cc_full[\"runtime\"][\"staging_dir\"] = self.staging_dir\n    cc_full[\"runtime\"].update(self._generation_medatadata)\n    cc_full[\"runtime\"].update({\"_datefmt\": self._datefmt})\n\n    # Process configuration\n    logger.info(\"Processing model configuration...\")\n    if callable(self.config):\n        # Run the __call__() method of the config object if it is callable passing\n        # the runtime instance, and fill in the context with what is returned\n        logger.info(\"Running configuration callable...\")\n        cc_full[\"config\"] = self.config(self)\n    else:\n        # Otherwise just fill in the context with the config instance itself\n        logger.info(\"Using static configuration...\")\n        cc_full[\"config\"] = self.config\n\n    # Render templates\n    logger.info(f\"Rendering model templates to {self.output_dir}/{self.run_id}...\")\n    staging_dir = render(\n        cc_full, self.config.template, self.output_dir, self.config.checkout\n    )\n\n    logger.info(\"\")\n    # Use the log_box utility function\n    from rompy.formatting import log_box\n\n    log_box(\n        title=\"MODEL GENERATION COMPLETE\",\n        logger=logger,\n        add_empty_line=False,\n    )\n    logger.info(f\"Model files generated at: {staging_dir}\")\n    return staging_dir\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.zip","title":"zip","text":"<pre><code>zip() -&gt; str\n</code></pre> <p>Zip the input files for the model run</p> <p>This function zips the input files for the model run and returns the name of the zip file. It also cleans up the staging directory leaving only the settings.json file that can be used to reproduce the run.</p>"},{"location":"core_concepts/#rompy.model.ModelRun.zip--returns","title":"returns","text":"<p>zip_fn : str</p> Source code in <code>rompy/model.py</code> <pre><code>def zip(self) -&gt; str:\n    \"\"\"Zip the input files for the model run\n\n    This function zips the input files for the model run and returns the\n    name of the zip file. It also cleans up the staging directory leaving\n    only the settings.json file that can be used to reproduce the run.\n\n    returns\n    -------\n    zip_fn : str\n    \"\"\"\n    # Use the log_box utility function\n    from rompy.formatting import log_box\n\n    log_box(\n        title=\"ARCHIVING MODEL FILES\",\n        logger=logger,\n    )\n\n    # Always remove previous zips\n    zip_fn = Path(str(self.staging_dir) + \".zip\")\n    if zip_fn.exists():\n        logger.info(f\"Removing existing archive at {zip_fn}\")\n        zip_fn.unlink()\n\n    # Count files to be archived\n    file_count = sum([len(fn) for _, _, fn in os.walk(self.staging_dir)])\n    logger.info(f\"Archiving {file_count} files from {self.staging_dir}\")\n\n    # Create zip archive\n    with zf.ZipFile(zip_fn, mode=\"w\", compression=zf.ZIP_DEFLATED) as z:\n        for dp, dn, fn in os.walk(self.staging_dir):\n            for filename in fn:\n                source_path = os.path.join(dp, filename)\n                rel_path = os.path.relpath(source_path, self.staging_dir)\n                z.write(source_path, rel_path)\n\n    # Clean up staging directory\n    logger.info(f\"Cleaning up staging directory {self.staging_dir}\")\n    shutil.rmtree(self.staging_dir)\n\n    from rompy.formatting import log_box\n\n    log_box(\n        f\"\u2713 Archive created successfully: {zip_fn}\",\n        logger=logger,\n        add_empty_line=False,\n    )\n    return zip_fn\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.run","title":"run","text":"<pre><code>run(backend: BackendConfig, workspace_dir: Optional[str] = None) -&gt; bool\n</code></pre> <p>Run the model using the specified backend configuration.</p> <p>This method uses Pydantic configuration objects that provide type safety and validation for all backend parameters.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>BackendConfig</code> <p>Pydantic configuration object (LocalConfig, DockerConfig, etc.)</p> required <code>workspace_dir</code> <code>Optional[str]</code> <p>Path to generated workspace directory (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if execution was successful, False otherwise</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If backend is not a BackendConfig instance</p> <p>Examples:</p> <p>from rompy.backends import LocalConfig, DockerConfig</p>"},{"location":"core_concepts/#rompy.model.ModelRun.run--local-execution","title":"Local execution","text":"<p>model.run(LocalConfig(timeout=3600, command=\"python run.py\"))</p>"},{"location":"core_concepts/#rompy.model.ModelRun.run--docker-execution","title":"Docker execution","text":"<p>model.run(DockerConfig(image=\"swan:latest\", cpu=4, memory=\"2g\"))</p> Source code in <code>rompy/model.py</code> <pre><code>def run(self, backend: BackendConfig, workspace_dir: Optional[str] = None) -&gt; bool:\n    \"\"\"\n    Run the model using the specified backend configuration.\n\n    This method uses Pydantic configuration objects that provide type safety\n    and validation for all backend parameters.\n\n    Args:\n        backend: Pydantic configuration object (LocalConfig, DockerConfig, etc.)\n        workspace_dir: Path to generated workspace directory (optional)\n\n    Returns:\n        True if execution was successful, False otherwise\n\n    Raises:\n        TypeError: If backend is not a BackendConfig instance\n\n    Examples:\n        from rompy.backends import LocalConfig, DockerConfig\n\n        # Local execution\n        model.run(LocalConfig(timeout=3600, command=\"python run.py\"))\n\n        # Docker execution\n        model.run(DockerConfig(image=\"swan:latest\", cpu=4, memory=\"2g\"))\n    \"\"\"\n    if not isinstance(backend, BaseBackendConfig):\n        raise TypeError(\n            f\"Backend must be a subclass of BaseBackendConfig, \"\n            f\"got {type(backend).__name__}\"\n        )\n\n    logger.debug(f\"Using backend config: {type(backend).__name__}\")\n\n    # Get the backend class directly from the configuration\n    backend_class = backend.get_backend_class()\n    backend_instance = backend_class()\n\n    # Pass the config object and workspace_dir to the backend\n    return backend_instance.run(self, config=backend, workspace_dir=workspace_dir)\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.postprocess","title":"postprocess","text":"<pre><code>postprocess(processor: str = 'noop', **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Postprocess the model outputs using the specified processor.</p> <p>This method uses entry points to load and execute the appropriate postprocessor. Available processors are automatically discovered from the rompy.postprocess entry point group.</p> <p>Built-in processors: - \"noop\": A placeholder processor that does nothing but returns success</p> <p>Parameters:</p> Name Type Description Default <code>processor</code> <code>str</code> <p>Name of the postprocessor to use (default: \"noop\")</p> <code>'noop'</code> <code>**kwargs</code> <p>Additional processor-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with results from the postprocessing</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified processor is not available</p> Source code in <code>rompy/model.py</code> <pre><code>def postprocess(self, processor: str = \"noop\", **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Postprocess the model outputs using the specified processor.\n\n    This method uses entry points to load and execute the appropriate postprocessor.\n    Available processors are automatically discovered from the rompy.postprocess entry point group.\n\n    Built-in processors:\n    - \"noop\": A placeholder processor that does nothing but returns success\n\n    Args:\n        processor: Name of the postprocessor to use (default: \"noop\")\n        **kwargs: Additional processor-specific parameters\n\n    Returns:\n        Dictionary with results from the postprocessing\n\n    Raises:\n        ValueError: If the specified processor is not available\n    \"\"\"\n    # Get the requested postprocessor class from entry points\n    if processor not in POSTPROCESSORS:\n        available = list(POSTPROCESSORS.keys())\n        raise ValueError(\n            f\"Unknown postprocessor: {processor}. \"\n            f\"Available processors: {', '.join(available)}\"\n        )\n\n    # Create an instance and process the outputs\n    processor_class = POSTPROCESSORS[processor]\n    processor_instance = processor_class()\n    return processor_instance.process(self, **kwargs)\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.pipeline","title":"pipeline","text":"<pre><code>pipeline(pipeline_backend: str = 'local', **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Run the complete model pipeline (generate, run, postprocess) using the specified pipeline backend.</p> <p>This method executes the entire model workflow from input generation through running the model to postprocessing outputs. It uses entry points to load and execute the appropriate pipeline backend from the rompy.pipeline entry point group.</p> <p>Built-in pipeline backends: - \"local\": Execute the complete pipeline locally using the existing ModelRun methods</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_backend</code> <code>str</code> <p>Name of the pipeline backend to use (default: \"local\")</p> <code>'local'</code> <code>**kwargs</code> <p>Additional backend-specific parameters. Common parameters include: - run_backend: Backend to use for the run stage (for local pipeline) - processor: Processor to use for postprocessing (for local pipeline) - run_kwargs: Additional parameters for the run stage - process_kwargs: Additional parameters for postprocessing</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with results from the pipeline execution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified pipeline backend is not available</p> Source code in <code>rompy/model.py</code> <pre><code>def pipeline(self, pipeline_backend: str = \"local\", **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Run the complete model pipeline (generate, run, postprocess) using the specified pipeline backend.\n\n    This method executes the entire model workflow from input generation through running\n    the model to postprocessing outputs. It uses entry points to load and execute the\n    appropriate pipeline backend from the rompy.pipeline entry point group.\n\n    Built-in pipeline backends:\n    - \"local\": Execute the complete pipeline locally using the existing ModelRun methods\n\n    Args:\n        pipeline_backend: Name of the pipeline backend to use (default: \"local\")\n        **kwargs: Additional backend-specific parameters. Common parameters include:\n            - run_backend: Backend to use for the run stage (for local pipeline)\n            - processor: Processor to use for postprocessing (for local pipeline)\n            - run_kwargs: Additional parameters for the run stage\n            - process_kwargs: Additional parameters for postprocessing\n\n    Returns:\n        Dictionary with results from the pipeline execution\n\n    Raises:\n        ValueError: If the specified pipeline backend is not available\n    \"\"\"\n    # Get the requested pipeline backend class from entry points\n    if pipeline_backend not in PIPELINE_BACKENDS:\n        available = list(PIPELINE_BACKENDS.keys())\n        raise ValueError(\n            f\"Unknown pipeline backend: {pipeline_backend}. \"\n            f\"Available backends: {', '.join(available)}\"\n        )\n\n    # Create an instance and execute the pipeline\n    backend_class = PIPELINE_BACKENDS[pipeline_backend]\n    backend_instance = backend_class()\n    return backend_instance.execute(self, **kwargs)\n</code></pre>"},{"location":"core_concepts/#rompy.core.config.BaseConfig","title":"BaseConfig","text":"<p>               Bases: <code>RompyBaseModel</code></p> <p>Base class for model templates.</p> <p>The template class provides the object that is used to set up the model configuration. When implemented for a given model, can move along a scale of complexity to suit the application.</p> <p>In its most basic form, as implemented in this base object, it consists of path to a cookiecutter template with the class providing the context for the {{config}} values in that template. Note that any {{runtime}} values are filled from the ModelRun object.</p> <p>If the template is a git repo, the checkout parameter can be used to specify a branch or tag and it will be cloned and used.</p> <p>If the object is callable, it will be colled prior to rendering the template. This mechanism can be used to perform tasks such as fetching exteral data, or providing additional context to the template beyond the arguments provided by the user..</p> Source code in <code>rompy/core/config.py</code> <pre><code>class BaseConfig(RompyBaseModel):\n    \"\"\"Base class for model templates.\n\n    The template class provides the object that is used to set up the model configuration.\n    When implemented for a given model, can move along a scale of complexity\n    to suit the application.\n\n    In its most basic form, as implemented in this base object, it consists of path to a cookiecutter template\n    with the class providing the context for the {{config}} values in that template. Note that any\n    {{runtime}} values are filled from the ModelRun object.\n\n    If the template is a git repo, the checkout parameter can be used to specify a branch or tag and it\n    will be cloned and used.\n\n    If the object is callable, it will be colled prior to rendering the template. This mechanism can be\n    used to perform tasks such as fetching exteral data, or providing additional context to the template\n    beyond the arguments provided by the user..\n    \"\"\"\n\n    model_type: Literal[\"base\"] = \"base\"\n    template: Optional[str] = Field(\n        description=\"The path to the model template\",\n        default=DEFAULT_TEMPLATE,\n    )\n    checkout: Optional[str] = Field(\n        description=\"The git branch to use if the template is a git repo\",\n        default=\"main\",\n    )\n\n    # noop call for config objects\n    def __call__(self, *args, **kwargs):\n        return self\n</code></pre>"},{"location":"core_concepts/#rompy.core.config.BaseConfig-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.config.BaseConfig.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['base'] = 'base'\n</code></pre>"},{"location":"core_concepts/#rompy.core.config.BaseConfig.template","title":"template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>template: Optional[str] = Field(description='The path to the model template', default=DEFAULT_TEMPLATE)\n</code></pre>"},{"location":"core_concepts/#rompy.core.config.BaseConfig.checkout","title":"checkout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>checkout: Optional[str] = Field(description='The git branch to use if the template is a git repo', default='main')\n</code></pre>"},{"location":"core_concepts/#core-objects","title":"Core objects","text":""},{"location":"core_concepts/#grid","title":"Grid","text":"<p>Grids form a core component of any model. Rompy provides a base class for grids, and a regular grid class. Support for other grid types will be added in the future.</p>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid","title":"BaseGrid","text":"<p>               Bases: <code>RompyBaseModel</code></p> <p>Representation of a grid in geographic space.</p> <p>This is the base class for all Grid objects. The minimum representation of a grid are two NumPy array's representing the vertices or nodes of some structured or unstructured grid, its bounding box and a boundary polygon. No knowledge of the grid connectivity is expected.</p> Source code in <code>rompy/core/grid.py</code> <pre><code>class BaseGrid(RompyBaseModel):\n    \"\"\"Representation of a grid in geographic space.\n\n    This is the base class for all Grid objects. The minimum representation of a grid\n    are two NumPy array's representing the vertices or nodes of some structured or\n    unstructured grid, its bounding box and a boundary polygon. No knowledge of the\n    grid connectivity is expected.\n\n    \"\"\"\n\n    grid_type: Literal[\"base\"] = \"base\"\n\n    @property\n    def x(self) -&gt; np.ndarray:\n        raise NotImplementedError\n\n    @property\n    def y(self) -&gt; np.ndarray:\n        raise NotImplementedError\n\n    @property\n    def minx(self) -&gt; float:\n        return np.nanmin(self.x)\n\n    @property\n    def maxx(self) -&gt; float:\n        return np.nanmax(self.x)\n\n    @property\n    def miny(self) -&gt; float:\n        return np.nanmin(self.y)\n\n    @property\n    def maxy(self) -&gt; float:\n        return np.nanmax(self.y)\n\n    def bbox(self, buffer=0.0) -&gt; Bbox:\n        \"\"\"Returns a bounding box for the spatial grid\n\n        This function returns a list [ll_x, ll_y, ur_x, ur_y]\n        where ll_x, ll_y (ur_x, ur_y) are the lower left (upper right)\n        x and y coordinates bounding box of the model domain\n\n        \"\"\"\n        ll_x = self.minx - buffer\n        ll_y = self.miny - buffer\n        ur_x = self.maxx + buffer\n        ur_y = self.maxy + buffer\n        bbox = [ll_x, ll_y, ur_x, ur_y]\n        return bbox\n\n    def _get_convex_hull(self, tolerance=0.2) -&gt; Polygon:\n        xys = list(zip(self.x.flatten(), self.y.flatten()))\n        polygon = MultiPoint(xys).convex_hull\n        polygon = polygon.simplify(tolerance=tolerance)\n        return polygon\n\n    def boundary(self, tolerance=0.2) -&gt; Polygon:\n        \"\"\"Returns the convex hull boundary polygon from the grid.\n\n        Parameters\n        ----------\n        tolerance: float\n            Simplify polygon shape based on maximum distance from original geometry,\n            see https://shapely.readthedocs.io/en/stable/manual.html#object.simplify.\n\n        Returns\n        -------\n        polygon: shapely.Polygon\n            See https://shapely.readthedocs.io/en/stable/manual.html#Polygon\n\n        \"\"\"\n        return self._get_convex_hull(tolerance=tolerance)\n\n    def boundary_points(self, spacing=None, tolerance=0.2) -&gt; tuple:\n        \"\"\"Returns array of coordinates from boundary polygon.\n\n        Parameters\n        ----------\n        tolerance: float\n            Simplify polygon shape based on maximum distance from original geometry,\n            see https://shapely.readthedocs.io/en/stable/manual.html#object.simplify.\n        spacing: float\n            If specified, points are returned evenly spaced along the boundary at the\n            specified spacing, otherwise all points are returned.\n\n        Returns:\n        --------\n        points: tuple\n            Tuple of x and y coordinates of the boundary points.\n\n        \"\"\"\n        polygon = self.boundary(tolerance=tolerance)\n        if spacing is None:\n            xpts, ypts = polygon.exterior.coords.xy\n        else:\n            perimeter = polygon.length\n            if perimeter &lt; spacing:\n                raise ValueError(f\"Spacing = {spacing} &gt; grid perimeter = {perimeter}\")\n            npts = int(np.ceil(perimeter / spacing))\n            points = [polygon.boundary.interpolate(i * spacing) for i in range(npts)]\n            xpts = [point.x for point in points]\n            ypts = [point.y for point in points]\n        return np.array(xpts), np.array(ypts)\n\n    def _figsize(self, x0, x1, y0, y1, fscale):\n        xlen = abs(x1 - x0)\n        ylen = abs(y1 - y0)\n        if xlen &gt;= ylen:\n            figsize = (fscale, fscale * ylen / xlen or fscale)\n        else:\n            figsize = (fscale * xlen / ylen or fscale, fscale)\n        return figsize\n\n    def plot(\n        self,\n        ax=None,\n        figsize=None,\n        fscale=10,\n        buffer=0.1,\n        borders=True,\n        land=True,\n        coastline=True,\n    ):\n        \"\"\"Plot the grid\"\"\"\n\n        projection = ccrs.PlateCarree()\n        transform = ccrs.PlateCarree()\n\n        # Set some plot parameters:\n        x0, y0, x1, y1 = self.bbox(buffer=buffer)\n\n        # create figure and plot/map\n        if ax is None:\n            if figsize is None:\n                figsize = self._figsize(x0, x1, y0, y1, fscale)\n            fig = plt.figure(figsize=figsize)\n            ax = fig.add_subplot(111, projection=projection)\n            ax.set_extent([x0, x1, y0, y1], crs=transform)\n\n            if borders:\n                ax.add_feature(cfeature.BORDERS)\n            if land:\n                ax.add_feature(cfeature.LAND)\n            if coastline:\n                ax.add_feature(cfeature.COASTLINE)\n        else:\n            fig = ax.figure\n\n        ax.gridlines(\n            crs=transform,\n            draw_labels=[\"left\", \"bottom\"],\n            linewidth=1,\n            color=\"gray\",\n            alpha=0.5,\n            linestyle=\"--\",\n        )\n\n        # Plot the model domain\n        bx, by = self.boundary_points()\n        poly = plt.Polygon(list(zip(bx, by)), facecolor=\"r\", alpha=0.05)\n        ax.add_patch(poly)\n        ax.plot(bx, by, lw=2, color=\"k\")\n        return fig, ax\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.x}, {self.y})\"\n\n    def __eq__(self, other):\n        return self.model_dump() == other.model_dump()\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.grid.BaseGrid.grid_type","title":"grid_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>grid_type: Literal['base'] = 'base'\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.x","title":"x  <code>property</code>","text":"<pre><code>x: ndarray\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.y","title":"y  <code>property</code>","text":"<pre><code>y: ndarray\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.minx","title":"minx  <code>property</code>","text":"<pre><code>minx: float\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.maxx","title":"maxx  <code>property</code>","text":"<pre><code>maxx: float\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.miny","title":"miny  <code>property</code>","text":"<pre><code>miny: float\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.maxy","title":"maxy  <code>property</code>","text":"<pre><code>maxy: float\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.core.grid.BaseGrid.bbox","title":"bbox","text":"<pre><code>bbox(buffer=0.0) -&gt; Bbox\n</code></pre> <p>Returns a bounding box for the spatial grid</p> <p>This function returns a list [ll_x, ll_y, ur_x, ur_y] where ll_x, ll_y (ur_x, ur_y) are the lower left (upper right) x and y coordinates bounding box of the model domain</p> Source code in <code>rompy/core/grid.py</code> <pre><code>def bbox(self, buffer=0.0) -&gt; Bbox:\n    \"\"\"Returns a bounding box for the spatial grid\n\n    This function returns a list [ll_x, ll_y, ur_x, ur_y]\n    where ll_x, ll_y (ur_x, ur_y) are the lower left (upper right)\n    x and y coordinates bounding box of the model domain\n\n    \"\"\"\n    ll_x = self.minx - buffer\n    ll_y = self.miny - buffer\n    ur_x = self.maxx + buffer\n    ur_y = self.maxy + buffer\n    bbox = [ll_x, ll_y, ur_x, ur_y]\n    return bbox\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.boundary","title":"boundary","text":"<pre><code>boundary(tolerance=0.2) -&gt; Polygon\n</code></pre> <p>Returns the convex hull boundary polygon from the grid.</p>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.boundary--parameters","title":"Parameters","text":"<p>tolerance: float     Simplify polygon shape based on maximum distance from original geometry,     see https://shapely.readthedocs.io/en/stable/manual.html#object.simplify.</p>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.boundary--returns","title":"Returns","text":"<p>polygon: shapely.Polygon     See https://shapely.readthedocs.io/en/stable/manual.html#Polygon</p> Source code in <code>rompy/core/grid.py</code> <pre><code>def boundary(self, tolerance=0.2) -&gt; Polygon:\n    \"\"\"Returns the convex hull boundary polygon from the grid.\n\n    Parameters\n    ----------\n    tolerance: float\n        Simplify polygon shape based on maximum distance from original geometry,\n        see https://shapely.readthedocs.io/en/stable/manual.html#object.simplify.\n\n    Returns\n    -------\n    polygon: shapely.Polygon\n        See https://shapely.readthedocs.io/en/stable/manual.html#Polygon\n\n    \"\"\"\n    return self._get_convex_hull(tolerance=tolerance)\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.boundary_points","title":"boundary_points","text":"<pre><code>boundary_points(spacing=None, tolerance=0.2) -&gt; tuple\n</code></pre> <p>Returns array of coordinates from boundary polygon.</p>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.boundary_points--parameters","title":"Parameters","text":"<p>tolerance: float     Simplify polygon shape based on maximum distance from original geometry,     see https://shapely.readthedocs.io/en/stable/manual.html#object.simplify. spacing: float     If specified, points are returned evenly spaced along the boundary at the     specified spacing, otherwise all points are returned.</p>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.boundary_points--returns","title":"Returns:","text":"<p>points: tuple     Tuple of x and y coordinates of the boundary points.</p> Source code in <code>rompy/core/grid.py</code> <pre><code>def boundary_points(self, spacing=None, tolerance=0.2) -&gt; tuple:\n    \"\"\"Returns array of coordinates from boundary polygon.\n\n    Parameters\n    ----------\n    tolerance: float\n        Simplify polygon shape based on maximum distance from original geometry,\n        see https://shapely.readthedocs.io/en/stable/manual.html#object.simplify.\n    spacing: float\n        If specified, points are returned evenly spaced along the boundary at the\n        specified spacing, otherwise all points are returned.\n\n    Returns:\n    --------\n    points: tuple\n        Tuple of x and y coordinates of the boundary points.\n\n    \"\"\"\n    polygon = self.boundary(tolerance=tolerance)\n    if spacing is None:\n        xpts, ypts = polygon.exterior.coords.xy\n    else:\n        perimeter = polygon.length\n        if perimeter &lt; spacing:\n            raise ValueError(f\"Spacing = {spacing} &gt; grid perimeter = {perimeter}\")\n        npts = int(np.ceil(perimeter / spacing))\n        points = [polygon.boundary.interpolate(i * spacing) for i in range(npts)]\n        xpts = [point.x for point in points]\n        ypts = [point.y for point in points]\n    return np.array(xpts), np.array(ypts)\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.BaseGrid.plot","title":"plot","text":"<pre><code>plot(ax=None, figsize=None, fscale=10, buffer=0.1, borders=True, land=True, coastline=True)\n</code></pre> <p>Plot the grid</p> Source code in <code>rompy/core/grid.py</code> <pre><code>def plot(\n    self,\n    ax=None,\n    figsize=None,\n    fscale=10,\n    buffer=0.1,\n    borders=True,\n    land=True,\n    coastline=True,\n):\n    \"\"\"Plot the grid\"\"\"\n\n    projection = ccrs.PlateCarree()\n    transform = ccrs.PlateCarree()\n\n    # Set some plot parameters:\n    x0, y0, x1, y1 = self.bbox(buffer=buffer)\n\n    # create figure and plot/map\n    if ax is None:\n        if figsize is None:\n            figsize = self._figsize(x0, x1, y0, y1, fscale)\n        fig = plt.figure(figsize=figsize)\n        ax = fig.add_subplot(111, projection=projection)\n        ax.set_extent([x0, x1, y0, y1], crs=transform)\n\n        if borders:\n            ax.add_feature(cfeature.BORDERS)\n        if land:\n            ax.add_feature(cfeature.LAND)\n        if coastline:\n            ax.add_feature(cfeature.COASTLINE)\n    else:\n        fig = ax.figure\n\n    ax.gridlines(\n        crs=transform,\n        draw_labels=[\"left\", \"bottom\"],\n        linewidth=1,\n        color=\"gray\",\n        alpha=0.5,\n        linestyle=\"--\",\n    )\n\n    # Plot the model domain\n    bx, by = self.boundary_points()\n    poly = plt.Polygon(list(zip(bx, by)), facecolor=\"r\", alpha=0.05)\n    ax.add_patch(poly)\n    ax.plot(bx, by, lw=2, color=\"k\")\n    return fig, ax\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid","title":"RegularGrid","text":"<p>               Bases: <code>BaseGrid</code></p> <p>Regular grid in geographic space.</p> <p>This object provides an abstract representation of a regular grid in some geographic space.</p> Source code in <code>rompy/core/grid.py</code> <pre><code>class RegularGrid(BaseGrid):\n    \"\"\"Regular grid in geographic space.\n\n    This object provides an abstract representation of a regular grid in some\n    geographic space.\n\n    \"\"\"\n\n    grid_type: Literal[\"regular\"] = Field(\n        \"regular\", description=\"Type of grid, must be 'regular'\"\n    )\n    x0: Optional[float] = Field(\n        default=None, description=\"X coordinate of the grid origin\"\n    )\n    y0: Optional[float] = Field(\n        default=None, description=\"Y coordinate of the grid origin\"\n    )\n    rot: Optional[float] = Field(\n        0.0, description=\"Rotation angle of the grid in degrees\"\n    )\n    dx: Optional[float] = Field(\n        default=None, description=\"Spacing between grid points in the x direction\"\n    )\n    dy: Optional[float] = Field(\n        default=None, description=\"Spacing between grid points in the y direction\"\n    )\n    nx: Optional[int] = Field(\n        default=None, description=\"Number of grid points in the x direction\"\n    )\n    ny: Optional[int] = Field(\n        default=None, description=\"Number of grid points in the y direction\"\n    )\n\n    @model_validator(mode=\"after\")\n    def generate(self) -&gt; \"RegularGrid\":\n        \"\"\"Generate the grid from the provided parameters.\"\"\"\n        keys = [\"x0\", \"y0\", \"dx\", \"dy\", \"nx\", \"ny\"]\n        if None in [getattr(self, key) for key in keys]:\n            raise ValueError(f\"All of {','.join(keys)} must be provided for REG grid\")\n        # Ensure x, y 2D coordinates are defined\n        return self\n\n    @property\n    def x(self) -&gt; np.ndarray:\n        x, y = self._gen_reg_cgrid()\n        return x\n\n    @property\n    def y(self) -&gt; np.ndarray:\n        x, y = self._gen_reg_cgrid()\n        return y\n\n    def _attrs_from_xy(self):\n        \"\"\"Generate regular grid attributes from x, y coordinates.\"\"\"\n        self.ny, self.nx = self.x.shape\n        self.x0 = self.x[0, 0]\n        self.y0 = self.y[0, 0]\n        self.rot = np.degrees(\n            np.arctan2(self.y[0, 1] - self.y0, self.x[0, 1] - self.x0)\n        )\n        self.dx = np.sqrt((self.x[0, 1] - self.x0) ** 2 + (self.y[0, 1] - self.y0) ** 2)\n        self.dy = np.sqrt((self.x[1, 0] - self.x0) ** 2 + (self.y[1, 0] - self.y0) ** 2)\n\n    @property\n    def xlen(self):\n        return self.dx * (self.nx - 1)\n\n    @property\n    def ylen(self):\n        return self.dy * (self.ny - 1)\n\n    def _gen_reg_cgrid(self):\n        # Grid at origin\n        i = np.arange(0.0, self.dx * self.nx, self.dx)\n        j = np.arange(0.0, self.dy * self.ny, self.dy)\n        ii, jj = np.meshgrid(i, j)\n\n        # Rotation\n        alpha = -self.rot * np.pi / 180.0\n        R = np.array([[np.cos(alpha), -np.sin(alpha)], [np.sin(alpha), np.cos(alpha)]])\n        gg = np.dot(np.vstack([ii.ravel(), jj.ravel()]).T, R)\n\n        # Translation\n        x = gg[:, 0] + self.x0\n        y = gg[:, 1] + self.y0\n\n        x = np.reshape(x, ii.shape)\n        y = np.reshape(y, ii.shape)\n        return x, y\n\n    def __eq__(self, other) -&gt; bool:\n        return (\n            (self.nx == other.nx)\n            &amp; (self.ny == other.ny)\n            &amp; (self.rot == other.rot)\n            &amp; (self.x0 == other.x0)\n            &amp; (self.y0 == other.y0)\n            &amp; (self.dx == other.dx)\n            &amp; (self.dy == other.dy)\n        )\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.nx}, {self.ny})\"\n\n    def __str__(self):\n        return f\"{self.__class__.__name__}({self.nx}, {self.ny})\"\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.grid.RegularGrid.grid_type","title":"grid_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>grid_type: Literal['regular'] = Field('regular', description=\"Type of grid, must be 'regular'\")\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.x0","title":"x0  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>x0: Optional[float] = Field(default=None, description='X coordinate of the grid origin')\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.y0","title":"y0  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>y0: Optional[float] = Field(default=None, description='Y coordinate of the grid origin')\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.rot","title":"rot  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rot: Optional[float] = Field(0.0, description='Rotation angle of the grid in degrees')\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.dx","title":"dx  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dx: Optional[float] = Field(default=None, description='Spacing between grid points in the x direction')\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.dy","title":"dy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dy: Optional[float] = Field(default=None, description='Spacing between grid points in the y direction')\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.nx","title":"nx  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nx: Optional[int] = Field(default=None, description='Number of grid points in the x direction')\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.ny","title":"ny  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ny: Optional[int] = Field(default=None, description='Number of grid points in the y direction')\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.x","title":"x  <code>property</code>","text":"<pre><code>x: ndarray\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.y","title":"y  <code>property</code>","text":"<pre><code>y: ndarray\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.xlen","title":"xlen  <code>property</code>","text":"<pre><code>xlen\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid.ylen","title":"ylen  <code>property</code>","text":"<pre><code>ylen\n</code></pre>"},{"location":"core_concepts/#rompy.core.grid.RegularGrid-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.core.grid.RegularGrid.generate","title":"generate","text":"<pre><code>generate() -&gt; RegularGrid\n</code></pre> <p>Generate the grid from the provided parameters.</p> Source code in <code>rompy/core/grid.py</code> <pre><code>@model_validator(mode=\"after\")\ndef generate(self) -&gt; \"RegularGrid\":\n    \"\"\"Generate the grid from the provided parameters.\"\"\"\n    keys = [\"x0\", \"y0\", \"dx\", \"dy\", \"nx\", \"ny\"]\n    if None in [getattr(self, key) for key in keys]:\n        raise ValueError(f\"All of {','.join(keys)} must be provided for REG grid\")\n    # Ensure x, y 2D coordinates are defined\n    return self\n</code></pre>"},{"location":"core_concepts/#data","title":"Data","text":"<p>Data objects are used to represent data inputs into the model. Rompy provides the following base classes for data objects:</p>"},{"location":"core_concepts/#rompy.core.data.DataBlob","title":"DataBlob","text":"<p>               Bases: <code>DataBase</code></p> <p>Data source for model ingestion.</p> <p>Generic data source for files that either need to be copied to the model directory or linked if <code>link</code> is set to True. Supports both local/cloud paths and remote HTTP/HTTPS URLs.</p>"},{"location":"core_concepts/#rompy.core.data.DataBlob--parameters","title":"Parameters","text":"<p>source : Path | HttpUrl     URI of the data source, either a local file path, cloud storage URI (s3://, gs://),     or a remote HTTP/HTTPS URL. link : bool     Whether to create a symbolic link instead of copying the file.     Note: Cannot be used with HTTP URLs (link=True with HTTP will raise ValueError).</p>"},{"location":"core_concepts/#rompy.core.data.DataBlob--examples","title":"Examples","text":"<p>from rompy.core.data import DataBlob from pathlib import Path</p> <p>Local file:</p> <p>blob = DataBlob(source=Path(\"/data/input.nc\")) blob.get(Path(\"/output/dir\"))  # doctest: +ELLIPSIS PosixPath('/output/dir/input.nc')</p> <p>Remote HTTP file (automatically downloaded):</p> <p>blob = DataBlob(source=\"https://example.com/data.nc\") blob.get(Path(\"/output/dir\"))  # doctest: +ELLIPSIS PosixPath('/output/dir/data.nc')</p> <p>Remote file with custom name:</p> <p>blob = DataBlob(source=\"https://example.com/data.nc\") blob.get(Path(\"/output/dir\"), name=\"custom.nc\")  # doctest: +ELLIPSIS PosixPath('/output/dir/custom.nc')</p> Source code in <code>rompy/core/data.py</code> <pre><code>class DataBlob(DataBase):\n    \"\"\"Data source for model ingestion.\n\n    Generic data source for files that either need to be copied to the model directory\n    or linked if `link` is set to True. Supports both local/cloud paths and remote\n    HTTP/HTTPS URLs.\n\n    Parameters\n    ----------\n    source : Path | HttpUrl\n        URI of the data source, either a local file path, cloud storage URI (s3://, gs://),\n        or a remote HTTP/HTTPS URL.\n    link : bool\n        Whether to create a symbolic link instead of copying the file.\n        Note: Cannot be used with HTTP URLs (link=True with HTTP will raise ValueError).\n\n    Examples\n    --------\n    &gt;&gt;&gt; from rompy.core.data import DataBlob\n    &gt;&gt;&gt; from pathlib import Path\n\n    Local file:\n\n    &gt;&gt;&gt; blob = DataBlob(source=Path(\"/data/input.nc\"))\n    &gt;&gt;&gt; blob.get(Path(\"/output/dir\"))  # doctest: +ELLIPSIS\n    PosixPath('/output/dir/input.nc')\n\n    Remote HTTP file (automatically downloaded):\n\n    &gt;&gt;&gt; blob = DataBlob(source=\"https://example.com/data.nc\")\n    &gt;&gt;&gt; blob.get(Path(\"/output/dir\"))  # doctest: +ELLIPSIS\n    PosixPath('/output/dir/data.nc')\n\n    Remote file with custom name:\n\n    &gt;&gt;&gt; blob = DataBlob(source=\"https://example.com/data.nc\")\n    &gt;&gt;&gt; blob.get(Path(\"/output/dir\"), name=\"custom.nc\")  # doctest: +ELLIPSIS\n    PosixPath('/output/dir/custom.nc')\n\n    \"\"\"\n\n    model_type: Literal[\"data_blob\", \"data_link\"] = Field(\n        default=\"data_blob\",\n        description=\"Model type discriminator\",\n    )\n    source: Union[AnyPath, HttpUrl] = Field(\n        description=(\n            \"URI of the data source: local file path, cloud storage URI (s3://, gs://), \"\n            \"or remote HTTP/HTTPS URL. HTTP/HTTPS URLs are automatically downloaded.\"\n        ),\n    )\n    link: bool = Field(\n        default=False,\n        description=\"Whether to create a symbolic link instead of copying the file\",\n    )\n    _copied: str = PrivateAttr(default=None)\n\n    @field_validator(\"source\", mode=\"before\")\n    @classmethod\n    def validate_source(cls, v):\n        if isinstance(v, str) and (v.startswith(\"http://\") or v.startswith(\"https://\")):\n            return HttpUrl(v)\n        return v\n\n    @model_validator(mode=\"after\")\n    def validate_http_link_mode(self):\n        \"\"\"Validate that HTTP URLs cannot be used with link=True.\"\"\"\n        if isinstance(self.source, HttpUrl) and self.link:\n            raise ValueError(\n                \"Cannot use link=True with HTTP URLs. \"\n                \"HTTP sources must be downloaded (link=False).\"\n            )\n        return self\n\n    def get(self, destdir: Union[str, Path], name: str = None, *args, **kwargs) -&gt; Path:\n        \"\"\"Copy, download, or link the data source to a new directory.\n\n        For HTTP/HTTPS URLs, the file is automatically downloaded with retry logic.\n        For local/cloud paths, the file is either copied or symlinked based on the `link` attribute.\n\n        Parameters\n        ----------\n        destdir : str | Path\n            The destination directory to copy/download/link the data source to.\n        name : str, optional\n            Override the output filename. For HTTP downloads, this overrides the filename\n            extracted from the URL.\n\n        Returns\n        -------\n        Path\n            The path to the downloaded/copied file or created symlink.\n\n        Notes\n        -----\n        HTTP downloads include:\n        - Retry logic with exponential backoff (3 retries by default)\n        - Atomic writes (download to temp file, then rename)\n        - Caching (skip download if file already exists in destination)\n        - 500MB maximum file size limit\n\n        Raises\n        ------\n        ValueError\n            If link=True is used with an HTTP URL.\n        \"\"\"\n        destdir = Path(destdir).resolve()\n\n        # Handle HTTP URLs\n        if isinstance(self.source, HttpUrl):\n            outfile = download_http_file(\n                url=str(self.source), dest_dir=destdir, name=name\n            )\n            self._copied = str(outfile)\n            return outfile\n\n        # Handle local/cloud paths\n        if self.link:\n            # Create a symbolic link\n            if name:\n                symlink_path = destdir / name\n            else:\n                symlink_path = destdir / self.source.name\n\n            # Ensure the destination directory exists\n            destdir.mkdir(parents=True, exist_ok=True)\n\n            # Remove existing symlink/file if it exists\n            if symlink_path.exists():\n                symlink_path.unlink()\n\n            # Compute the relative path from destdir to self.source\n            relative_source_path = os.path.relpath(self.source.resolve(), destdir)\n\n            # Create symlink\n            os.symlink(relative_source_path, symlink_path)\n            self._copied = symlink_path\n\n            return symlink_path\n        else:\n            # Copy the data source\n            if self.source.is_dir():\n                # Copy directory\n                outfile = copytree(self.source, destdir)\n            else:\n                if name:\n                    outfile = destdir / name\n                else:\n                    outfile = destdir / self.source.name\n                if outfile.resolve() != self.source.resolve():\n                    outfile.write_bytes(self.source.read_bytes())\n            self._copied = outfile\n            return outfile\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataBlob-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.data.DataBlob.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['data_blob', 'data_link'] = Field(default='data_blob', description='Model type discriminator')\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataBlob.source","title":"source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source: Union[AnyPath, HttpUrl] = Field(description='URI of the data source: local file path, cloud storage URI (s3://, gs://), or remote HTTP/HTTPS URL. HTTP/HTTPS URLs are automatically downloaded.')\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataBlob.link","title":"link  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>link: bool = Field(default=False, description='Whether to create a symbolic link instead of copying the file')\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataBlob-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.core.data.DataBlob.validate_source","title":"validate_source  <code>classmethod</code>","text":"<pre><code>validate_source(v)\n</code></pre> Source code in <code>rompy/core/data.py</code> <pre><code>@field_validator(\"source\", mode=\"before\")\n@classmethod\ndef validate_source(cls, v):\n    if isinstance(v, str) and (v.startswith(\"http://\") or v.startswith(\"https://\")):\n        return HttpUrl(v)\n    return v\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataBlob.validate_http_link_mode","title":"validate_http_link_mode","text":"<pre><code>validate_http_link_mode()\n</code></pre> <p>Validate that HTTP URLs cannot be used with link=True.</p> Source code in <code>rompy/core/data.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_http_link_mode(self):\n    \"\"\"Validate that HTTP URLs cannot be used with link=True.\"\"\"\n    if isinstance(self.source, HttpUrl) and self.link:\n        raise ValueError(\n            \"Cannot use link=True with HTTP URLs. \"\n            \"HTTP sources must be downloaded (link=False).\"\n        )\n    return self\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataBlob.get","title":"get","text":"<pre><code>get(destdir: Union[str, Path], name: str = None, *args, **kwargs) -&gt; Path\n</code></pre> <p>Copy, download, or link the data source to a new directory.</p> <p>For HTTP/HTTPS URLs, the file is automatically downloaded with retry logic. For local/cloud paths, the file is either copied or symlinked based on the <code>link</code> attribute.</p>"},{"location":"core_concepts/#rompy.core.data.DataBlob.get--parameters","title":"Parameters","text":"<p>destdir : str | Path     The destination directory to copy/download/link the data source to. name : str, optional     Override the output filename. For HTTP downloads, this overrides the filename     extracted from the URL.</p>"},{"location":"core_concepts/#rompy.core.data.DataBlob.get--returns","title":"Returns","text":"<p>Path     The path to the downloaded/copied file or created symlink.</p>"},{"location":"core_concepts/#rompy.core.data.DataBlob.get--notes","title":"Notes","text":"<p>HTTP downloads include: - Retry logic with exponential backoff (3 retries by default) - Atomic writes (download to temp file, then rename) - Caching (skip download if file already exists in destination) - 500MB maximum file size limit</p>"},{"location":"core_concepts/#rompy.core.data.DataBlob.get--raises","title":"Raises","text":"<p>ValueError     If link=True is used with an HTTP URL.</p> Source code in <code>rompy/core/data.py</code> <pre><code>def get(self, destdir: Union[str, Path], name: str = None, *args, **kwargs) -&gt; Path:\n    \"\"\"Copy, download, or link the data source to a new directory.\n\n    For HTTP/HTTPS URLs, the file is automatically downloaded with retry logic.\n    For local/cloud paths, the file is either copied or symlinked based on the `link` attribute.\n\n    Parameters\n    ----------\n    destdir : str | Path\n        The destination directory to copy/download/link the data source to.\n    name : str, optional\n        Override the output filename. For HTTP downloads, this overrides the filename\n        extracted from the URL.\n\n    Returns\n    -------\n    Path\n        The path to the downloaded/copied file or created symlink.\n\n    Notes\n    -----\n    HTTP downloads include:\n    - Retry logic with exponential backoff (3 retries by default)\n    - Atomic writes (download to temp file, then rename)\n    - Caching (skip download if file already exists in destination)\n    - 500MB maximum file size limit\n\n    Raises\n    ------\n    ValueError\n        If link=True is used with an HTTP URL.\n    \"\"\"\n    destdir = Path(destdir).resolve()\n\n    # Handle HTTP URLs\n    if isinstance(self.source, HttpUrl):\n        outfile = download_http_file(\n            url=str(self.source), dest_dir=destdir, name=name\n        )\n        self._copied = str(outfile)\n        return outfile\n\n    # Handle local/cloud paths\n    if self.link:\n        # Create a symbolic link\n        if name:\n            symlink_path = destdir / name\n        else:\n            symlink_path = destdir / self.source.name\n\n        # Ensure the destination directory exists\n        destdir.mkdir(parents=True, exist_ok=True)\n\n        # Remove existing symlink/file if it exists\n        if symlink_path.exists():\n            symlink_path.unlink()\n\n        # Compute the relative path from destdir to self.source\n        relative_source_path = os.path.relpath(self.source.resolve(), destdir)\n\n        # Create symlink\n        os.symlink(relative_source_path, symlink_path)\n        self._copied = symlink_path\n\n        return symlink_path\n    else:\n        # Copy the data source\n        if self.source.is_dir():\n            # Copy directory\n            outfile = copytree(self.source, destdir)\n        else:\n            if name:\n                outfile = destdir / name\n            else:\n                outfile = destdir / self.source.name\n            if outfile.resolve() != self.source.resolve():\n                outfile.write_bytes(self.source.read_bytes())\n        self._copied = outfile\n        return outfile\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataGrid","title":"DataGrid","text":"<p>               Bases: <code>DataPoint</code></p> <p>Data object for gridded source data.</p> <p>Generic data object for xarray datasets that with gridded spatial dimensions</p>"},{"location":"core_concepts/#rompy.core.data.DataGrid--note","title":"Note","text":"<p>The fields <code>filter_grid</code> and <code>filter_time</code> trigger updates to the crop filter from the grid and time range objects passed to the get method. This is useful for data sources that are not defined on the same grid as the model grid or the same time range as the model run.</p> Source code in <code>rompy/core/data.py</code> <pre><code>class DataGrid(DataPoint):\n    \"\"\"Data object for gridded source data.\n\n    Generic data object for xarray datasets that with gridded spatial dimensions\n\n    Note\n    ----\n    The fields `filter_grid` and `filter_time` trigger updates to the crop filter from\n    the grid and time range objects passed to the get method. This is useful for data\n    sources that are not defined on the same grid as the model grid or the same time\n    range as the model run.\n\n    \"\"\"\n\n    model_type: Literal[\"grid\"] = Field(\n        default=\"grid\",\n        description=\"Model type discriminator\",\n    )\n    source: Union[SOURCE_TYPES] = Field(\n        description=\"Source reader, must return an xarray gridded dataset in the open method\",\n        discriminator=\"model_type\",\n    )\n\n    def _filter_grid(self, grid: GRID_TYPES):\n        \"\"\"Define the filters to use to extract data to this grid\"\"\"\n        x0, y0, x1, y1 = grid.bbox(buffer=self.buffer)\n        self.filter.crop.update(\n            {\n                self.coords.x: Slice(start=x0, stop=x1),\n                self.coords.y: Slice(start=y0, stop=y1),\n            }\n        )\n\n    def _figsize(self, x0, x1, y0, y1, fscale):\n        xlen = abs(x1 - x0)\n        ylen = abs(y1 - y0)\n        if xlen &gt;= ylen:\n            figsize = (fscale, (fscale * ylen / xlen or fscale) * 0.8)\n        else:\n            figsize = ((fscale * xlen / ylen) * 1.2 or fscale, fscale)\n        return figsize\n\n    def plot(\n        self,\n        param,\n        isel={},\n        model_grid=None,\n        cmap=\"turbo\",\n        figsize=None,\n        fscale=10,\n        borders=True,\n        land=True,\n        coastline=True,\n        **kwargs,\n    ):\n        \"\"\"Plot the grid.\"\"\"\n\n        projection = ccrs.PlateCarree()\n        transform = ccrs.PlateCarree()\n\n        # Sanity checks\n        try:\n            ds = self.ds[param].isel(isel)\n        except KeyError as err:\n            raise ValueError(f\"Parameter {param} not in dataset\") from err\n\n        if ds[self.coords.x].size &lt;= 1:\n            raise ValueError(f\"Cannot plot {param} with only one x coordinate\\n\\n{ds}\")\n        if ds[self.coords.y].size &lt;= 1:\n            raise ValueError(f\"Cannot plot {param} with only one y coordinate\\n\\n{ds}\")\n\n        # Set some plot parameters:\n        x0 = ds[self.coords.x].values[0]\n        y0 = ds[self.coords.y].values[0]\n        x1 = ds[self.coords.x].values[-1]\n        y1 = ds[self.coords.y].values[-1]\n\n        # create figure and plot/map\n        if figsize is None:\n            figsize = self._figsize(x0, x1, y0, y1, fscale)\n        fig = plt.figure(figsize=figsize)\n        ax = fig.add_subplot(111, projection=projection)\n\n        ds.plot.pcolormesh(ax=ax, cmap=cmap, **kwargs)\n\n        if borders:\n            ax.add_feature(cfeature.BORDERS)\n        if land:\n            ax.add_feature(cfeature.LAND, zorder=1)\n        if coastline:\n            ax.add_feature(cfeature.COASTLINE)\n\n        ax.gridlines(\n            crs=transform,\n            draw_labels=[\"left\", \"bottom\"],\n            linewidth=1,\n            color=\"gray\",\n            alpha=0.5,\n            linestyle=\"--\",\n        )\n\n        # Plot the model domain\n        if model_grid:\n            bx, by = model_grid.boundary_points()\n            poly = plt.Polygon(list(zip(bx, by)), facecolor=\"r\", alpha=0.05)\n            ax.add_patch(poly)\n            ax.plot(bx, by, lw=2, color=\"k\")\n        return fig, ax\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataGrid-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.data.DataGrid.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['grid'] = Field(default='grid', description='Model type discriminator')\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataGrid.source","title":"source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source: Union[SOURCE_TYPES] = Field(description='Source reader, must return an xarray gridded dataset in the open method', discriminator='model_type')\n</code></pre>"},{"location":"core_concepts/#rompy.core.data.DataGrid-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.core.data.DataGrid.plot","title":"plot","text":"<pre><code>plot(param, isel={}, model_grid=None, cmap='turbo', figsize=None, fscale=10, borders=True, land=True, coastline=True, **kwargs)\n</code></pre> <p>Plot the grid.</p> Source code in <code>rompy/core/data.py</code> <pre><code>def plot(\n    self,\n    param,\n    isel={},\n    model_grid=None,\n    cmap=\"turbo\",\n    figsize=None,\n    fscale=10,\n    borders=True,\n    land=True,\n    coastline=True,\n    **kwargs,\n):\n    \"\"\"Plot the grid.\"\"\"\n\n    projection = ccrs.PlateCarree()\n    transform = ccrs.PlateCarree()\n\n    # Sanity checks\n    try:\n        ds = self.ds[param].isel(isel)\n    except KeyError as err:\n        raise ValueError(f\"Parameter {param} not in dataset\") from err\n\n    if ds[self.coords.x].size &lt;= 1:\n        raise ValueError(f\"Cannot plot {param} with only one x coordinate\\n\\n{ds}\")\n    if ds[self.coords.y].size &lt;= 1:\n        raise ValueError(f\"Cannot plot {param} with only one y coordinate\\n\\n{ds}\")\n\n    # Set some plot parameters:\n    x0 = ds[self.coords.x].values[0]\n    y0 = ds[self.coords.y].values[0]\n    x1 = ds[self.coords.x].values[-1]\n    y1 = ds[self.coords.y].values[-1]\n\n    # create figure and plot/map\n    if figsize is None:\n        figsize = self._figsize(x0, x1, y0, y1, fscale)\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot(111, projection=projection)\n\n    ds.plot.pcolormesh(ax=ax, cmap=cmap, **kwargs)\n\n    if borders:\n        ax.add_feature(cfeature.BORDERS)\n    if land:\n        ax.add_feature(cfeature.LAND, zorder=1)\n    if coastline:\n        ax.add_feature(cfeature.COASTLINE)\n\n    ax.gridlines(\n        crs=transform,\n        draw_labels=[\"left\", \"bottom\"],\n        linewidth=1,\n        color=\"gray\",\n        alpha=0.5,\n        linestyle=\"--\",\n    )\n\n    # Plot the model domain\n    if model_grid:\n        bx, by = model_grid.boundary_points()\n        poly = plt.Polygon(list(zip(bx, by)), facecolor=\"r\", alpha=0.05)\n        ax.add_patch(poly)\n        ax.plot(bx, by, lw=2, color=\"k\")\n    return fig, ax\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceBase","title":"SourceBase","text":"<p>               Bases: <code>RompyBaseModel</code>, <code>ABC</code></p> <p>Abstract base class for a source dataset.</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceBase(RompyBaseModel, ABC):\n    \"\"\"Abstract base class for a source dataset.\"\"\"\n\n    model_type: Literal[\"base_source\"] = Field(\n        description=\"Model type discriminator, must be overriden by a subclass\",\n    )\n\n    @abstractmethod\n    def _open(self) -&gt; xr.Dataset:\n        \"\"\"This abstract private method should return a xarray dataset object.\"\"\"\n        pass\n\n    @cached_property\n    def coordinates(self) -&gt; xr.Dataset:\n        \"\"\"Return the coordinates of the datasource.\"\"\"\n        return self.open().coords\n\n    def open(self, variables: list = [], filters: Filter = {}, **kwargs) -&gt; xr.Dataset:\n        \"\"\"Return the filtered dataset object.\n\n        Parameters\n        ----------\n        variables : list, optional\n            List of variables to select from the dataset.\n        filters : Filter, optional\n            Filters to apply to the dataset.\n\n        Notes\n        -----\n        The kwargs are only a placeholder in case a subclass needs to pass additional\n        arguments to the open method.\n\n        \"\"\"\n        ds = self._open()\n        if variables:\n            try:\n                ds = ds[variables]\n            except KeyError as e:\n                dataset_variables = list(ds.data_vars.keys())\n                missing_variables = list(set(variables) - set(dataset_variables))\n                raise ValueError(\n                    f\"Cannot find requested variables in dataset.\\n\\n\"\n                    f\"Requested variables in the Data object: {variables}\\n\"\n                    f\"Available variables in source dataset: {dataset_variables}\\n\"\n                    f\"Missing variables: {missing_variables}\\n\\n\"\n                    f\"Please check:\\n\"\n                    f\"1. The variable names in your Data object, make sure you check for default values\\n\"\n                    f\"2. The data source contains the expected variables\\n\"\n                    f\"3. If using a custom data source, ensure it creates variables with the correct names\"\n                ) from e\n        if filters:\n            ds = filters(ds)\n        return ds\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceBase-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.source.SourceBase.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['base_source'] = Field(description='Model type discriminator, must be overriden by a subclass')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceBase.coordinates","title":"coordinates  <code>cached</code> <code>property</code>","text":"<pre><code>coordinates: Dataset\n</code></pre> <p>Return the coordinates of the datasource.</p>"},{"location":"core_concepts/#rompy.core.source.SourceBase-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.core.source.SourceBase.open","title":"open","text":"<pre><code>open(variables: list = [], filters: Filter = {}, **kwargs) -&gt; Dataset\n</code></pre> <p>Return the filtered dataset object.</p>"},{"location":"core_concepts/#rompy.core.source.SourceBase.open--parameters","title":"Parameters","text":"<p>variables : list, optional     List of variables to select from the dataset. filters : Filter, optional     Filters to apply to the dataset.</p>"},{"location":"core_concepts/#rompy.core.source.SourceBase.open--notes","title":"Notes","text":"<p>The kwargs are only a placeholder in case a subclass needs to pass additional arguments to the open method.</p> Source code in <code>rompy/core/source.py</code> <pre><code>def open(self, variables: list = [], filters: Filter = {}, **kwargs) -&gt; xr.Dataset:\n    \"\"\"Return the filtered dataset object.\n\n    Parameters\n    ----------\n    variables : list, optional\n        List of variables to select from the dataset.\n    filters : Filter, optional\n        Filters to apply to the dataset.\n\n    Notes\n    -----\n    The kwargs are only a placeholder in case a subclass needs to pass additional\n    arguments to the open method.\n\n    \"\"\"\n    ds = self._open()\n    if variables:\n        try:\n            ds = ds[variables]\n        except KeyError as e:\n            dataset_variables = list(ds.data_vars.keys())\n            missing_variables = list(set(variables) - set(dataset_variables))\n            raise ValueError(\n                f\"Cannot find requested variables in dataset.\\n\\n\"\n                f\"Requested variables in the Data object: {variables}\\n\"\n                f\"Available variables in source dataset: {dataset_variables}\\n\"\n                f\"Missing variables: {missing_variables}\\n\\n\"\n                f\"Please check:\\n\"\n                f\"1. The variable names in your Data object, make sure you check for default values\\n\"\n                f\"2. The data source contains the expected variables\\n\"\n                f\"3. If using a custom data source, ensure it creates variables with the correct names\"\n            ) from e\n    if filters:\n        ds = filters(ds)\n    return ds\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceFile","title":"SourceFile","text":"<p>               Bases: <code>SourceBase</code></p> <p>Source dataset from file to open with xarray.open_dataset.</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceFile(SourceBase):\n    \"\"\"Source dataset from file to open with xarray.open_dataset.\"\"\"\n\n    model_type: Literal[\"file\"] = Field(\n        default=\"file\",\n        description=\"Model type discriminator\",\n    )\n    uri: Union[str, Path] = Field(description=\"Path to the dataset\")\n    kwargs: dict = Field(\n        default={},\n        description=\"Keyword arguments to pass to xarray.open_dataset\",\n    )\n\n    variable: Optional[str] = Field(\n        default=None,\n        description=\"Variable to select from the dataset\",\n    )\n\n    # Enable arbitrary types for Path objects\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __str__(self) -&gt; str:\n        return f\"SourceFile(uri={self.uri})\"\n\n    def _open(self) -&gt; Union[xr.Dataset, xr.DataArray]:\n        # Handle Path objects by using str() to ensure compatibility\n        uri_str = str(self.uri) if isinstance(self.uri, Path) else self.uri\n        if self.variable:\n            # If a variable is specified, open the dataset and select the variable\n            return xr.open_dataset(uri_str, **self.kwargs)[self.variable]\n        else:\n            return xr.open_dataset(uri_str, **self.kwargs)\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceFile-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.source.SourceFile.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['file'] = Field(default='file', description='Model type discriminator')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceFile.uri","title":"uri  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uri: Union[str, Path] = Field(description='Path to the dataset')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceFile.kwargs","title":"kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kwargs: dict = Field(default={}, description='Keyword arguments to pass to xarray.open_dataset')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceFile.variable","title":"variable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>variable: Optional[str] = Field(default=None, description='Variable to select from the dataset')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceFile.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceIntake","title":"SourceIntake","text":"<p>               Bases: <code>SourceBase</code></p> <p>Source dataset from intake catalog.</p>"},{"location":"core_concepts/#rompy.core.source.SourceIntake--note","title":"note","text":"<p>The intake catalog can be prescribed either by the URI of an existing catalog file or by a YAML string defining the catalog. The YAML string can be obtained from calling the <code>yaml()</code> method on an intake dataset instance.</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceIntake(SourceBase):\n    \"\"\"Source dataset from intake catalog.\n\n    note\n    ----\n    The intake catalog can be prescribed either by the URI of an existing catalog file\n    or by a YAML string defining the catalog. The YAML string can be obtained from\n    calling the `yaml()` method on an intake dataset instance.\n\n    \"\"\"\n\n    model_type: Literal[\"intake\"] = Field(\n        default=\"intake\",\n        description=\"Model type discriminator\",\n    )\n    dataset_id: str = Field(description=\"The id of the dataset to read in the catalog\")\n    catalog_uri: Optional[str | Path] = Field(\n        default=None,\n        description=\"The URI of the catalog to read from\",\n    )\n    catalog_yaml: Optional[str] = Field(\n        default=None,\n        description=\"The YAML string of the catalog to read from\",\n    )\n    kwargs: dict = Field(\n        default={},\n        description=\"Keyword arguments to define intake dataset parameters\",\n    )\n\n    @model_validator(mode=\"after\")\n    def check_catalog(self) -&gt; \"SourceIntake\":\n        if self.catalog_uri is None and self.catalog_yaml is None:\n            raise ValueError(\"Either catalog_uri or catalog_yaml must be provided\")\n        elif self.catalog_uri is not None and self.catalog_yaml is not None:\n            raise ValueError(\"Only one of catalog_uri or catalog_yaml can be provided\")\n        return self\n\n    def __str__(self) -&gt; str:\n        return f\"SourceIntake(catalog_uri={self.catalog_uri}, dataset_id={self.dataset_id})\"\n\n    @property\n    def catalog(self) -&gt; Catalog:\n        \"\"\"The intake catalog instance.\"\"\"\n        if self.catalog_uri:\n            return intake.open_catalog(self.catalog_uri)\n        else:\n            fs = fsspec.filesystem(\"memory\")\n            fs_map = fs.get_mapper()\n            fs_map[\"/temp.yaml\"] = self.catalog_yaml.encode(\"utf-8\")\n            return YAMLFileCatalog(\"temp.yaml\", fs=fs)\n\n    def _open(self) -&gt; xr.Dataset:\n        return self.catalog[self.dataset_id](**self.kwargs).to_dask()\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceIntake-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.source.SourceIntake.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['intake'] = Field(default='intake', description='Model type discriminator')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceIntake.dataset_id","title":"dataset_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_id: str = Field(description='The id of the dataset to read in the catalog')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceIntake.catalog_uri","title":"catalog_uri  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>catalog_uri: Optional[str | Path] = Field(default=None, description='The URI of the catalog to read from')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceIntake.catalog_yaml","title":"catalog_yaml  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>catalog_yaml: Optional[str] = Field(default=None, description='The YAML string of the catalog to read from')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceIntake.kwargs","title":"kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kwargs: dict = Field(default={}, description='Keyword arguments to define intake dataset parameters')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceIntake.catalog","title":"catalog  <code>property</code>","text":"<pre><code>catalog: Catalog\n</code></pre> <p>The intake catalog instance.</p>"},{"location":"core_concepts/#rompy.core.source.SourceIntake-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.core.source.SourceIntake.check_catalog","title":"check_catalog","text":"<pre><code>check_catalog() -&gt; SourceIntake\n</code></pre> Source code in <code>rompy/core/source.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_catalog(self) -&gt; \"SourceIntake\":\n    if self.catalog_uri is None and self.catalog_yaml is None:\n        raise ValueError(\"Either catalog_uri or catalog_yaml must be provided\")\n    elif self.catalog_uri is not None and self.catalog_yaml is not None:\n        raise ValueError(\"Only one of catalog_uri or catalog_yaml can be provided\")\n    return self\n</code></pre>"},{"location":"core_concepts/#boundary","title":"Boundary","text":""},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation","title":"BoundaryWaveStation","text":"<p>               Bases: <code>DataBoundary</code></p> <p>Wave boundary data from station datasets.</p>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation--note","title":"Note","text":"<p>The <code>tolerance</code> behaves differently with sel_methods <code>idw</code> and <code>nearest</code>; in <code>idw</code> sites with no enough neighbours within <code>tolerance</code> are masked whereas in <code>nearest</code> an exception is raised (see wavespectra documentation for more details).</p>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation--note_1","title":"Note","text":"<p>Be aware that when using <code>idw</code> missing values will be returned for sites with less than 2 neighbours within <code>tolerance</code> in the original dataset. This is okay for land mask areas but could cause boundary issues when on an open boundary location. To avoid this either use <code>nearest</code> or increase <code>tolerance</code> to include more neighbours.</p> Source code in <code>rompy/core/boundary.py</code> <pre><code>class BoundaryWaveStation(DataBoundary):\n    \"\"\"Wave boundary data from station datasets.\n\n    Note\n    ----\n    The `tolerance` behaves differently with sel_methods `idw` and `nearest`; in `idw`\n    sites with no enough neighbours within `tolerance` are masked whereas in `nearest`\n    an exception is raised (see wavespectra documentation for more details).\n\n    Note\n    ----\n    Be aware that when using `idw` missing values will be returned for sites with less\n    than 2 neighbours within `tolerance` in the original dataset. This is okay for land\n    mask areas but could cause boundary issues when on an open boundary location. To\n    avoid this either use `nearest` or increase `tolerance` to include more neighbours.\n\n    \"\"\"\n\n    grid_type: Literal[\"boundary_wave_station\"] = Field(\n        default=\"boundary_wave_station\",\n        description=\"Model type discriminator\",\n    )\n    source: Union[SOURCE_TYPES] = Field(\n        description=(\n            \"Dataset source reader, must return a wavespectra-enabled \"\n            \"xarray dataset in the open method\"\n        ),\n        discriminator=\"model_type\",\n    )\n    sel_method: Literal[\"idw\", \"nearest\"] = Field(\n        default=\"idw\",\n        description=(\n            \"Wavespectra method to use for selecting boundary points from the dataset\"\n        ),\n    )\n    buffer: float = Field(\n        default=2.0,\n        description=\"Space to buffer the grid bounding box if `filter_grid` is True\",\n    )\n\n    def model_post_init(self, __context):\n        self.variables = [\"efth\", \"lon\", \"lat\"]\n\n    # @model_validator(mode=\"after\")\n    # def assert_has_wavespectra_accessor(self) -&gt; \"BoundaryWaveStation\":\n    #     dset = self.source.open()\n    #     if not hasattr(dset, \"spec\"):\n    #         raise ValueError(f\"Wavespectra compatible source is required\")\n    #     return self\n\n    def _source_grid_spacing(self, grid) -&gt; float:\n        \"\"\"Return the lowest spacing between points in the source dataset.\"\"\"\n        # Select dataset points just outside the actual grid to optimise the search\n        xbnd, ybnd = grid.boundary().exterior.coords.xy\n        dx = np.diff(xbnd).min()\n        dy = np.diff(ybnd).min()\n        buffer = 2 * min(dx, dy)\n        x0, y0, x1, y1 = grid.bbox(buffer=buffer)\n        ds = self.ds.spec.sel([x0, x1], [y0, y1], method=\"bbox\")\n        # Return the closest distance between adjacent points in cropped dataset\n        points = list(zip(ds.lon.values, ds.lat.values))\n        return find_minimum_distance(points)\n\n    def _set_spacing(self, grid) -&gt; float:\n        \"\"\"Define spacing from the parent dataset if required.\"\"\"\n        if self.spacing == \"parent\":\n            return self._source_grid_spacing(grid)\n        else:\n            return self.spacing\n\n    def _boundary_points(self, grid) -&gt; tuple:\n        \"\"\"Returns the x and y arrays representing the boundary points to select.\n\n        Override the default method to use grid when setting the default spacing.\n\n        \"\"\"\n        xbnd, ybnd = grid.boundary_points(spacing=self._set_spacing(grid))\n        return xbnd, ybnd\n\n    def _sel_boundary(self, grid) -&gt; xr.Dataset:\n        \"\"\"Select the boundary points from the dataset.\"\"\"\n        xbnd, ybnd = self._boundary_points(grid=grid)\n        ds = self.ds.spec.sel(\n            lons=xbnd,\n            lats=ybnd,\n            method=self.sel_method,\n            **self.sel_method_kwargs,\n        )\n        return ds\n\n    @property\n    def ds(self):\n        \"\"\"Return the filtered xarray dataset instance.\"\"\"\n        dset = super().ds\n        if dset.efth.size == 0:\n            raise ValueError(f\"Empty dataset after applying filter {self.filter}\")\n        return dset\n\n    def get(\n        self, destdir: str | Path, grid: RegularGrid, time: Optional[TimeRange] = None\n    ) -&gt; str:\n        \"\"\"Write the selected boundary data to a netcdf file.\n\n        Parameters\n        ----------\n        destdir : str | Path\n            Destination directory for the netcdf file.\n        grid : RegularGrid\n            Grid instance to use for selecting the boundary points.\n        time: TimeRange, optional\n            The times to filter the data to, only used if `self.crop_data` is True.\n\n        Returns\n        -------\n        outfile : Path\n            Path to the netcdf file.\n\n        \"\"\"\n        if self.crop_data:\n            if time is not None:\n                self._filter_time(time)\n            if grid is not None:\n                self._filter_grid(grid)\n        ds = self._sel_boundary(grid)\n        outfile = Path(destdir) / f\"{self.id}.nc\"\n        ds.spec.to_netcdf(outfile)\n        return outfile\n</code></pre>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation.grid_type","title":"grid_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>grid_type: Literal['boundary_wave_station'] = Field(default='boundary_wave_station', description='Model type discriminator')\n</code></pre>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation.source","title":"source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source: Union[SOURCE_TYPES] = Field(description='Dataset source reader, must return a wavespectra-enabled xarray dataset in the open method', discriminator='model_type')\n</code></pre>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation.sel_method","title":"sel_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sel_method: Literal['idw', 'nearest'] = Field(default='idw', description='Wavespectra method to use for selecting boundary points from the dataset')\n</code></pre>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation.buffer","title":"buffer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer: float = Field(default=2.0, description='Space to buffer the grid bounding box if `filter_grid` is True')\n</code></pre>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation.ds","title":"ds  <code>property</code>","text":"<pre><code>ds\n</code></pre> <p>Return the filtered xarray dataset instance.</p>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context)\n</code></pre> Source code in <code>rompy/core/boundary.py</code> <pre><code>def model_post_init(self, __context):\n    self.variables = [\"efth\", \"lon\", \"lat\"]\n</code></pre>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation.get","title":"get","text":"<pre><code>get(destdir: str | Path, grid: RegularGrid, time: Optional[TimeRange] = None) -&gt; str\n</code></pre> <p>Write the selected boundary data to a netcdf file.</p>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation.get--parameters","title":"Parameters","text":"<p>destdir : str | Path     Destination directory for the netcdf file. grid : RegularGrid     Grid instance to use for selecting the boundary points. time: TimeRange, optional     The times to filter the data to, only used if <code>self.crop_data</code> is True.</p>"},{"location":"core_concepts/#rompy.core.boundary.BoundaryWaveStation.get--returns","title":"Returns","text":"<p>outfile : Path     Path to the netcdf file.</p> Source code in <code>rompy/core/boundary.py</code> <pre><code>def get(\n    self, destdir: str | Path, grid: RegularGrid, time: Optional[TimeRange] = None\n) -&gt; str:\n    \"\"\"Write the selected boundary data to a netcdf file.\n\n    Parameters\n    ----------\n    destdir : str | Path\n        Destination directory for the netcdf file.\n    grid : RegularGrid\n        Grid instance to use for selecting the boundary points.\n    time: TimeRange, optional\n        The times to filter the data to, only used if `self.crop_data` is True.\n\n    Returns\n    -------\n    outfile : Path\n        Path to the netcdf file.\n\n    \"\"\"\n    if self.crop_data:\n        if time is not None:\n            self._filter_time(time)\n        if grid is not None:\n            self._filter_grid(grid)\n    ds = self._sel_boundary(grid)\n    outfile = Path(destdir) / f\"{self.id}.nc\"\n    ds.spec.to_netcdf(outfile)\n    return outfile\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceWavespectra","title":"SourceWavespectra","text":"<p>               Bases: <code>SourceBase</code></p> <p>Wavespectra dataset from wavespectra reader.</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceWavespectra(SourceBase):\n    \"\"\"Wavespectra dataset from wavespectra reader.\"\"\"\n\n    model_type: Literal[\"wavespectra\"] = Field(\n        default=\"wavespectra\",\n        description=\"Model type discriminator\",\n    )\n    uri: str | Path = Field(description=\"Path to the dataset\")\n    reader: str = Field(\n        description=\"Name of the wavespectra reader to use, e.g., read_swan\",\n    )\n    kwargs: dict = Field(\n        default={},\n        description=\"Keyword arguments to pass to the wavespectra reader\",\n    )\n\n    def __str__(self) -&gt; str:\n        return f\"SourceWavespectra(uri={self.uri}, reader={self.reader})\"\n\n    def _open(self):\n        return getattr(wavespectra, self.reader)(self.uri, **self.kwargs)\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceWavespectra-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.source.SourceWavespectra.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['wavespectra'] = Field(default='wavespectra', description='Model type discriminator')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceWavespectra.uri","title":"uri  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uri: str | Path = Field(description='Path to the dataset')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceWavespectra.reader","title":"reader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reader: str = Field(description='Name of the wavespectra reader to use, e.g., read_swan')\n</code></pre>"},{"location":"core_concepts/#rompy.core.source.SourceWavespectra.kwargs","title":"kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kwargs: dict = Field(default={}, description='Keyword arguments to pass to the wavespectra reader')\n</code></pre>"},{"location":"core_concepts/#spectrum","title":"Spectrum","text":""},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency","title":"LogFrequency","text":"<p>               Bases: <code>RompyBaseModel</code></p> <p>Logarithmic wave frequencies.</p> <p>Frequencies are defined according to:</p> <p>:math:<code>f_{i+1} = \\gamma * f_{i}</code></p>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency--note","title":"Note","text":"<p>The number of frequency bins <code>nbin</code> is always kept unchanged when provided. This implies other parameters may be adjusted so <code>nbin</code> bins can be defined. Specify <code>f0</code>, <code>f1</code> and <code>finc</code> and let <code>nbin</code> be calculated to avoid those values changing.</p>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency--note_1","title":"Note","text":"<p>Choose <code>finc=0.1</code> for a 10% increment between frequencies that satisfies the DIA.</p>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency--examples","title":"Examples","text":"<p>.. ipython:: python     :okwarning:</p> <pre><code>from rompy.core.spectrum import LogFrequency\n\nLogFrequency(f0=0.04, f1=1.0, nbin=34)\nLogFrequency(f0=0.04, f1=1.0, finc=0.1)\nLogFrequency(f0=0.04, nbin=34, finc=0.1)\nLogFrequency(f1=1.0, nbin=34, finc=0.1)\n</code></pre> Source code in <code>rompy/core/spectrum.py</code> <pre><code>class LogFrequency(RompyBaseModel):\n    \"\"\"Logarithmic wave frequencies.\n\n    Frequencies are defined according to:\n\n    :math:`f_{i+1} = \\gamma * f_{i}`\n\n    Note\n    ----\n    The number of frequency bins `nbin` is always kept unchanged when provided. This\n    implies other parameters may be adjusted so `nbin` bins can be defined. Specify\n    `f0`, `f1` and `finc` and let `nbin` be calculated to avoid those values changing.\n\n    Note\n    ----\n    Choose `finc=0.1` for a 10% increment between frequencies that satisfies the DIA.\n\n    Examples\n    --------\n\n    .. ipython:: python\n        :okwarning:\n\n        from rompy.core.spectrum import LogFrequency\n\n        LogFrequency(f0=0.04, f1=1.0, nbin=34)\n        LogFrequency(f0=0.04, f1=1.0, finc=0.1)\n        LogFrequency(f0=0.04, nbin=34, finc=0.1)\n        LogFrequency(f1=1.0, nbin=34, finc=0.1)\n\n    \"\"\"\n\n    model_type: Literal[\"log\", \"LOG\"] = Field(\n        default=\"log\", description=\"Model type discriminator\"\n    )\n    f0: Optional[float] = Field(\n        default=None, description=\"Lower frequency boundary (Hz)\", gt=0.0\n    )\n    f1: Optional[float] = Field(\n        default=None, description=\"Upper frequency boundary (Hz)\"\n    )\n    finc: Optional[float] = Field(\n        default=None, description=\"Log frequency increment\", gt=0.0\n    )\n    nbin: Optional[int] = Field(\n        default=None,\n        description=\"Number of frequency bins, one less the size of frequency array\",\n        gt=0,\n    )\n\n    @model_validator(mode=\"after\")\n    def init_options(self) -&gt; \"LogFrequency\":\n        \"\"\"Set the missing frequency parameters.\"\"\"\n        if sum([v is not None for v in [self.f0, self.f1, self.finc, self.nbin]]) != 3:\n            raise ValueError(\"Three (only) of (f0, f1, finc, nbin) must be provided\")\n\n        # Calculate the missing frequency parameters\n        if self.finc is None:\n            self.finc = self._finc()\n        elif self.nbin is None:\n            self.nbin = self._nbin(self.f0, self.f1, self.finc)\n        elif self.f1 is None:\n            self.f1 = self.f0 * self.gamma**self.nbin\n        else:\n            self.f0 = self._f0(self.f1, self.nbin, self.gamma)\n\n        # Redefine parameters based on the calculated values\n        self.f0 = self()[0]\n        self.f1 = self()[-1]\n        self.finc = self._finc()\n        self.nbin = len(self()) - 1\n\n        return self\n\n    def __call__(self) -&gt; Np1DArray:\n        \"\"\"Frequency array.\"\"\"\n        return np.geomspace(self.f0, self.f1, self.nf)\n\n    def __getitem__(self, index) -&gt; float | list[float]:\n        \"\"\"Slicing from the frequency array.\"\"\"\n        return self.__call__()[index]\n\n    def __len__(self):\n        \"\"\"Returns the length of the frequency array.\"\"\"\n        return len(self())\n\n    def _finc(self):\n        return (self()[1] - self()[0]) / self()[0]\n\n    def _nbin(self, f0, f1, finc):\n        return np.round(np.log(f1 / f0) / np.log(1 + finc)).astype(\"int\")\n\n    def _f0(self, f1, nbin, gamma):\n        \"\"\"Returns f0 given f1, nbin and gamma.\"\"\"\n        freqs = [f1]\n        for n in range(nbin):\n            freqs.append(freqs[-1] / gamma)\n        return freqs[-1]\n\n    @property\n    def nf(self):\n        return self.nbin + 1\n\n    @property\n    def gamma(self):\n        return self.finc + 1\n\n    @property\n    def flen(self):\n        return self.f1 - self.f0\n</code></pre>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['log', 'LOG'] = Field(default='log', description='Model type discriminator')\n</code></pre>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency.f0","title":"f0  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>f0: Optional[float] = Field(default=None, description='Lower frequency boundary (Hz)', gt=0.0)\n</code></pre>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency.f1","title":"f1  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>f1: Optional[float] = Field(default=None, description='Upper frequency boundary (Hz)')\n</code></pre>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency.finc","title":"finc  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>finc: Optional[float] = Field(default=None, description='Log frequency increment', gt=0.0)\n</code></pre>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency.nbin","title":"nbin  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nbin: Optional[int] = Field(default=None, description='Number of frequency bins, one less the size of frequency array', gt=0)\n</code></pre>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency.nf","title":"nf  <code>property</code>","text":"<pre><code>nf\n</code></pre>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency.gamma","title":"gamma  <code>property</code>","text":"<pre><code>gamma\n</code></pre>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency.flen","title":"flen  <code>property</code>","text":"<pre><code>flen\n</code></pre>"},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.core.spectrum.LogFrequency.init_options","title":"init_options","text":"<pre><code>init_options() -&gt; LogFrequency\n</code></pre> <p>Set the missing frequency parameters.</p> Source code in <code>rompy/core/spectrum.py</code> <pre><code>@model_validator(mode=\"after\")\ndef init_options(self) -&gt; \"LogFrequency\":\n    \"\"\"Set the missing frequency parameters.\"\"\"\n    if sum([v is not None for v in [self.f0, self.f1, self.finc, self.nbin]]) != 3:\n        raise ValueError(\"Three (only) of (f0, f1, finc, nbin) must be provided\")\n\n    # Calculate the missing frequency parameters\n    if self.finc is None:\n        self.finc = self._finc()\n    elif self.nbin is None:\n        self.nbin = self._nbin(self.f0, self.f1, self.finc)\n    elif self.f1 is None:\n        self.f1 = self.f0 * self.gamma**self.nbin\n    else:\n        self.f0 = self._f0(self.f1, self.nbin, self.gamma)\n\n    # Redefine parameters based on the calculated values\n    self.f0 = self()[0]\n    self.f1 = self()[-1]\n    self.finc = self._finc()\n    self.nbin = len(self()) - 1\n\n    return self\n</code></pre>"},{"location":"core_concepts/#model-run","title":"Model Run","text":""},{"location":"core_concepts/#rompy.model.ModelRun","title":"ModelRun","text":"<p>               Bases: <code>RompyBaseModel</code></p> <p>A model run.</p> <p>It is intented to be model agnostic. It deals primarily with how the model is to be run, i.e. the period of the run and where the output is going. The actual configuration of the run is provided by the config object.</p> <p>Further explanation is given in the rompy.core.Baseconfig docstring.</p> Source code in <code>rompy/model.py</code> <pre><code>class ModelRun(RompyBaseModel):\n    \"\"\"A model run.\n\n    It is intented to be model agnostic.\n    It deals primarily with how the model is to be run, i.e. the period of the run\n    and where the output is going. The actual configuration of the run is\n    provided by the config object.\n\n    Further explanation is given in the rompy.core.Baseconfig docstring.\n    \"\"\"\n\n    # Initialize formatting variables in __init__\n\n    model_type: Literal[\"modelrun\"] = Field(\"modelrun\", description=\"The model type.\")\n    run_id: str = Field(\"run_id\", description=\"The run id\")\n    period: TimeRange = Field(\n        TimeRange(\n            start=datetime(2020, 2, 21, 4),\n            end=datetime(2020, 2, 24, 4),\n            interval=\"15M\",\n        ),\n        description=\"The time period to run the model\",\n    )\n    output_dir: Path = Field(\"./simulations\", description=\"The output directory\")\n    config: Union[CONFIG_TYPES] = Field(\n        default_factory=BaseConfig,\n        description=\"The configuration object\",\n        discriminator=\"model_type\",\n    )\n    delete_existing: bool = Field(False, description=\"Delete existing output directory\")\n    run_id_subdir: bool = Field(\n        True, description=\"Use run_id subdirectory in the output directory\"\n    )\n    _datefmt: str = \"%Y%m%d.%H%M%S\"\n    _staging_dir: Path = None\n\n    @property\n    def staging_dir(self):\n        \"\"\"The directory where the model is staged for execution\n\n        returns\n        -------\n        staging_dir : str\n        \"\"\"\n\n        if self._staging_dir is None:\n            self._staging_dir = self._create_staging_dir()\n        return self._staging_dir\n\n    def _create_staging_dir(self):\n        if self.run_id_subdir:\n            odir = Path(self.output_dir) / self.run_id\n        else:\n            odir = Path(self.output_dir)\n        if self.delete_existing and odir.exists():\n            shutil.rmtree(odir)\n        odir.mkdir(parents=True, exist_ok=True)\n        return odir\n\n    @property\n    def _generation_medatadata(self):\n        return dict(\n            _generated_at=str(datetime.now(timezone.utc)),\n            _generated_by=os.environ.get(\"USER\"),\n            _generated_on=platform.node(),\n        )\n\n    def generate(self) -&gt; str:\n        \"\"\"Generate the model input files\n\n        returns\n        -------\n        staging_dir : str\n\n        \"\"\"\n        # Import formatting utilities\n        from rompy.formatting import format_table_row, log_box\n\n        # Format model settings in a structured way\n        config_type = type(self.config).__name__\n        duration = self.period.end - self.period.start\n        formatted_duration = self.period.format_duration(duration)\n\n        # Create table rows for the model run info\n        rows = [\n            format_table_row(\"Run ID\", str(self.run_id)),\n            format_table_row(\"Model Type\", config_type),\n            format_table_row(\"Start Time\", self.period.start.isoformat()),\n            format_table_row(\"End Time\", self.period.end.isoformat()),\n            format_table_row(\"Duration\", formatted_duration),\n            format_table_row(\"Time Interval\", str(self.period.interval)),\n            format_table_row(\"Output Directory\", str(self.output_dir)),\n        ]\n\n        # Add description if available\n        if hasattr(self.config, \"description\") and self.config.description:\n            rows.append(format_table_row(\"Description\", self.config.description))\n\n        # Create a formatted table with proper alignment\n        formatted_rows = []\n        key_lengths = []\n\n        # First pass: collect all valid rows and calculate max key length\n        for row in rows:\n            try:\n                # Split the row by the box-drawing vertical line character\n                parts = [p.strip() for p in row.split(\"\u2503\") if p.strip()]\n                if len(parts) &gt;= 2:  # We expect at least key and value parts\n                    key = parts[0].strip()\n                    value = parts[1].strip() if len(parts) &gt; 1 else \"\"\n                    key_lengths.append(len(key))\n                    formatted_rows.append((key, value))\n            except Exception as e:\n                logger.warning(f\"Error processing row '{row}': {e}\")\n\n        if not formatted_rows:\n            logger.warning(\"No valid rows found for model run configuration table\")\n            return self._staging_dir\n\n        max_key_len = max(key_lengths) if key_lengths else 0\n\n        # Format the rows with proper alignment\n        aligned_rows = []\n        for key, value in formatted_rows:\n            aligned_row = f\"{key:&gt;{max_key_len}} : {value}\"\n            aligned_rows.append(aligned_row)\n\n        # Log the box with the model run info\n        log_box(title=\"MODEL RUN CONFIGURATION\", logger=logger, add_empty_line=False)\n\n        # Log each row of the content with proper indentation\n        for row in aligned_rows:\n            logger.info(f\"  {row}\")\n\n        # Log the bottom of the box\n        log_box(\n            title=None,\n            logger=logger,\n            add_empty_line=True,  # Just the bottom border\n        )\n\n        # Display detailed configuration info using the new formatting framework\n        from rompy.formatting import log_box\n\n        # Create a box with the configuration type as title\n        log_box(f\"MODEL CONFIGURATION ({config_type})\")\n\n        # Use the model's string representation which now uses the new formatting\n        try:\n            # The __str__ method of RompyBaseModel already handles the formatting\n            config_str = str(self.config)\n            for line in config_str.split(\"\\n\"):\n                logger.info(line)\n        except Exception as e:\n            # If anything goes wrong with config formatting, log the error and minimal info\n            logger.info(f\"Using {type(self.config).__name__} configuration\")\n            logger.debug(f\"Configuration string formatting error: {str(e)}\")\n\n        logger.info(\"\")\n        log_box(\n            title=\"STARTING MODEL GENERATION\",\n            logger=logger,\n            add_empty_line=False,\n        )\n        logger.info(f\"Preparing input files in {self.output_dir}\")\n\n        # Collect context data\n        cc_full = {}\n        cc_full[\"runtime\"] = self.model_dump()\n        cc_full[\"runtime\"][\"staging_dir\"] = self.staging_dir\n        cc_full[\"runtime\"].update(self._generation_medatadata)\n        cc_full[\"runtime\"].update({\"_datefmt\": self._datefmt})\n\n        # Process configuration\n        logger.info(\"Processing model configuration...\")\n        if callable(self.config):\n            # Run the __call__() method of the config object if it is callable passing\n            # the runtime instance, and fill in the context with what is returned\n            logger.info(\"Running configuration callable...\")\n            cc_full[\"config\"] = self.config(self)\n        else:\n            # Otherwise just fill in the context with the config instance itself\n            logger.info(\"Using static configuration...\")\n            cc_full[\"config\"] = self.config\n\n        # Render templates\n        logger.info(f\"Rendering model templates to {self.output_dir}/{self.run_id}...\")\n        staging_dir = render(\n            cc_full, self.config.template, self.output_dir, self.config.checkout\n        )\n\n        logger.info(\"\")\n        # Use the log_box utility function\n        from rompy.formatting import log_box\n\n        log_box(\n            title=\"MODEL GENERATION COMPLETE\",\n            logger=logger,\n            add_empty_line=False,\n        )\n        logger.info(f\"Model files generated at: {staging_dir}\")\n        return staging_dir\n\n    def zip(self) -&gt; str:\n        \"\"\"Zip the input files for the model run\n\n        This function zips the input files for the model run and returns the\n        name of the zip file. It also cleans up the staging directory leaving\n        only the settings.json file that can be used to reproduce the run.\n\n        returns\n        -------\n        zip_fn : str\n        \"\"\"\n        # Use the log_box utility function\n        from rompy.formatting import log_box\n\n        log_box(\n            title=\"ARCHIVING MODEL FILES\",\n            logger=logger,\n        )\n\n        # Always remove previous zips\n        zip_fn = Path(str(self.staging_dir) + \".zip\")\n        if zip_fn.exists():\n            logger.info(f\"Removing existing archive at {zip_fn}\")\n            zip_fn.unlink()\n\n        # Count files to be archived\n        file_count = sum([len(fn) for _, _, fn in os.walk(self.staging_dir)])\n        logger.info(f\"Archiving {file_count} files from {self.staging_dir}\")\n\n        # Create zip archive\n        with zf.ZipFile(zip_fn, mode=\"w\", compression=zf.ZIP_DEFLATED) as z:\n            for dp, dn, fn in os.walk(self.staging_dir):\n                for filename in fn:\n                    source_path = os.path.join(dp, filename)\n                    rel_path = os.path.relpath(source_path, self.staging_dir)\n                    z.write(source_path, rel_path)\n\n        # Clean up staging directory\n        logger.info(f\"Cleaning up staging directory {self.staging_dir}\")\n        shutil.rmtree(self.staging_dir)\n\n        from rompy.formatting import log_box\n\n        log_box(\n            f\"\u2713 Archive created successfully: {zip_fn}\",\n            logger=logger,\n            add_empty_line=False,\n        )\n        return zip_fn\n\n    def __call__(self):\n        return self.generate()\n\n    def run(self, backend: BackendConfig, workspace_dir: Optional[str] = None) -&gt; bool:\n        \"\"\"\n        Run the model using the specified backend configuration.\n\n        This method uses Pydantic configuration objects that provide type safety\n        and validation for all backend parameters.\n\n        Args:\n            backend: Pydantic configuration object (LocalConfig, DockerConfig, etc.)\n            workspace_dir: Path to generated workspace directory (optional)\n\n        Returns:\n            True if execution was successful, False otherwise\n\n        Raises:\n            TypeError: If backend is not a BackendConfig instance\n\n        Examples:\n            from rompy.backends import LocalConfig, DockerConfig\n\n            # Local execution\n            model.run(LocalConfig(timeout=3600, command=\"python run.py\"))\n\n            # Docker execution\n            model.run(DockerConfig(image=\"swan:latest\", cpu=4, memory=\"2g\"))\n        \"\"\"\n        if not isinstance(backend, BaseBackendConfig):\n            raise TypeError(\n                f\"Backend must be a subclass of BaseBackendConfig, \"\n                f\"got {type(backend).__name__}\"\n            )\n\n        logger.debug(f\"Using backend config: {type(backend).__name__}\")\n\n        # Get the backend class directly from the configuration\n        backend_class = backend.get_backend_class()\n        backend_instance = backend_class()\n\n        # Pass the config object and workspace_dir to the backend\n        return backend_instance.run(self, config=backend, workspace_dir=workspace_dir)\n\n    def postprocess(self, processor: str = \"noop\", **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        Postprocess the model outputs using the specified processor.\n\n        This method uses entry points to load and execute the appropriate postprocessor.\n        Available processors are automatically discovered from the rompy.postprocess entry point group.\n\n        Built-in processors:\n        - \"noop\": A placeholder processor that does nothing but returns success\n\n        Args:\n            processor: Name of the postprocessor to use (default: \"noop\")\n            **kwargs: Additional processor-specific parameters\n\n        Returns:\n            Dictionary with results from the postprocessing\n\n        Raises:\n            ValueError: If the specified processor is not available\n        \"\"\"\n        # Get the requested postprocessor class from entry points\n        if processor not in POSTPROCESSORS:\n            available = list(POSTPROCESSORS.keys())\n            raise ValueError(\n                f\"Unknown postprocessor: {processor}. \"\n                f\"Available processors: {', '.join(available)}\"\n            )\n\n        # Create an instance and process the outputs\n        processor_class = POSTPROCESSORS[processor]\n        processor_instance = processor_class()\n        return processor_instance.process(self, **kwargs)\n\n    def pipeline(self, pipeline_backend: str = \"local\", **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"\n        Run the complete model pipeline (generate, run, postprocess) using the specified pipeline backend.\n\n        This method executes the entire model workflow from input generation through running\n        the model to postprocessing outputs. It uses entry points to load and execute the\n        appropriate pipeline backend from the rompy.pipeline entry point group.\n\n        Built-in pipeline backends:\n        - \"local\": Execute the complete pipeline locally using the existing ModelRun methods\n\n        Args:\n            pipeline_backend: Name of the pipeline backend to use (default: \"local\")\n            **kwargs: Additional backend-specific parameters. Common parameters include:\n                - run_backend: Backend to use for the run stage (for local pipeline)\n                - processor: Processor to use for postprocessing (for local pipeline)\n                - run_kwargs: Additional parameters for the run stage\n                - process_kwargs: Additional parameters for postprocessing\n\n        Returns:\n            Dictionary with results from the pipeline execution\n\n        Raises:\n            ValueError: If the specified pipeline backend is not available\n        \"\"\"\n        # Get the requested pipeline backend class from entry points\n        if pipeline_backend not in PIPELINE_BACKENDS:\n            available = list(PIPELINE_BACKENDS.keys())\n            raise ValueError(\n                f\"Unknown pipeline backend: {pipeline_backend}. \"\n                f\"Available backends: {', '.join(available)}\"\n            )\n\n        # Create an instance and execute the pipeline\n        backend_class = PIPELINE_BACKENDS[pipeline_backend]\n        backend_instance = backend_class()\n        return backend_instance.execute(self, **kwargs)\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun-attributes","title":"Attributes","text":""},{"location":"core_concepts/#rompy.model.ModelRun.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['modelrun'] = Field('modelrun', description='The model type.')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.run_id","title":"run_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>run_id: str = Field('run_id', description='The run id')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.period","title":"period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>period: TimeRange = Field(TimeRange(start=datetime(2020, 2, 21, 4), end=datetime(2020, 2, 24, 4), interval='15M'), description='The time period to run the model')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.output_dir","title":"output_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_dir: Path = Field('./simulations', description='The output directory')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: Union[CONFIG_TYPES] = Field(default_factory=BaseConfig, description='The configuration object', discriminator='model_type')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.delete_existing","title":"delete_existing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>delete_existing: bool = Field(False, description='Delete existing output directory')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.run_id_subdir","title":"run_id_subdir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>run_id_subdir: bool = Field(True, description='Use run_id subdirectory in the output directory')\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.staging_dir","title":"staging_dir  <code>property</code>","text":"<pre><code>staging_dir\n</code></pre> <p>The directory where the model is staged for execution</p>"},{"location":"core_concepts/#rompy.model.ModelRun.staging_dir--returns","title":"returns","text":"<p>staging_dir : str</p>"},{"location":"core_concepts/#rompy.model.ModelRun-functions","title":"Functions","text":""},{"location":"core_concepts/#rompy.model.ModelRun.generate","title":"generate","text":"<pre><code>generate() -&gt; str\n</code></pre> <p>Generate the model input files</p>"},{"location":"core_concepts/#rompy.model.ModelRun.generate--returns","title":"returns","text":"<p>staging_dir : str</p> Source code in <code>rompy/model.py</code> <pre><code>def generate(self) -&gt; str:\n    \"\"\"Generate the model input files\n\n    returns\n    -------\n    staging_dir : str\n\n    \"\"\"\n    # Import formatting utilities\n    from rompy.formatting import format_table_row, log_box\n\n    # Format model settings in a structured way\n    config_type = type(self.config).__name__\n    duration = self.period.end - self.period.start\n    formatted_duration = self.period.format_duration(duration)\n\n    # Create table rows for the model run info\n    rows = [\n        format_table_row(\"Run ID\", str(self.run_id)),\n        format_table_row(\"Model Type\", config_type),\n        format_table_row(\"Start Time\", self.period.start.isoformat()),\n        format_table_row(\"End Time\", self.period.end.isoformat()),\n        format_table_row(\"Duration\", formatted_duration),\n        format_table_row(\"Time Interval\", str(self.period.interval)),\n        format_table_row(\"Output Directory\", str(self.output_dir)),\n    ]\n\n    # Add description if available\n    if hasattr(self.config, \"description\") and self.config.description:\n        rows.append(format_table_row(\"Description\", self.config.description))\n\n    # Create a formatted table with proper alignment\n    formatted_rows = []\n    key_lengths = []\n\n    # First pass: collect all valid rows and calculate max key length\n    for row in rows:\n        try:\n            # Split the row by the box-drawing vertical line character\n            parts = [p.strip() for p in row.split(\"\u2503\") if p.strip()]\n            if len(parts) &gt;= 2:  # We expect at least key and value parts\n                key = parts[0].strip()\n                value = parts[1].strip() if len(parts) &gt; 1 else \"\"\n                key_lengths.append(len(key))\n                formatted_rows.append((key, value))\n        except Exception as e:\n            logger.warning(f\"Error processing row '{row}': {e}\")\n\n    if not formatted_rows:\n        logger.warning(\"No valid rows found for model run configuration table\")\n        return self._staging_dir\n\n    max_key_len = max(key_lengths) if key_lengths else 0\n\n    # Format the rows with proper alignment\n    aligned_rows = []\n    for key, value in formatted_rows:\n        aligned_row = f\"{key:&gt;{max_key_len}} : {value}\"\n        aligned_rows.append(aligned_row)\n\n    # Log the box with the model run info\n    log_box(title=\"MODEL RUN CONFIGURATION\", logger=logger, add_empty_line=False)\n\n    # Log each row of the content with proper indentation\n    for row in aligned_rows:\n        logger.info(f\"  {row}\")\n\n    # Log the bottom of the box\n    log_box(\n        title=None,\n        logger=logger,\n        add_empty_line=True,  # Just the bottom border\n    )\n\n    # Display detailed configuration info using the new formatting framework\n    from rompy.formatting import log_box\n\n    # Create a box with the configuration type as title\n    log_box(f\"MODEL CONFIGURATION ({config_type})\")\n\n    # Use the model's string representation which now uses the new formatting\n    try:\n        # The __str__ method of RompyBaseModel already handles the formatting\n        config_str = str(self.config)\n        for line in config_str.split(\"\\n\"):\n            logger.info(line)\n    except Exception as e:\n        # If anything goes wrong with config formatting, log the error and minimal info\n        logger.info(f\"Using {type(self.config).__name__} configuration\")\n        logger.debug(f\"Configuration string formatting error: {str(e)}\")\n\n    logger.info(\"\")\n    log_box(\n        title=\"STARTING MODEL GENERATION\",\n        logger=logger,\n        add_empty_line=False,\n    )\n    logger.info(f\"Preparing input files in {self.output_dir}\")\n\n    # Collect context data\n    cc_full = {}\n    cc_full[\"runtime\"] = self.model_dump()\n    cc_full[\"runtime\"][\"staging_dir\"] = self.staging_dir\n    cc_full[\"runtime\"].update(self._generation_medatadata)\n    cc_full[\"runtime\"].update({\"_datefmt\": self._datefmt})\n\n    # Process configuration\n    logger.info(\"Processing model configuration...\")\n    if callable(self.config):\n        # Run the __call__() method of the config object if it is callable passing\n        # the runtime instance, and fill in the context with what is returned\n        logger.info(\"Running configuration callable...\")\n        cc_full[\"config\"] = self.config(self)\n    else:\n        # Otherwise just fill in the context with the config instance itself\n        logger.info(\"Using static configuration...\")\n        cc_full[\"config\"] = self.config\n\n    # Render templates\n    logger.info(f\"Rendering model templates to {self.output_dir}/{self.run_id}...\")\n    staging_dir = render(\n        cc_full, self.config.template, self.output_dir, self.config.checkout\n    )\n\n    logger.info(\"\")\n    # Use the log_box utility function\n    from rompy.formatting import log_box\n\n    log_box(\n        title=\"MODEL GENERATION COMPLETE\",\n        logger=logger,\n        add_empty_line=False,\n    )\n    logger.info(f\"Model files generated at: {staging_dir}\")\n    return staging_dir\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.zip","title":"zip","text":"<pre><code>zip() -&gt; str\n</code></pre> <p>Zip the input files for the model run</p> <p>This function zips the input files for the model run and returns the name of the zip file. It also cleans up the staging directory leaving only the settings.json file that can be used to reproduce the run.</p>"},{"location":"core_concepts/#rompy.model.ModelRun.zip--returns","title":"returns","text":"<p>zip_fn : str</p> Source code in <code>rompy/model.py</code> <pre><code>def zip(self) -&gt; str:\n    \"\"\"Zip the input files for the model run\n\n    This function zips the input files for the model run and returns the\n    name of the zip file. It also cleans up the staging directory leaving\n    only the settings.json file that can be used to reproduce the run.\n\n    returns\n    -------\n    zip_fn : str\n    \"\"\"\n    # Use the log_box utility function\n    from rompy.formatting import log_box\n\n    log_box(\n        title=\"ARCHIVING MODEL FILES\",\n        logger=logger,\n    )\n\n    # Always remove previous zips\n    zip_fn = Path(str(self.staging_dir) + \".zip\")\n    if zip_fn.exists():\n        logger.info(f\"Removing existing archive at {zip_fn}\")\n        zip_fn.unlink()\n\n    # Count files to be archived\n    file_count = sum([len(fn) for _, _, fn in os.walk(self.staging_dir)])\n    logger.info(f\"Archiving {file_count} files from {self.staging_dir}\")\n\n    # Create zip archive\n    with zf.ZipFile(zip_fn, mode=\"w\", compression=zf.ZIP_DEFLATED) as z:\n        for dp, dn, fn in os.walk(self.staging_dir):\n            for filename in fn:\n                source_path = os.path.join(dp, filename)\n                rel_path = os.path.relpath(source_path, self.staging_dir)\n                z.write(source_path, rel_path)\n\n    # Clean up staging directory\n    logger.info(f\"Cleaning up staging directory {self.staging_dir}\")\n    shutil.rmtree(self.staging_dir)\n\n    from rompy.formatting import log_box\n\n    log_box(\n        f\"\u2713 Archive created successfully: {zip_fn}\",\n        logger=logger,\n        add_empty_line=False,\n    )\n    return zip_fn\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.run","title":"run","text":"<pre><code>run(backend: BackendConfig, workspace_dir: Optional[str] = None) -&gt; bool\n</code></pre> <p>Run the model using the specified backend configuration.</p> <p>This method uses Pydantic configuration objects that provide type safety and validation for all backend parameters.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>BackendConfig</code> <p>Pydantic configuration object (LocalConfig, DockerConfig, etc.)</p> required <code>workspace_dir</code> <code>Optional[str]</code> <p>Path to generated workspace directory (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if execution was successful, False otherwise</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If backend is not a BackendConfig instance</p> <p>Examples:</p> <p>from rompy.backends import LocalConfig, DockerConfig</p>"},{"location":"core_concepts/#rompy.model.ModelRun.run--local-execution","title":"Local execution","text":"<p>model.run(LocalConfig(timeout=3600, command=\"python run.py\"))</p>"},{"location":"core_concepts/#rompy.model.ModelRun.run--docker-execution","title":"Docker execution","text":"<p>model.run(DockerConfig(image=\"swan:latest\", cpu=4, memory=\"2g\"))</p> Source code in <code>rompy/model.py</code> <pre><code>def run(self, backend: BackendConfig, workspace_dir: Optional[str] = None) -&gt; bool:\n    \"\"\"\n    Run the model using the specified backend configuration.\n\n    This method uses Pydantic configuration objects that provide type safety\n    and validation for all backend parameters.\n\n    Args:\n        backend: Pydantic configuration object (LocalConfig, DockerConfig, etc.)\n        workspace_dir: Path to generated workspace directory (optional)\n\n    Returns:\n        True if execution was successful, False otherwise\n\n    Raises:\n        TypeError: If backend is not a BackendConfig instance\n\n    Examples:\n        from rompy.backends import LocalConfig, DockerConfig\n\n        # Local execution\n        model.run(LocalConfig(timeout=3600, command=\"python run.py\"))\n\n        # Docker execution\n        model.run(DockerConfig(image=\"swan:latest\", cpu=4, memory=\"2g\"))\n    \"\"\"\n    if not isinstance(backend, BaseBackendConfig):\n        raise TypeError(\n            f\"Backend must be a subclass of BaseBackendConfig, \"\n            f\"got {type(backend).__name__}\"\n        )\n\n    logger.debug(f\"Using backend config: {type(backend).__name__}\")\n\n    # Get the backend class directly from the configuration\n    backend_class = backend.get_backend_class()\n    backend_instance = backend_class()\n\n    # Pass the config object and workspace_dir to the backend\n    return backend_instance.run(self, config=backend, workspace_dir=workspace_dir)\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.postprocess","title":"postprocess","text":"<pre><code>postprocess(processor: str = 'noop', **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Postprocess the model outputs using the specified processor.</p> <p>This method uses entry points to load and execute the appropriate postprocessor. Available processors are automatically discovered from the rompy.postprocess entry point group.</p> <p>Built-in processors: - \"noop\": A placeholder processor that does nothing but returns success</p> <p>Parameters:</p> Name Type Description Default <code>processor</code> <code>str</code> <p>Name of the postprocessor to use (default: \"noop\")</p> <code>'noop'</code> <code>**kwargs</code> <p>Additional processor-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with results from the postprocessing</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified processor is not available</p> Source code in <code>rompy/model.py</code> <pre><code>def postprocess(self, processor: str = \"noop\", **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Postprocess the model outputs using the specified processor.\n\n    This method uses entry points to load and execute the appropriate postprocessor.\n    Available processors are automatically discovered from the rompy.postprocess entry point group.\n\n    Built-in processors:\n    - \"noop\": A placeholder processor that does nothing but returns success\n\n    Args:\n        processor: Name of the postprocessor to use (default: \"noop\")\n        **kwargs: Additional processor-specific parameters\n\n    Returns:\n        Dictionary with results from the postprocessing\n\n    Raises:\n        ValueError: If the specified processor is not available\n    \"\"\"\n    # Get the requested postprocessor class from entry points\n    if processor not in POSTPROCESSORS:\n        available = list(POSTPROCESSORS.keys())\n        raise ValueError(\n            f\"Unknown postprocessor: {processor}. \"\n            f\"Available processors: {', '.join(available)}\"\n        )\n\n    # Create an instance and process the outputs\n    processor_class = POSTPROCESSORS[processor]\n    processor_instance = processor_class()\n    return processor_instance.process(self, **kwargs)\n</code></pre>"},{"location":"core_concepts/#rompy.model.ModelRun.pipeline","title":"pipeline","text":"<pre><code>pipeline(pipeline_backend: str = 'local', **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Run the complete model pipeline (generate, run, postprocess) using the specified pipeline backend.</p> <p>This method executes the entire model workflow from input generation through running the model to postprocessing outputs. It uses entry points to load and execute the appropriate pipeline backend from the rompy.pipeline entry point group.</p> <p>Built-in pipeline backends: - \"local\": Execute the complete pipeline locally using the existing ModelRun methods</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_backend</code> <code>str</code> <p>Name of the pipeline backend to use (default: \"local\")</p> <code>'local'</code> <code>**kwargs</code> <p>Additional backend-specific parameters. Common parameters include: - run_backend: Backend to use for the run stage (for local pipeline) - processor: Processor to use for postprocessing (for local pipeline) - run_kwargs: Additional parameters for the run stage - process_kwargs: Additional parameters for postprocessing</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with results from the pipeline execution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified pipeline backend is not available</p> Source code in <code>rompy/model.py</code> <pre><code>def pipeline(self, pipeline_backend: str = \"local\", **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Run the complete model pipeline (generate, run, postprocess) using the specified pipeline backend.\n\n    This method executes the entire model workflow from input generation through running\n    the model to postprocessing outputs. It uses entry points to load and execute the\n    appropriate pipeline backend from the rompy.pipeline entry point group.\n\n    Built-in pipeline backends:\n    - \"local\": Execute the complete pipeline locally using the existing ModelRun methods\n\n    Args:\n        pipeline_backend: Name of the pipeline backend to use (default: \"local\")\n        **kwargs: Additional backend-specific parameters. Common parameters include:\n            - run_backend: Backend to use for the run stage (for local pipeline)\n            - processor: Processor to use for postprocessing (for local pipeline)\n            - run_kwargs: Additional parameters for the run stage\n            - process_kwargs: Additional parameters for postprocessing\n\n    Returns:\n        Dictionary with results from the pipeline execution\n\n    Raises:\n        ValueError: If the specified pipeline backend is not available\n    \"\"\"\n    # Get the requested pipeline backend class from entry points\n    if pipeline_backend not in PIPELINE_BACKENDS:\n        available = list(PIPELINE_BACKENDS.keys())\n        raise ValueError(\n            f\"Unknown pipeline backend: {pipeline_backend}. \"\n            f\"Available backends: {', '.join(available)}\"\n        )\n\n    # Create an instance and execute the pipeline\n    backend_class = PIPELINE_BACKENDS[pipeline_backend]\n    backend_instance = backend_class()\n    return backend_instance.execute(self, **kwargs)\n</code></pre>"},{"location":"demo/","title":"Demonstration Notebooks","text":""},{"location":"demo/#thumbnails-gallery","title":"Thumbnails Gallery","text":"<p>The following demonstration notebooks showcase various aspects of ROMPY's functionality:</p> <ul> <li>Demo Notebook - Basic demonstration of ROMPY features</li> <li>Templates Demo - Using cookiecutter templates for model setup</li> <li>SWAN Declarative Example - Declarative configuration of SWAN models</li> <li>SWAN Procedural Example - Procedural configuration of SWAN models</li> <li>SWAN Sensitivity Example - Sensitivity analysis with SWAN models</li> <li>Oceanum Example - Using Oceanum data sources</li> <li>SWAN Config Components - Detailed configuration components for SWAN</li> <li>Physics - Physics-based model configurations</li> <li>SCHISM Procedural - Procedural configuration of SCHISM models</li> </ul> <p>These notebooks provide practical examples of how to use ROMPY for various ocean modeling tasks. Each notebook demonstrates different aspects of the framework, from basic usage to advanced features.</p>"},{"location":"demo/#getting-started","title":"Getting Started","text":"<p>To run these notebooks:</p> <ol> <li> <p>Install ROMPY with notebook dependencies:    <pre><code>pip install rompy[notebooks]\n</code></pre></p> </li> <li> <p>Start Jupyter:    <pre><code>jupyter notebook\n</code></pre></p> </li> <li> <p>Open any of the demonstration notebooks and run the cells.</p> </li> </ol>"},{"location":"demo/#prerequisites","title":"Prerequisites","text":"<p>Some notebooks require additional dependencies:</p> <ul> <li>SWAN Notebooks: SWAN model installation</li> <li>SCHISM Notebooks: SCHISM model installation</li> <li>Oceanum Notebooks: Oceanum account and API key</li> </ul>"},{"location":"demo/#contributing-examples","title":"Contributing Examples","text":"<p>To contribute new demonstration notebooks:</p> <ol> <li>Create a new notebook in the appropriate directory</li> <li>Follow the existing structure and conventions</li> <li>Include clear explanations and comments</li> <li>Add the notebook to the gallery in this document</li> <li>Submit a pull request</li> </ol> <p>For more information on contributing, see contributing.</p>"},{"location":"formatting_and_logging/","title":"Formatting and Logging","text":""},{"location":"formatting_and_logging/#overview","title":"Overview","text":"<p>ROMPY provides a comprehensive framework for consistent formatting and logging across the codebase. This framework ensures that:</p> <ol> <li>Log messages are consistent and configurable</li> <li>String representations of objects are clear and hierarchical</li> <li>Output formatting is visually appealing and consistent</li> <li>Configuration is flexible and environment-aware</li> </ol>"},{"location":"formatting_and_logging/#core-components","title":"Core Components","text":"<p>The framework consists of several key components:</p> <ol> <li>Centralized Logging System</li> <li>Consistent log formatting and handling</li> <li>Environment variable configuration</li> <li> <p>Multiple log levels and output formats</p> </li> <li> <p>Hierarchical String Representation</p> </li> <li>Clean, readable output of complex objects</li> <li>Recursive handling of nested structures</li> <li> <p>Type-specific formatting</p> </li> <li> <p>Formatted Output</p> </li> <li>Boxes and visual elements</li> <li>Consistent headers and footers</li> <li>Progress indicators</li> </ol>"},{"location":"formatting_and_logging/#logging-system","title":"Logging System","text":"<p>ROMPY's logging system is built on Python's standard <code>logging</code> module but provides additional features and consistency.</p>"},{"location":"formatting_and_logging/#basic-usage","title":"Basic Usage","text":"<pre><code>from rompy.logging import get_logger\n\n# Get a logger for your module\nlogger = get_logger(__name__)\n\n# Log messages at different levels\nlogger.debug(\"Detailed debug information\")\nlogger.info(\"Informational message\")\nlogger.warning(\"Warning message\")\nlogger.error(\"Error message\")\nlogger.critical(\"Critical error\")\n</code></pre>"},{"location":"formatting_and_logging/#configuration","title":"Configuration","text":"<p>Logging can be configured via environment variables:</p> Variable Default Description <code>ROMPY_LOG_LEVEL</code> <code>INFO</code> Minimum log level (DEBUG, INFO, WARNING, ERROR, CRITICAL) <code>ROMPY_LOG_FORMAT</code> <code>detailed</code> Log format style (<code>simple</code> or <code>detailed</code>) <code>ROMPY_LOG_FILE</code> None Optional file path for log output <p>Programmatic configuration is also available:</p> <pre><code>from rompy.logging import LoggingConfig\n\n# Configure logging\nconfig = LoggingConfig()\nconfig.update(\n    level=\"DEBUG\",\n    format=\"verbose\",\n    log_dir=\"./logs\"\n)\n</code></pre>"},{"location":"formatting_and_logging/#hierarchical-string-representation","title":"Hierarchical String Representation","text":"<p>All ROMPY models include a hierarchical string representation for better readability of complex objects.</p>"},{"location":"formatting_and_logging/#basic-usage_1","title":"Basic Usage","text":"<pre><code>class MyModel(RompyBaseModel):\n    name: str\n    value: float\n    nested: dict\n\nobj = MyModel(name=\"test\", value=42.0, nested={\"a\": 1, \"b\": 2})\nprint(obj)\n</code></pre> <p>Output:</p> <pre><code>MyModel:\n  name: test\n  value: 42.0\n  nested:\n    a: 1\n    b: 2\n</code></pre>"},{"location":"formatting_and_logging/#custom-formatting","title":"Custom Formatting","text":"<p>Customize formatting by overriding the <code>_format_value</code> method:</p> <pre><code>class CustomModel(RompyBaseModel):\n    timestamp: datetime\n\n    def _format_value(self, obj: Any) -&gt; Optional[str]:\n        if isinstance(obj, datetime):\n            return obj.strftime(\"%Y-%m-%d %H:%M\")\n        return None\n</code></pre>"},{"location":"formatting_and_logging/#formatted-output","title":"Formatted Output","text":"<p>ROMPY provides utilities for creating consistent, visually appealing output.</p>"},{"location":"formatting_and_logging/#boxes-and-sections","title":"Boxes and Sections","text":"<pre><code>from rompy.formatting import get_formatted_box, log_box\n\n# Create a simple box\nprint(get_formatted_box(\"Important Message\"))\n\n# Create a section with content - using the logger\nfrom rompy.logging import get_logger\nlogger = get_logger(__name__)\nlog_box(\"Processing Results\", logger=logger)\n</code></pre>"},{"location":"formatting_and_logging/#progress-indicators","title":"Progress Indicators","text":"<pre><code>from rompy.logging import get_logger\nfrom rompy.formatting import log_horizontal_line\n\nlogger = get_logger(__name__)\n\n# Show progress with logging\nlogger.info(\"Starting processing...\")\nlog_horizontal_line(logger)\nfor i in range(100):\n    if i % 10 == 0:\n        logger.info(f\"Progress: {i}%\")\nlogger.success(\"Processing complete!\")\n</code></pre>"},{"location":"formatting_and_logging/#best-practices","title":"Best Practices","text":"<ol> <li>Logging</li> <li>Use appropriate log levels (DEBUG for detailed info, INFO for normal operations, etc.)</li> <li>Include relevant context in log messages</li> <li> <p>Use structured logging for machine-readable output</p> </li> <li> <p>String Representation</p> </li> <li>Keep string representations concise but informative</li> <li>Include all relevant attributes</li> <li> <p>Handle nested objects appropriately</p> </li> <li> <p>Formatting</p> </li> <li>Be consistent with formatting across the codebase</li> <li>Use the provided utilities for common formatting needs</li> <li>Consider readability in different output contexts (CLI, logs, etc.)</li> </ol>"},{"location":"formatting_and_logging/#example-integration","title":"Example Integration","text":"<p>Here's how these components work together in a typical ROMPY module:</p> <pre><code>from rompy.logging import get_logger\nfrom rompy.formatting import section\nfrom rompy.core.types import RompyBaseModel\n\nlogger = get_logger(__name__)\n\nclass DataProcessor(RompyBaseModel):\n    \"\"\"Process data with logging and formatted output.\"\"\"\n\n    def process(self, data):\n        logger.info(\"Starting data processing\")\n\n        with section(\"Processing Data\"):\n            # Process data here\n            logger.debug(f\"Processing {len(data)} items\")\n\n            # Log progress\n            for i, item in enumerate(data, 1):\n                self._process_item(item)\n                logger.debug(f\"Processed item {i}/{len(data)}\")\n\n        logger.info(\"Processing complete\")\n\n    def _process_item(self, item):\n        # Process individual items\n        pass\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>To install rompy, run:</p> <pre><code>pip install rompy\n</code></pre> <p>Or install from source:</p> <pre><code>git clone git@github.com:rom-py/rompy.git\ncd rompy\npip install -e .\n</code></pre>"},{"location":"models/","title":"Models","text":""},{"location":"models/#overview","title":"Overview","text":"<p>ROMPY provides model-specific implementations for various ocean, wave, and hydrodynamic models. Each model implementation includes configuration classes, grid definitions, data handling, and execution backends.</p>"},{"location":"models/#supported-models","title":"Supported Models","text":"<p>ROMPY currently supports the following models:</p> <ul> <li>SWAN - Spectral Wave Nearshore model</li> <li>SCHISM - Semi-implicit Cross-scale Hydroscience Integrated Modeling System</li> </ul>"},{"location":"models/#model-architecture","title":"Model Architecture","text":"<p>Each model implementation follows a consistent architecture:</p>"},{"location":"models/#configuration","title":"Configuration","text":"<p>Model-specific configuration classes that define model parameters, grid settings, and data sources.</p>"},{"location":"models/#grid","title":"Grid","text":"<p>Grid definitions that specify the model domain, resolution, and coordinate system.</p>"},{"location":"models/#data","title":"Data","text":"<p>Data handling classes for preparing model input data from various sources.</p>"},{"location":"models/#execution","title":"Execution","text":"<p>Backend configurations for running the model in different environments (local, Docker, HPC).</p>"},{"location":"models/#postprocessing","title":"Postprocessing","text":"<p>Classes for analyzing and visualizing model output.</p>"},{"location":"models/#extending-rompy","title":"Extending ROMPY","text":"<p>To add support for a new model:</p> <ol> <li>Create a new model package in the <code>rompy</code> namespace</li> <li>Implement the required base classes:</li> <li><code>BaseModel</code> - Model configuration and execution</li> <li><code>BaseGrid</code> - Grid definition and handling</li> <li><code>DataSource</code> - Data input handling</li> <li>Add backend support for execution environments</li> <li>Implement postprocessing capabilities</li> <li>Add documentation and examples</li> </ol>"},{"location":"models/#model-integration","title":"Model Integration","text":"<p>Models integrate with ROMPY's core framework through:</p> <ul> <li>Pydantic-based configuration classes for type safety</li> <li>XArray accessors for data manipulation</li> <li>Intake drivers for data catalog integration</li> <li>Cookiecutter templates for model setup</li> <li>Unified execution backends for consistent deployment</li> </ul>"},{"location":"models/#best-practices","title":"Best Practices","text":"<p>When working with models:</p> <ol> <li>Use Type Safety: Leverage Pydantic models for configuration validation</li> <li>Modular Design: Keep model components modular and reusable</li> <li>Documentation: Document model-specific parameters and usage</li> <li>Testing: Include comprehensive tests for model implementations</li> <li>Examples: Provide clear examples for common use cases</li> </ol>"},{"location":"models/#model-specific-documentation","title":"Model-Specific Documentation","text":"<p>For detailed information about each supported model, see:</p> <ul> <li>SWAN Model - Spectral Wave Nearshore model documentation</li> <li>SCHISM Model - Semi-implicit Cross-scale Hydroscience Integrated Modeling System documentation</li> </ul>"},{"location":"plugin_architecture/","title":"Execution and Output Processing Plugin Architecture","text":"<p>Rompy features a flexible plugin-based architecture that allows for extensible model execution and output processing. The system uses Python entry points to automatically discover and load backends, making it easy to extend with custom implementations.</p>"},{"location":"plugin_architecture/#overview","title":"Overview","text":"<p>The plugin architecture is built around three main categories:</p> <ol> <li>Run Backends (<code>rompy.run</code>): Handle model execution in different environments</li> <li>Postprocessors (<code>rompy.postprocess</code>): Handle model output analysis and transformation</li> <li>Pipeline Backends (<code>rompy.pipeline</code>): Orchestrate complete model workflows</li> </ol> <p>Each category uses Python entry points for automatic discovery and loading, allowing third-party packages to easily extend rompy's capabilities.</p>"},{"location":"plugin_architecture/#run-backends","title":"Run Backends","text":"<p>Run backends are responsible for executing models in different environments. They all implement a common interface with a <code>run()</code> method.</p>"},{"location":"plugin_architecture/#built-in-run-backends","title":"Built-in Run Backends","text":""},{"location":"plugin_architecture/#local-backend","title":"Local Backend","text":"<p>The <code>local</code> backend executes models directly on the local system:</p> <pre><code># Basic local execution\nsuccess = model.run(backend=\"local\")\n\n# With custom command\nsuccess = model.run(\n    backend=\"local\",\n    command=\"./my_model_executable\",\n    env_vars={\"OMP_NUM_THREADS\": \"4\"},\n    timeout=3600\n)\n</code></pre>"},{"location":"plugin_architecture/#docker-backend","title":"Docker Backend","text":"<p>The <code>docker</code> backend executes models inside Docker containers:</p> <pre><code># Using pre-built image\nsuccess = model.run(\n    backend=\"docker\",\n    image=\"rompy/schism:latest\",\n    executable=\"/usr/local/bin/schism\",\n    cpu=4,\n    volumes=[\"./data:/data:ro\"],\n    env_vars={\"MODEL_CONFIG\": \"production\"}\n)\n\n# Building from Dockerfile\nsuccess = model.run(\n    backend=\"docker\",\n    dockerfile=\"./docker/Dockerfile\",\n    build_args={\"MODEL_VERSION\": \"1.0.0\"},\n    executable=\"/usr/local/bin/model\",\n    mpiexec=\"mpiexec\",\n    cpu=8\n)\n</code></pre>"},{"location":"plugin_architecture/#custom-run-backends","title":"Custom Run Backends","text":"<p>You can create custom run backends by implementing the run interface:</p> <pre><code>class CustomRunBackend:\n    \"\"\"Custom run backend example.\"\"\"\n\n    def run(self, model_run, **kwargs):\n        \"\"\"Execute the model run.\n\n        Args:\n            model_run: The ModelRun instance\n            **kwargs: Backend-specific parameters\n\n        Returns:\n            bool: True if successful, False otherwise\n        \"\"\"\n        try:\n            # Generate model inputs\n            model_run.generate()\n\n            # Custom execution logic here\n            return self._execute_custom_logic(model_run, **kwargs)\n\n        except Exception as e:\n            logger.exception(f\"Custom backend failed: {e}\")\n            return False\n</code></pre> <p>Register custom backends via entry points in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"rompy.run\"]\ncustom = \"mypackage.backends:CustomRunBackend\"\n</code></pre>"},{"location":"plugin_architecture/#postprocessors","title":"Postprocessors","text":"<p>Postprocessors handle analysis and transformation of model outputs. They implement a <code>process()</code> method that returns a dictionary with results.</p>"},{"location":"plugin_architecture/#built-in-postprocessors","title":"Built-in Postprocessors","text":""},{"location":"plugin_architecture/#no-op-processor","title":"No-op Processor","text":"<p>The <code>noop</code> processor provides basic validation without processing:</p> <pre><code># Basic validation\nresults = model.postprocess(processor=\"noop\")\n\n# With custom validation\nresults = model.postprocess(\n    processor=\"noop\",\n    validate_outputs=True,\n    output_dir=\"./custom_output\"\n)\n</code></pre>"},{"location":"plugin_architecture/#custom-postprocessors","title":"Custom Postprocessors","text":"<p>Create custom postprocessors by implementing the process interface:</p> <pre><code>class AnalysisPostprocessor:\n    \"\"\"Custom postprocessor for model analysis.\"\"\"\n\n    def process(self, model_run, **kwargs):\n        \"\"\"Process model outputs.\n\n        Args:\n            model_run: The ModelRun instance\n            **kwargs: Processor-specific parameters\n\n        Returns:\n            dict: Processing results\n        \"\"\"\n        try:\n            output_dir = Path(model_run.output_dir) / model_run.run_id\n\n            # Custom analysis logic\n            metrics = self._calculate_metrics(output_dir)\n            plots = self._generate_plots(output_dir)\n\n            return {\n                \"success\": True,\n                \"metrics\": metrics,\n                \"plots\": plots,\n                \"message\": \"Analysis completed successfully\"\n            }\n\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"message\": f\"Analysis failed: {e}\"\n            }\n</code></pre> <p>Register via entry points:</p> <pre><code>[project.entry-points.\"rompy.postprocess\"]\nanalysis = \"mypackage.processors:AnalysisPostprocessor\"\n</code></pre>"},{"location":"plugin_architecture/#pipeline-backends","title":"Pipeline Backends","text":"<p>Pipeline backends orchestrate the complete model workflow from input generation through execution to output processing.</p>"},{"location":"plugin_architecture/#built-in-pipeline-backends","title":"Built-in Pipeline Backends","text":""},{"location":"plugin_architecture/#local-pipeline","title":"Local Pipeline","text":"<p>The <code>local</code> pipeline executes all stages locally:</p> <pre><code># Basic pipeline\nresults = model.pipeline(pipeline_backend=\"local\")\n\n# With custom backends\nresults = model.pipeline(\n    pipeline_backend=\"local\",\n    run_backend=\"docker\",\n    processor=\"analysis\",\n    run_kwargs={\"image\": \"rompy/model:latest\", \"cpu\": 4},\n    process_kwargs={\"create_plots\": True},\n    cleanup_on_failure=True\n)\n</code></pre>"},{"location":"plugin_architecture/#custom-pipeline-backends","title":"Custom Pipeline Backends","text":"<p>Create custom pipeline backends for distributed or cloud execution:</p> <pre><code>class CloudPipelineBackend:\n    \"\"\"Pipeline backend for cloud execution.\"\"\"\n\n    def execute(self, model_run, **kwargs):\n        \"\"\"Execute the complete pipeline.\n\n        Args:\n            model_run: The ModelRun instance\n            **kwargs: Pipeline-specific parameters\n\n        Returns:\n            dict: Pipeline execution results\n        \"\"\"\n        results = {\n            \"success\": False,\n            \"run_id\": model_run.run_id,\n            \"stages_completed\": []\n        }\n\n        try:\n            # Stage 1: Generate inputs\n            model_run.generate()\n            results[\"stages_completed\"].append(\"generate\")\n\n            # Stage 2: Submit to cloud\n            job_id = self._submit_cloud_job(model_run, **kwargs)\n            results[\"job_id\"] = job_id\n            results[\"stages_completed\"].append(\"submit\")\n\n            # Stage 3: Wait for completion\n            self._wait_for_completion(job_id)\n            results[\"stages_completed\"].append(\"execute\")\n\n            # Stage 4: Download and process results\n            outputs = self._download_results(job_id)\n            processed = self._process_outputs(outputs, **kwargs)\n            results[\"outputs\"] = processed\n            results[\"stages_completed\"].append(\"postprocess\")\n\n            results[\"success\"] = True\n            return results\n\n        except Exception as e:\n            results[\"error\"] = str(e)\n            return results\n</code></pre>"},{"location":"plugin_architecture/#best-practices","title":"Best Practices","text":""},{"location":"plugin_architecture/#error-handling","title":"Error Handling","text":"<ul> <li>Always wrap main logic in try-catch blocks</li> <li>Return appropriate boolean/dict responses</li> <li>Log errors with sufficient detail for debugging</li> <li>Clean up resources on failure when possible</li> </ul>"},{"location":"plugin_architecture/#parameter-validation","title":"Parameter Validation","text":"<ul> <li>Validate required parameters early</li> <li>Provide clear error messages for invalid inputs</li> <li>Use type hints for better IDE support</li> <li>Document all parameters in docstrings</li> </ul>"},{"location":"plugin_architecture/#logging","title":"Logging","text":"<ul> <li>Use structured logging with appropriate levels</li> <li>Include run_id and context in log messages</li> <li>Log progress for long-running operations</li> <li>Avoid logging sensitive information</li> </ul>"},{"location":"plugin_architecture/#resource-management","title":"Resource Management","text":"<ul> <li>Clean up temporary files and directories</li> <li>Handle timeouts gracefully</li> <li>Implement proper cancellation mechanisms</li> <li>Monitor resource usage for long-running processes</li> </ul>"},{"location":"plugin_architecture/#testing","title":"Testing","text":"<ul> <li>Write unit tests for all backend methods</li> <li>Mock external dependencies (Docker, cloud APIs)</li> <li>Test error conditions and edge cases</li> <li>Include integration tests where appropriate</li> </ul>"},{"location":"plugin_architecture/#examples","title":"Examples","text":"<p>Complete examples demonstrating the plugin architecture can be found in the <code>examples/backends/</code> directory:</p> <ul> <li><code>01_basic_local_run.py</code>: Simple local execution</li> <li><code>02_docker_run.py</code>: Docker container execution</li> <li><code>03_custom_postprocessor.py</code>: Custom output processing</li> <li><code>04_complete_workflow.py</code>: End-to-end custom workflow</li> </ul> <p>For interactive examples, see the <code>notebooks/backend_examples.ipynb</code> notebook.</p>"},{"location":"plugin_architecture/#api-reference","title":"API Reference","text":""},{"location":"plugin_architecture/#rompy.run","title":"run","text":"<p>Local execution backend for model runs.</p> <p>This module provides the local run backend implementation.</p>"},{"location":"plugin_architecture/#rompy.run-attributes","title":"Attributes","text":""},{"location":"plugin_architecture/#rompy.run.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"plugin_architecture/#rompy.run-classes","title":"Classes","text":""},{"location":"plugin_architecture/#rompy.run.LocalRunBackend","title":"LocalRunBackend","text":"<p>Execute models locally using the system's Python interpreter.</p> <p>This is the simplest backend that just runs the model directly on the local system.</p> Source code in <code>rompy/run/__init__.py</code> <pre><code>class LocalRunBackend:\n    \"\"\"Execute models locally using the system's Python interpreter.\n\n    This is the simplest backend that just runs the model directly\n    on the local system.\n    \"\"\"\n\n    def run(\n        self, model_run, config: \"LocalConfig\", workspace_dir: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Run the model locally.\n\n        Args:\n            model_run: The ModelRun instance to execute\n            config: LocalConfig instance with execution parameters\n            workspace_dir: Path to the generated workspace directory (if None, will generate)\n\n        Returns:\n            True if execution was successful, False otherwise\n\n        Raises:\n            ValueError: If model_run is invalid\n            TimeoutError: If execution exceeds timeout\n        \"\"\"\n        # Validate input parameters\n        if not model_run:\n            raise ValueError(\"model_run cannot be None\")\n\n        if not hasattr(model_run, \"run_id\"):\n            raise ValueError(\"model_run must have a run_id attribute\")\n\n        # Use config parameters\n        exec_command = config.command\n        exec_working_dir = config.working_dir\n        exec_env_vars = config.env_vars\n        exec_timeout = config.timeout\n        exec_stream_output = getattr(config, \"stream_output\", False)\n\n        logger.debug(\n            f\"Using LocalConfig: timeout={exec_timeout}, env_vars={list(exec_env_vars.keys())}\"\n        )\n\n        logger.info(f\"Starting local execution for run_id: {model_run.run_id}\")\n\n        try:\n            # Use provided workspace or generate if not provided (for backwards compatibility)\n            if workspace_dir is None:\n                logger.warning(\n                    \"No workspace_dir provided, generating files (this may cause double generation in pipeline)\"\n                )\n                staging_dir = model_run.generate()\n                logger.info(f\"Model inputs generated in: {staging_dir}\")\n            else:\n                logger.info(f\"Using provided workspace directory: {workspace_dir}\")\n                staging_dir = workspace_dir\n\n            # Set working directory\n            if exec_working_dir:\n                work_dir = Path(exec_working_dir)\n            else:\n                work_dir = (\n                    Path(staging_dir)\n                    if staging_dir\n                    else Path(model_run.output_dir) / model_run.run_id\n                )\n\n            if not work_dir.exists():\n                logger.error(f\"Working directory does not exist: {work_dir}\")\n                return False\n\n            # Prepare environment\n            env = os.environ.copy()\n            if exec_env_vars:\n                env.update(exec_env_vars)\n                logger.debug(\n                    f\"Added environment variables: {list(exec_env_vars.keys())}\"\n                )\n\n            # Execute command or config.run()\n            if exec_command:\n                success = self._execute_command(\n                    exec_command, work_dir, env, exec_timeout, exec_stream_output\n                )\n            else:\n                success = self._execute_config_run(model_run, work_dir, env)\n\n            if success:\n                logger.info(\n                    f\"Local execution completed successfully for run_id: {model_run.run_id}\"\n                )\n            else:\n                logger.error(f\"Local execution failed for run_id: {model_run.run_id}\")\n\n            return success\n\n        except TimeoutError:\n            logger.error(f\"Model execution timed out after {exec_timeout} seconds\")\n            raise\n        except Exception as e:\n            logger.exception(f\"Failed to run model locally: {e}\")\n            return False\n\n    def _execute_command(\n        self,\n        command: str,\n        work_dir: Path,\n        env: Dict[str, str],\n        timeout: Optional[int],\n        stream_output: bool = False,\n    ) -&gt; bool:\n        \"\"\"Execute a shell command.\n\n        Args:\n            command: Command to execute\n            work_dir: Working directory\n            env: Environment variables\n            timeout: Execution timeout\n            stream_output: Whether to stream output in real-time\n\n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        if stream_output:\n            return self._execute_command_streaming(command, work_dir, env, timeout)\n        else:\n            return self._execute_command_buffered(command, work_dir, env, timeout)\n\n    def _execute_command_buffered(\n        self, command: str, work_dir: Path, env: Dict[str, str], timeout: Optional[int]\n    ) -&gt; bool:\n        \"\"\"Execute a shell command with buffered output.\"\"\"\n        logger.info(f\"Executing command: {command}\")\n        logger.debug(f\"Working directory: {work_dir}\")\n\n        try:\n            result = subprocess.run(\n                command,\n                shell=True,\n                cwd=work_dir,\n                env=env,\n                timeout=timeout,\n                capture_output=True,\n                text=True,\n                check=False,\n            )\n\n            if result.stdout:\n                logger.info(f\"Command stdout:\\n{result.stdout}\")\n            if result.stderr:\n                if result.returncode == 0:\n                    logger.warning(f\"Command stderr:\\n{result.stderr}\")\n                else:\n                    logger.error(f\"Command stderr:\\n{result.stderr}\")\n\n            if result.returncode == 0:\n                logger.debug(\"Command completed successfully\")\n                return True\n            else:\n                logger.error(f\"Command failed with return code: {result.returncode}\")\n                return False\n\n        except subprocess.TimeoutExpired:\n            logger.error(f\"Command timed out after {timeout} seconds\")\n            raise TimeoutError(f\"Command execution timed out after {timeout} seconds\")\n        except Exception as e:\n            logger.exception(f\"Error executing command: {e}\")\n            return False\n\n    def _execute_command_streaming(\n        self, command: str, work_dir: Path, env: Dict[str, str], timeout: Optional[int]\n    ) -&gt; bool:\n        \"\"\"Execute a shell command with streaming output.\"\"\"\n        logger.info(f\"Executing command: {command}\")\n        logger.debug(f\"Working directory: {work_dir}\")\n\n        try:\n            process = subprocess.Popen(\n                command,\n                shell=True,\n                cwd=work_dir,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                bufsize=1,\n            )\n\n            # Capture output while streaming\n            stdout_lines = []\n            stderr_lines = []\n\n            def read_stream(stream, lines, log_func):\n                \"\"\"Read from a stream and log each line.\"\"\"\n                for line in stream:\n                    line = line.rstrip()\n                    lines.append(line)\n                    log_func(line)\n\n            # Start threads to read stdout and stderr concurrently\n            stdout_thread = threading.Thread(\n                target=read_stream,\n                args=(process.stdout, stdout_lines, logger.info),\n            )\n            stderr_thread = threading.Thread(\n                target=read_stream,\n                args=(process.stderr, stderr_lines, lambda msg: logger.warning(msg)),\n            )\n\n            stdout_thread.start()\n            stderr_thread.start()\n\n            # Wait for process to complete with timeout\n            try:\n                returncode = process.wait(timeout=timeout)\n            except subprocess.TimeoutExpired:\n                process.kill()\n                process.wait()\n                logger.error(f\"Command timed out after {timeout} seconds\")\n                raise TimeoutError(\n                    f\"Command execution timed out after {timeout} seconds\"\n                )\n\n            # Wait for reader threads to finish\n            stdout_thread.join()\n            stderr_thread.join()\n\n            # Log remaining stderr if any (after process completed)\n            # (Thread already handled most output, but capture any final lines)\n\n            if returncode == 0:\n                logger.debug(\"Command completed successfully\")\n                return True\n            else:\n                logger.error(f\"Command failed with return code: {returncode}\")\n                if stderr_lines:\n                    logger.error(\"Command stderr:\\n\" + \"\\n\".join(stderr_lines))\n                return False\n\n        except Exception as e:\n            logger.exception(f\"Error executing command: {e}\")\n            return False\n\n    def _execute_config_run(\n        self, model_run, work_dir: Path, env: Dict[str, str]\n    ) -&gt; bool:\n        \"\"\"Execute the model using config.run() method.\n\n        Args:\n            model_run: The ModelRun instance\n            work_dir: Working directory\n            env: Environment variables\n\n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        # Check if config has a run method\n        if not hasattr(model_run.config, \"run\") or not callable(model_run.config.run):\n            logger.warning(\n                \"Model config does not have a run method. Nothing to execute.\"\n            )\n            return True\n\n        logger.info(\"Executing model using config.run() method\")\n\n        try:\n            # Set working directory in environment for config.run()\n            original_cwd = os.getcwd()\n            os.chdir(work_dir)\n\n            # Update environment\n            original_env = {}\n            for key, value in env.items():\n                if key in os.environ:\n                    original_env[key] = os.environ[key]\n                os.environ[key] = value\n\n            try:\n                # Execute the config run method\n                result = model_run.config.run(model_run)\n\n                if isinstance(result, bool):\n                    return result\n                else:\n                    logger.warning(f\"config.run() returned non-boolean value: {result}\")\n                    return True\n\n            finally:\n                # Restore original environment and directory\n                os.chdir(original_cwd)\n                for key, value in env.items():\n                    if key in original_env:\n                        os.environ[key] = original_env[key]\n                    else:\n                        os.environ.pop(key, None)\n\n        except Exception as e:\n            logger.exception(f\"Error in config.run(): {e}\")\n            return False\n</code></pre>"},{"location":"plugin_architecture/#rompy.run.LocalRunBackend-functions","title":"Functions","text":""},{"location":"plugin_architecture/#rompy.run.LocalRunBackend.run","title":"run","text":"<pre><code>run(model_run, config: LocalConfig, workspace_dir: Optional[str] = None) -&gt; bool\n</code></pre> <p>Run the model locally.</p> <p>Parameters:</p> Name Type Description Default <code>model_run</code> <p>The ModelRun instance to execute</p> required <code>config</code> <code>LocalConfig</code> <p>LocalConfig instance with execution parameters</p> required <code>workspace_dir</code> <code>Optional[str]</code> <p>Path to the generated workspace directory (if None, will generate)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if execution was successful, False otherwise</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_run is invalid</p> <code>TimeoutError</code> <p>If execution exceeds timeout</p> Source code in <code>rompy/run/__init__.py</code> <pre><code>def run(\n    self, model_run, config: \"LocalConfig\", workspace_dir: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Run the model locally.\n\n    Args:\n        model_run: The ModelRun instance to execute\n        config: LocalConfig instance with execution parameters\n        workspace_dir: Path to the generated workspace directory (if None, will generate)\n\n    Returns:\n        True if execution was successful, False otherwise\n\n    Raises:\n        ValueError: If model_run is invalid\n        TimeoutError: If execution exceeds timeout\n    \"\"\"\n    # Validate input parameters\n    if not model_run:\n        raise ValueError(\"model_run cannot be None\")\n\n    if not hasattr(model_run, \"run_id\"):\n        raise ValueError(\"model_run must have a run_id attribute\")\n\n    # Use config parameters\n    exec_command = config.command\n    exec_working_dir = config.working_dir\n    exec_env_vars = config.env_vars\n    exec_timeout = config.timeout\n    exec_stream_output = getattr(config, \"stream_output\", False)\n\n    logger.debug(\n        f\"Using LocalConfig: timeout={exec_timeout}, env_vars={list(exec_env_vars.keys())}\"\n    )\n\n    logger.info(f\"Starting local execution for run_id: {model_run.run_id}\")\n\n    try:\n        # Use provided workspace or generate if not provided (for backwards compatibility)\n        if workspace_dir is None:\n            logger.warning(\n                \"No workspace_dir provided, generating files (this may cause double generation in pipeline)\"\n            )\n            staging_dir = model_run.generate()\n            logger.info(f\"Model inputs generated in: {staging_dir}\")\n        else:\n            logger.info(f\"Using provided workspace directory: {workspace_dir}\")\n            staging_dir = workspace_dir\n\n        # Set working directory\n        if exec_working_dir:\n            work_dir = Path(exec_working_dir)\n        else:\n            work_dir = (\n                Path(staging_dir)\n                if staging_dir\n                else Path(model_run.output_dir) / model_run.run_id\n            )\n\n        if not work_dir.exists():\n            logger.error(f\"Working directory does not exist: {work_dir}\")\n            return False\n\n        # Prepare environment\n        env = os.environ.copy()\n        if exec_env_vars:\n            env.update(exec_env_vars)\n            logger.debug(\n                f\"Added environment variables: {list(exec_env_vars.keys())}\"\n            )\n\n        # Execute command or config.run()\n        if exec_command:\n            success = self._execute_command(\n                exec_command, work_dir, env, exec_timeout, exec_stream_output\n            )\n        else:\n            success = self._execute_config_run(model_run, work_dir, env)\n\n        if success:\n            logger.info(\n                f\"Local execution completed successfully for run_id: {model_run.run_id}\"\n            )\n        else:\n            logger.error(f\"Local execution failed for run_id: {model_run.run_id}\")\n\n        return success\n\n    except TimeoutError:\n        logger.error(f\"Model execution timed out after {exec_timeout} seconds\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Failed to run model locally: {e}\")\n        return False\n</code></pre>"},{"location":"plugin_architecture/#rompy.postprocess","title":"postprocess","text":"<p>No-op postprocessor for model outputs.</p> <p>This module provides a basic postprocessor that does nothing.</p>"},{"location":"plugin_architecture/#rompy.postprocess-attributes","title":"Attributes","text":""},{"location":"plugin_architecture/#rompy.postprocess.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"plugin_architecture/#rompy.postprocess-classes","title":"Classes","text":""},{"location":"plugin_architecture/#rompy.postprocess.NoopPostprocessor","title":"NoopPostprocessor","text":"<p>A postprocessor that does nothing.</p> <p>This is a placeholder implementation that simply returns a success message. It's useful as a base class or for testing.</p> Source code in <code>rompy/postprocess/__init__.py</code> <pre><code>class NoopPostprocessor:\n    \"\"\"A postprocessor that does nothing.\n\n    This is a placeholder implementation that simply returns a success message.\n    It's useful as a base class or for testing.\n    \"\"\"\n\n    def process(\n        self,\n        model_run,\n        validate_outputs: bool = True,\n        output_dir: Optional[Union[str, Path]] = None,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Process the output of a model run (does nothing).\n\n        Args:\n            model_run: The ModelRun instance whose outputs to process\n            validate_outputs: Whether to validate that output directory exists\n            output_dir: Override output directory to check (defaults to model_run output)\n            **kwargs: Additional parameters (unused)\n\n        Returns:\n            Dictionary with processing results\n\n        Raises:\n            ValueError: If model_run is invalid\n        \"\"\"\n        # Validate input parameters\n        if not model_run:\n            raise ValueError(\"model_run cannot be None\")\n\n        if not hasattr(model_run, \"run_id\"):\n            raise ValueError(\"model_run must have a run_id attribute\")\n\n        logger.info(f\"Starting no-op postprocessing for run_id: {model_run.run_id}\")\n\n        try:\n            # Determine output directory\n            if output_dir:\n                check_dir = Path(output_dir)\n            else:\n                check_dir = Path(model_run.output_dir) / model_run.run_id\n\n            # Validate outputs if requested\n            if validate_outputs:\n                if not check_dir.exists():\n                    logger.warning(f\"Output directory does not exist: {check_dir}\")\n                    return {\n                        \"success\": False,\n                        \"message\": f\"Output directory not found: {check_dir}\",\n                        \"run_id\": model_run.run_id,\n                        \"output_dir\": str(check_dir),\n                    }\n                else:\n                    # Count files in output directory\n                    file_count = sum(1 for f in check_dir.rglob(\"*\") if f.is_file())\n                    logger.info(f\"Found {file_count} output files in {check_dir}\")\n\n            logger.info(\n                f\"No-op postprocessing completed for run_id: {model_run.run_id}\"\n            )\n\n            return {\n                \"success\": True,\n                \"message\": \"No postprocessing requested - validation only\",\n                \"run_id\": model_run.run_id,\n                \"output_dir\": str(check_dir),\n                \"validated\": validate_outputs,\n            }\n\n        except Exception as e:\n            logger.exception(f\"Error in no-op postprocessor: {e}\")\n            return {\n                \"success\": False,\n                \"message\": f\"Error in postprocessor: {str(e)}\",\n                \"run_id\": getattr(model_run, \"run_id\", \"unknown\"),\n                \"error\": str(e),\n            }\n</code></pre>"},{"location":"plugin_architecture/#rompy.postprocess.NoopPostprocessor-functions","title":"Functions","text":""},{"location":"plugin_architecture/#rompy.postprocess.NoopPostprocessor.process","title":"process","text":"<pre><code>process(model_run, validate_outputs: bool = True, output_dir: Optional[Union[str, Path]] = None, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Process the output of a model run (does nothing).</p> <p>Parameters:</p> Name Type Description Default <code>model_run</code> <p>The ModelRun instance whose outputs to process</p> required <code>validate_outputs</code> <code>bool</code> <p>Whether to validate that output directory exists</p> <code>True</code> <code>output_dir</code> <code>Optional[Union[str, Path]]</code> <p>Override output directory to check (defaults to model_run output)</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters (unused)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with processing results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_run is invalid</p> Source code in <code>rompy/postprocess/__init__.py</code> <pre><code>def process(\n    self,\n    model_run,\n    validate_outputs: bool = True,\n    output_dir: Optional[Union[str, Path]] = None,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"Process the output of a model run (does nothing).\n\n    Args:\n        model_run: The ModelRun instance whose outputs to process\n        validate_outputs: Whether to validate that output directory exists\n        output_dir: Override output directory to check (defaults to model_run output)\n        **kwargs: Additional parameters (unused)\n\n    Returns:\n        Dictionary with processing results\n\n    Raises:\n        ValueError: If model_run is invalid\n    \"\"\"\n    # Validate input parameters\n    if not model_run:\n        raise ValueError(\"model_run cannot be None\")\n\n    if not hasattr(model_run, \"run_id\"):\n        raise ValueError(\"model_run must have a run_id attribute\")\n\n    logger.info(f\"Starting no-op postprocessing for run_id: {model_run.run_id}\")\n\n    try:\n        # Determine output directory\n        if output_dir:\n            check_dir = Path(output_dir)\n        else:\n            check_dir = Path(model_run.output_dir) / model_run.run_id\n\n        # Validate outputs if requested\n        if validate_outputs:\n            if not check_dir.exists():\n                logger.warning(f\"Output directory does not exist: {check_dir}\")\n                return {\n                    \"success\": False,\n                    \"message\": f\"Output directory not found: {check_dir}\",\n                    \"run_id\": model_run.run_id,\n                    \"output_dir\": str(check_dir),\n                }\n            else:\n                # Count files in output directory\n                file_count = sum(1 for f in check_dir.rglob(\"*\") if f.is_file())\n                logger.info(f\"Found {file_count} output files in {check_dir}\")\n\n        logger.info(\n            f\"No-op postprocessing completed for run_id: {model_run.run_id}\"\n        )\n\n        return {\n            \"success\": True,\n            \"message\": \"No postprocessing requested - validation only\",\n            \"run_id\": model_run.run_id,\n            \"output_dir\": str(check_dir),\n            \"validated\": validate_outputs,\n        }\n\n    except Exception as e:\n        logger.exception(f\"Error in no-op postprocessor: {e}\")\n        return {\n            \"success\": False,\n            \"message\": f\"Error in postprocessor: {str(e)}\",\n            \"run_id\": getattr(model_run, \"run_id\", \"unknown\"),\n            \"error\": str(e),\n        }\n</code></pre>"},{"location":"plugin_architecture/#rompy.pipeline","title":"pipeline","text":"<p>Local pipeline backend for model execution.</p> <p>This module provides the local pipeline backend implementation.</p>"},{"location":"plugin_architecture/#rompy.pipeline-attributes","title":"Attributes","text":""},{"location":"plugin_architecture/#rompy.pipeline.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"plugin_architecture/#rompy.pipeline-classes","title":"Classes","text":""},{"location":"plugin_architecture/#rompy.pipeline.LocalPipelineBackend","title":"LocalPipelineBackend","text":"<p>Local pipeline backend that executes the full workflow locally.</p> <p>This backend uses the existing generate(), run() and postprocess() methods to execute the complete pipeline locally.</p> Source code in <code>rompy/pipeline/__init__.py</code> <pre><code>class LocalPipelineBackend:\n    \"\"\"Local pipeline backend that executes the full workflow locally.\n\n    This backend uses the existing generate(), run() and postprocess() methods\n    to execute the complete pipeline locally.\n    \"\"\"\n\n    def execute(\n        self,\n        model_run,\n        run_backend: str = \"local\",\n        processor: str = \"noop\",\n        run_kwargs: Optional[Dict[str, Any]] = None,\n        process_kwargs: Optional[Dict[str, Any]] = None,\n        cleanup_on_failure: bool = False,\n        validate_stages: bool = True,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the model pipeline locally.\n\n        Args:\n            model_run: The ModelRun instance to execute\n            run_backend: Backend to use for the run stage (\"local\" or \"docker\")\n            processor: Processor to use for the postprocess stage\n            run_kwargs: Additional parameters for the run stage\n            process_kwargs: Additional parameters for the postprocess stage\n            cleanup_on_failure: Whether to cleanup outputs on pipeline failure\n            validate_stages: Whether to validate each stage before proceeding\n            **kwargs: Additional parameters (unused)\n\n        Returns:\n            Combined results from the pipeline execution\n\n        Raises:\n            ValueError: If model_run is invalid or parameters are invalid\n        \"\"\"\n        # Validate input parameters\n        if not model_run:\n            raise ValueError(\"model_run cannot be None\")\n\n        if not hasattr(model_run, \"run_id\"):\n            raise ValueError(\"model_run must have a run_id attribute\")\n\n        if not isinstance(run_backend, str) or not run_backend.strip():\n            raise ValueError(\"run_backend must be a non-empty string\")\n\n        if not isinstance(processor, str) or not processor.strip():\n            raise ValueError(\"processor must be a non-empty string\")\n\n        # Initialize parameters\n        run_kwargs = run_kwargs or {}\n        process_kwargs = process_kwargs or {}\n\n        logger.info(f\"Starting pipeline execution for run_id: {model_run.run_id}\")\n        logger.info(\n            f\"Pipeline configuration: run_backend='{run_backend}', processor='{processor}'\"\n        )\n\n        pipeline_results = {\n            \"success\": False,\n            \"run_id\": model_run.run_id,\n            \"stages_completed\": [],\n            \"run_backend\": run_backend,\n            \"processor\": processor,\n        }\n\n        try:\n            # Stage 1: Generate input files\n            logger.info(f\"Stage 1: Generating input files for {model_run.run_id}\")\n\n            try:\n                staging_dir = model_run.generate()\n                pipeline_results[\"staging_dir\"] = (\n                    str(staging_dir) if staging_dir else None\n                )\n                pipeline_results[\"stages_completed\"].append(\"generate\")\n                logger.info(f\"Input files generated successfully in: {staging_dir}\")\n            except Exception as e:\n                logger.exception(f\"Failed to generate input files: {e}\")\n                return {\n                    **pipeline_results,\n                    \"stage\": \"generate\",\n                    \"message\": f\"Input file generation failed: {str(e)}\",\n                    \"error\": str(e),\n                }\n\n            # Validate generation stage\n            if validate_stages:\n                output_dir = Path(model_run.output_dir) / model_run.run_id\n                if not output_dir.exists():\n                    logger.error(f\"Output directory was not created: {output_dir}\")\n                    return {\n                        **pipeline_results,\n                        \"stage\": \"generate\",\n                        \"message\": f\"Output directory not found after generation: {output_dir}\",\n                    }\n\n            # Stage 2: Run the model\n            logger.info(f\"Stage 2: Running model using {run_backend} backend\")\n\n            try:\n                # Create appropriate backend configuration\n                backend_config = self._create_backend_config(run_backend, run_kwargs)\n\n                # Pass the generated workspace directory to avoid duplicate generation\n                run_success = model_run.run(\n                    backend=backend_config, workspace_dir=staging_dir\n                )\n                pipeline_results[\"run_success\"] = run_success\n\n                if not run_success:\n                    logger.error(\"Model run failed\")\n                    if cleanup_on_failure:\n                        self._cleanup_outputs(model_run)\n                    return {\n                        **pipeline_results,\n                        \"stage\": \"run\",\n                        \"message\": \"Model run failed\",\n                    }\n\n                pipeline_results[\"stages_completed\"].append(\"run\")\n                logger.info(\"Model run completed successfully\")\n\n            except Exception as e:\n                logger.exception(f\"Error during model run: {e}\")\n                if cleanup_on_failure:\n                    self._cleanup_outputs(model_run)\n                return {\n                    **pipeline_results,\n                    \"stage\": \"run\",\n                    \"message\": f\"Model run error: {str(e)}\",\n                    \"error\": str(e),\n                }\n\n            # Stage 3: Postprocess outputs\n            logger.info(f\"Stage 3: Postprocessing with {processor}\")\n\n            try:\n                postprocess_results = model_run.postprocess(\n                    processor=processor, **process_kwargs\n                )\n                pipeline_results[\"postprocess_results\"] = postprocess_results\n                pipeline_results[\"stages_completed\"].append(\"postprocess\")\n\n                # Check if postprocessing was successful\n                if isinstance(\n                    postprocess_results, dict\n                ) and not postprocess_results.get(\"success\", True):\n                    logger.warning(\n                        \"Postprocessing reported failure but pipeline will continue\"\n                    )\n\n                logger.info(\"Postprocessing completed\")\n\n            except Exception as e:\n                logger.exception(f\"Error during postprocessing: {e}\")\n                return {\n                    **pipeline_results,\n                    \"stage\": \"postprocess\",\n                    \"message\": f\"Postprocessing error: {str(e)}\",\n                    \"error\": str(e),\n                }\n\n            # Pipeline completed successfully\n            pipeline_results[\"success\"] = True\n            pipeline_results[\"message\"] = \"Pipeline completed successfully\"\n\n            logger.info(\n                f\"Pipeline execution completed successfully for run_id: {model_run.run_id}\"\n            )\n            return pipeline_results\n\n        except Exception as e:\n            logger.exception(f\"Unexpected error in pipeline execution: {e}\")\n            if cleanup_on_failure:\n                self._cleanup_outputs(model_run)\n            return {\n                **pipeline_results,\n                \"stage\": \"pipeline\",\n                \"message\": f\"Pipeline error: {str(e)}\",\n                \"error\": str(e),\n            }\n\n    def _cleanup_outputs(self, model_run) -&gt; None:\n        \"\"\"Clean up output files on pipeline failure.\n\n        Args:\n            model_run: The ModelRun instance\n        \"\"\"\n        try:\n            output_dir = Path(model_run.output_dir) / model_run.run_id\n            if output_dir.exists():\n                logger.info(f\"Cleaning up output directory: {output_dir}\")\n                import shutil\n\n                shutil.rmtree(output_dir)\n                logger.info(\"Cleanup completed\")\n        except Exception as e:\n            logger.warning(f\"Failed to cleanup output directory: {e}\")\n\n    def _create_backend_config(self, run_backend: str, run_kwargs: Dict[str, Any]):\n        \"\"\"Create appropriate backend configuration from string name and kwargs.\n\n        Args:\n            run_backend: Backend name (\"local\" or \"docker\")\n            run_kwargs: Additional configuration parameters\n\n        Returns:\n            Backend configuration object\n\n        Raises:\n            ValueError: If backend name is not supported\n        \"\"\"\n        if run_backend == \"local\":\n            # Filter kwargs to only include valid LocalConfig fields\n            valid_fields = set(LocalConfig.model_fields.keys())\n            filtered_kwargs = {k: v for k, v in run_kwargs.items() if k in valid_fields}\n            if filtered_kwargs != run_kwargs:\n                invalid_fields = set(run_kwargs.keys()) - valid_fields\n                logger.warning(f\"Ignoring invalid LocalConfig fields: {invalid_fields}\")\n            return LocalConfig(**filtered_kwargs)\n        elif run_backend == \"docker\":\n            # Filter kwargs to only include valid DockerConfig fields\n            valid_fields = set(DockerConfig.model_fields.keys())\n            filtered_kwargs = {k: v for k, v in run_kwargs.items() if k in valid_fields}\n            if filtered_kwargs != run_kwargs:\n                invalid_fields = set(run_kwargs.keys()) - valid_fields\n                logger.warning(\n                    f\"Ignoring invalid DockerConfig fields: {invalid_fields}\"\n                )\n            return DockerConfig(**filtered_kwargs)\n        else:\n            raise ValueError(\n                f\"Unsupported backend: {run_backend}. Supported: local, docker\"\n            )\n</code></pre>"},{"location":"plugin_architecture/#rompy.pipeline.LocalPipelineBackend-functions","title":"Functions","text":""},{"location":"plugin_architecture/#rompy.pipeline.LocalPipelineBackend.execute","title":"execute","text":"<pre><code>execute(model_run, run_backend: str = 'local', processor: str = 'noop', run_kwargs: Optional[Dict[str, Any]] = None, process_kwargs: Optional[Dict[str, Any]] = None, cleanup_on_failure: bool = False, validate_stages: bool = True, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Execute the model pipeline locally.</p> <p>Parameters:</p> Name Type Description Default <code>model_run</code> <p>The ModelRun instance to execute</p> required <code>run_backend</code> <code>str</code> <p>Backend to use for the run stage (\"local\" or \"docker\")</p> <code>'local'</code> <code>processor</code> <code>str</code> <p>Processor to use for the postprocess stage</p> <code>'noop'</code> <code>run_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters for the run stage</p> <code>None</code> <code>process_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters for the postprocess stage</p> <code>None</code> <code>cleanup_on_failure</code> <code>bool</code> <p>Whether to cleanup outputs on pipeline failure</p> <code>False</code> <code>validate_stages</code> <code>bool</code> <p>Whether to validate each stage before proceeding</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters (unused)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Combined results from the pipeline execution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_run is invalid or parameters are invalid</p> Source code in <code>rompy/pipeline/__init__.py</code> <pre><code>def execute(\n    self,\n    model_run,\n    run_backend: str = \"local\",\n    processor: str = \"noop\",\n    run_kwargs: Optional[Dict[str, Any]] = None,\n    process_kwargs: Optional[Dict[str, Any]] = None,\n    cleanup_on_failure: bool = False,\n    validate_stages: bool = True,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the model pipeline locally.\n\n    Args:\n        model_run: The ModelRun instance to execute\n        run_backend: Backend to use for the run stage (\"local\" or \"docker\")\n        processor: Processor to use for the postprocess stage\n        run_kwargs: Additional parameters for the run stage\n        process_kwargs: Additional parameters for the postprocess stage\n        cleanup_on_failure: Whether to cleanup outputs on pipeline failure\n        validate_stages: Whether to validate each stage before proceeding\n        **kwargs: Additional parameters (unused)\n\n    Returns:\n        Combined results from the pipeline execution\n\n    Raises:\n        ValueError: If model_run is invalid or parameters are invalid\n    \"\"\"\n    # Validate input parameters\n    if not model_run:\n        raise ValueError(\"model_run cannot be None\")\n\n    if not hasattr(model_run, \"run_id\"):\n        raise ValueError(\"model_run must have a run_id attribute\")\n\n    if not isinstance(run_backend, str) or not run_backend.strip():\n        raise ValueError(\"run_backend must be a non-empty string\")\n\n    if not isinstance(processor, str) or not processor.strip():\n        raise ValueError(\"processor must be a non-empty string\")\n\n    # Initialize parameters\n    run_kwargs = run_kwargs or {}\n    process_kwargs = process_kwargs or {}\n\n    logger.info(f\"Starting pipeline execution for run_id: {model_run.run_id}\")\n    logger.info(\n        f\"Pipeline configuration: run_backend='{run_backend}', processor='{processor}'\"\n    )\n\n    pipeline_results = {\n        \"success\": False,\n        \"run_id\": model_run.run_id,\n        \"stages_completed\": [],\n        \"run_backend\": run_backend,\n        \"processor\": processor,\n    }\n\n    try:\n        # Stage 1: Generate input files\n        logger.info(f\"Stage 1: Generating input files for {model_run.run_id}\")\n\n        try:\n            staging_dir = model_run.generate()\n            pipeline_results[\"staging_dir\"] = (\n                str(staging_dir) if staging_dir else None\n            )\n            pipeline_results[\"stages_completed\"].append(\"generate\")\n            logger.info(f\"Input files generated successfully in: {staging_dir}\")\n        except Exception as e:\n            logger.exception(f\"Failed to generate input files: {e}\")\n            return {\n                **pipeline_results,\n                \"stage\": \"generate\",\n                \"message\": f\"Input file generation failed: {str(e)}\",\n                \"error\": str(e),\n            }\n\n        # Validate generation stage\n        if validate_stages:\n            output_dir = Path(model_run.output_dir) / model_run.run_id\n            if not output_dir.exists():\n                logger.error(f\"Output directory was not created: {output_dir}\")\n                return {\n                    **pipeline_results,\n                    \"stage\": \"generate\",\n                    \"message\": f\"Output directory not found after generation: {output_dir}\",\n                }\n\n        # Stage 2: Run the model\n        logger.info(f\"Stage 2: Running model using {run_backend} backend\")\n\n        try:\n            # Create appropriate backend configuration\n            backend_config = self._create_backend_config(run_backend, run_kwargs)\n\n            # Pass the generated workspace directory to avoid duplicate generation\n            run_success = model_run.run(\n                backend=backend_config, workspace_dir=staging_dir\n            )\n            pipeline_results[\"run_success\"] = run_success\n\n            if not run_success:\n                logger.error(\"Model run failed\")\n                if cleanup_on_failure:\n                    self._cleanup_outputs(model_run)\n                return {\n                    **pipeline_results,\n                    \"stage\": \"run\",\n                    \"message\": \"Model run failed\",\n                }\n\n            pipeline_results[\"stages_completed\"].append(\"run\")\n            logger.info(\"Model run completed successfully\")\n\n        except Exception as e:\n            logger.exception(f\"Error during model run: {e}\")\n            if cleanup_on_failure:\n                self._cleanup_outputs(model_run)\n            return {\n                **pipeline_results,\n                \"stage\": \"run\",\n                \"message\": f\"Model run error: {str(e)}\",\n                \"error\": str(e),\n            }\n\n        # Stage 3: Postprocess outputs\n        logger.info(f\"Stage 3: Postprocessing with {processor}\")\n\n        try:\n            postprocess_results = model_run.postprocess(\n                processor=processor, **process_kwargs\n            )\n            pipeline_results[\"postprocess_results\"] = postprocess_results\n            pipeline_results[\"stages_completed\"].append(\"postprocess\")\n\n            # Check if postprocessing was successful\n            if isinstance(\n                postprocess_results, dict\n            ) and not postprocess_results.get(\"success\", True):\n                logger.warning(\n                    \"Postprocessing reported failure but pipeline will continue\"\n                )\n\n            logger.info(\"Postprocessing completed\")\n\n        except Exception as e:\n            logger.exception(f\"Error during postprocessing: {e}\")\n            return {\n                **pipeline_results,\n                \"stage\": \"postprocess\",\n                \"message\": f\"Postprocessing error: {str(e)}\",\n                \"error\": str(e),\n            }\n\n        # Pipeline completed successfully\n        pipeline_results[\"success\"] = True\n        pipeline_results[\"message\"] = \"Pipeline completed successfully\"\n\n        logger.info(\n            f\"Pipeline execution completed successfully for run_id: {model_run.run_id}\"\n        )\n        return pipeline_results\n\n    except Exception as e:\n        logger.exception(f\"Unexpected error in pipeline execution: {e}\")\n        if cleanup_on_failure:\n            self._cleanup_outputs(model_run)\n        return {\n            **pipeline_results,\n            \"stage\": \"pipeline\",\n            \"message\": f\"Pipeline error: {str(e)}\",\n            \"error\": str(e),\n        }\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#installation","title":"Installation","text":""},{"location":"quickstart/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install rompy\n</code></pre>"},{"location":"quickstart/#install-from-source-code","title":"Install from source code","text":"<pre><code>git clone git@github.com:rom-py/rompy.git\ncd rompy\npip install -e .\n</code></pre>"},{"location":"quickstart/#usage","title":"Usage","text":""},{"location":"quickstart/#simple-example","title":"Simple example","text":"<pre><code>from rompy import ModelRun\n\n# Initiate a model run\nrun = ModelRun()\n\n# Execute the model run\nrun()\n</code></pre>"},{"location":"templating/","title":"Template Variable Substitution","text":"<p>Rompy supports template variable substitution in YAML configuration files using <code>${VAR}</code> syntax. This allows you to:</p> <ul> <li>Use environment variables in configs</li> <li>Set default values for missing variables</li> <li>Process datetime values with filters</li> <li>Share configs across different environments</li> </ul>"},{"location":"templating/#syntax","title":"Syntax","text":""},{"location":"templating/#basic-substitution","title":"Basic Substitution","text":"<pre><code>output_dir: \"${OUTPUT_ROOT}/my_run\"\nrun_id: \"${RUN_ID}\"\n</code></pre>"},{"location":"templating/#default-values","title":"Default Values","text":"<p>Provide fallback values when variables are not set:</p> <pre><code>output_dir: \"${OUTPUT_ROOT:-./output}/my_run\"\ntimeout: \"${JOB_TIMEOUT:-3600}\"\nthreads: \"${NUM_THREADS:-4}\"\n</code></pre>"},{"location":"templating/#type-conversion","title":"Type Conversion","text":"<p>When a YAML value is exactly one template expression, type conversion is automatic:</p> <pre><code>timeout: \"${TIMEOUT}\"          # \"3600\" \u2192 3600 (int)\ndebug: \"${DEBUG}\"               # \"true\" \u2192 True (bool)\npi: \"${PI}\"                     # \"3.14\" \u2192 3.14 (float)\n</code></pre> <p>Embedded templates always produce strings:</p> <pre><code>path: \"/data/${USER}/file\"     # Always string\n</code></pre>"},{"location":"templating/#datetime-filters","title":"Datetime Filters","text":""},{"location":"templating/#available-filters","title":"Available Filters","text":"Filter Description Example <code>as_datetime</code> Parse ISO-8601 datetime <code>${CYCLE\\|as_datetime}</code> <code>strftime</code> Format datetime <code>${CYCLE\\|strftime:%Y%m%d}</code> <code>shift</code> Add/subtract time <code>${CYCLE\\|shift:-1d}</code>"},{"location":"templating/#filter-chaining","title":"Filter Chaining","text":"<p>Combine filters with <code>|</code>:</p> <pre><code>previous_day: \"${CYCLE|as_datetime|shift:-1d|strftime:%Y-%m-%d}\"\n</code></pre>"},{"location":"templating/#datetime-examples","title":"Datetime Examples","text":"<pre><code>cycle_date: \"${CYCLE|as_datetime}\"\nfilename: \"wind_${CYCLE|strftime:%Y%m%d}.nc\"\nprev_cycle: \"${CYCLE|as_datetime|shift:-1d}\"\nend_time: \"${CYCLE|as_datetime|shift:+24h}\"\n</code></pre>"},{"location":"templating/#shift-syntax","title":"Shift Syntax","text":"<p>Time deltas use: <code>[+|-]&lt;number&gt;&lt;unit&gt;</code></p> <p>Units: - <code>d</code> = days - <code>h</code> = hours - <code>m</code> = minutes - <code>s</code> = seconds</p> <p>Examples: - <code>+1d</code> = add 1 day - <code>-6h</code> = subtract 6 hours - <code>+30m</code> = add 30 minutes</p>"},{"location":"templating/#complete-examples","title":"Complete Examples","text":""},{"location":"templating/#basic-config","title":"Basic Config","text":"<pre><code>run_id: \"cycle_${CYCLE|strftime:%Y%m%d}\"\n\nperiod:\n  start: \"${CYCLE}\"\n  end: \"${CYCLE|as_datetime|shift:+1d}\"\n  interval: \"1H\"\n\noutput_dir: \"${OUTPUT_ROOT:-./output}/cycle_${CYCLE|strftime:%Y%m%d}\"\n\ninput_files:\n  wind: \"${DATA_ROOT}/wind/wind_${CYCLE|strftime:%Y%m%d}.nc\"\n  wave: \"${DATA_ROOT}/wave/wave_${CYCLE|strftime:%Y%m%d}.nc\"\n</code></pre> <p>Usage: <pre><code>export CYCLE=2023-01-01T00:00:00\nexport DATA_ROOT=/scratch/data\nrompy generate config.yml\n</code></pre></p>"},{"location":"templating/#backend-config","title":"Backend Config","text":"<pre><code>type: local\ntimeout: \"${JOB_TIMEOUT:-3600}\"\ncommand: \"python run_model.py\"\n\nenv_vars:\n  OMP_NUM_THREADS: \"${NUM_THREADS:-4}\"\n  WORK_DIR: \"${WORK_DIR}\"\n</code></pre> <p>Usage: <pre><code>export WORK_DIR=/scratch/my_job\nexport NUM_THREADS=8\nrompy run config.yml --backend-config backend.yml\n</code></pre></p>"},{"location":"templating/#lookback-pattern","title":"Lookback Pattern","text":"<p>Access previous time periods:</p> <pre><code>input_files:\n  current: \"${DATA_ROOT}/data_${CYCLE|strftime:%Y%m%d}.nc\"\n  previous: \"${DATA_ROOT}/data_${CYCLE|as_datetime|shift:-1d|strftime:%Y%m%d}.nc\"\n  week_ago: \"${DATA_ROOT}/data_${CYCLE|as_datetime|shift:-7d|strftime:%Y%m%d}.nc\"\n</code></pre>"},{"location":"templating/#nested-directory-structures","title":"Nested Directory Structures","text":"<pre><code>output_dir: \"${DATA_ROOT}/output/${CYCLE|strftime:%Y/%m/%d}\"\n</code></pre> <p>With <code>CYCLE=2023-01-15T00:00:00</code> \u2192 <code>/data/output/2023/01/15</code></p>"},{"location":"templating/#how-it-works","title":"How It Works","text":"<p>Template rendering happens after YAML parsing but before Pydantic validation:</p> <pre><code>1. Load YAML file \u2192 dict\n2. Render templates: ${VAR} \u2192 actual values\n3. Pydantic validation: dict \u2192 ModelRun object\n</code></pre> <p>This ensures: - Type safety (Pydantic sees resolved values) - Clear error messages (template errors before validation errors) - Datetime objects work with Pydantic models</p>"},{"location":"templating/#error-handling","title":"Error Handling","text":""},{"location":"templating/#missing-variables","title":"Missing Variables","text":"<p>By default, missing variables cause an error:</p> <pre><code>path: \"${MISSING_VAR}\"  # Error: Variable 'MISSING_VAR' not found\n</code></pre> <p>Use defaults to make variables optional:</p> <pre><code>path: \"${OPTIONAL_VAR:-/default/path}\"  # OK if OPTIONAL_VAR not set\n</code></pre>"},{"location":"templating/#invalid-filters","title":"Invalid Filters","text":"<p>Unknown filters produce clear errors:</p> <pre><code>date: \"${CYCLE|unknown_filter}\"  # Error: Unknown filter 'unknown_filter'\n</code></pre>"},{"location":"templating/#datetime-parsing","title":"Datetime Parsing","text":"<p>Invalid datetime strings fail early:</p> <pre><code>date: \"${CYCLE|as_datetime}\"  # Error if CYCLE is not ISO-8601 format\n</code></pre>"},{"location":"templating/#tips","title":"Tips","text":""},{"location":"templating/#quote-values-with-","title":"Quote Values with <code>:-</code>","text":"<p>YAML interprets <code>:</code> as mapping syntax. Quote defaults containing colons:</p> <pre><code>path: \"${VAR:-/path/with:colon}\"  # GOOD - quoted\npath: ${VAR:-/path/with:colon}    # BAD - YAML parse error\n</code></pre>"},{"location":"templating/#environment-variables","title":"Environment Variables","text":"<p>Templates use <code>os.environ</code> by default:</p> <pre><code>export MY_VAR=value\nrompy generate config.yml  # ${MY_VAR} resolved automatically\n</code></pre>"},{"location":"templating/#separation-from-jinja2","title":"Separation from Jinja2","text":"<p>Don't confuse with rompy's existing Jinja2 templates (used for model control files):</p> <ul> <li><code>${VAR}</code> = Config templating (pre-load, env vars)</li> <li><code>{{runtime.var}}</code> = File templating (post-load, Python objects)</li> </ul> <p>They serve different purposes and run at different times.</p>"},{"location":"templating/#see-also","title":"See Also","text":"<ul> <li>Example configs: <code>examples/configs/templated_*.yml</code></li> <li>Tests: <code>tests/test_templating.py</code></li> <li>Implementation: <code>src/rompy/templating.py</code></li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>Rompy provides a framework for ocean model configuration and setup. Here are some usage examples:</p>"},{"location":"usage/#basic-model-run","title":"Basic Model Run","text":"<pre><code>from rompy import ModelRun\n\n# Create a model run instance\nrun = ModelRun()\n\n# Execute the model run\nrun()\n</code></pre>"},{"location":"developer/","title":"Developer Guide","text":"<p>Welcome to the rompy developer documentation. This section provides detailed technical information about rompy's architecture, design patterns, and extension mechanisms.</p> <p>rompy is designed with extensibility and maintainability in mind, featuring a modular architecture that separates concerns between configuration, execution, and processing. Understanding these architectural decisions will help you contribute effectively to the project or extend it for your specific needs.</p>"},{"location":"developer/#architecture-design","title":"Architecture &amp; Design","text":"<p>rompy's architecture is built around several key principles:</p>"},{"location":"developer/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Configuration (what to compute) is separated from execution (how to compute) and processing (what to do with results).</p>"},{"location":"developer/#plugin-architecture","title":"Plugin Architecture","text":"<p>Core functionality can be extended through entry points without modifying the base code.</p>"},{"location":"developer/#type-safety","title":"Type Safety","text":"<p>Pydantic models provide strong validation for configuration while maintaining flexibility for runtime concerns.</p>"},{"location":"developer/#reproducibility","title":"Reproducibility","text":"<p>Model configurations are fully serializable and version-controlled, ensuring reproducible science.</p>"},{"location":"developer/#environment-agnostic","title":"Environment Agnostic","text":"<p>The same model configuration can be executed in different environments (local, HPC, cloud, containers).</p>"},{"location":"developer/#key-architectural-decisions","title":"Key Architectural Decisions","text":"<p>The rompy codebase makes several important architectural decisions that affect how you should approach development:</p> <ol> <li> <p>Dual Extension Patterns: Different extension mechanisms for different concerns (Pydantic unions for configs, entry points for backends)</p> </li> <li> <p>Configuration Immutability: Model configurations are treated as immutable scientific artifacts</p> </li> <li> <p>Late Binding: Execution backends are resolved at runtime to support environment-specific deployments</p> </li> <li> <p>Composable Workflows: Pipeline architecture allows mixing and matching of different execution and processing strategies</p> </li> </ol>"},{"location":"developer/#quick-start-for-developers","title":"Quick Start for Developers","text":"<p>If you're new to rompy development, start here:</p> <ol> <li>Understanding the Architecture: Read Component Selection Patterns to understand the core design patterns</li> </ol>"},{"location":"developer/#design-philosophy","title":"Design Philosophy","text":"<p>rompy is designed around the principle that configuration should be declarative and execution should be imperative. This means:</p> <ul> <li>What to compute (model physics, grids, forcing) is declared in strongly-typed, validated configuration objects</li> <li>How to compute (local vs cloud, serial vs parallel) is handled by pluggable execution backends</li> <li>What to do with results (analysis, visualization, archiving) is handled by composable processing pipelines</li> </ul> <p>This separation allows the same scientific model configuration to be executed in vastly different computational environments while maintaining reproducibility and traceability.</p>"},{"location":"developer/#getting-help","title":"Getting Help","text":"<ul> <li>Architecture Questions: Check the architecture guides in this developer section</li> <li>Community: Join discussions in the project's issue tracker or mailing list</li> </ul> <p>The developer documentation is continuously evolving. If you find gaps or have suggestions for improvement, please contribute back to help other developers.</p>"},{"location":"developer/#table-of-contents","title":"Table of Contents","text":""},{"location":"developer/#architecture-design_1","title":"Architecture &amp; Design","text":"<ul> <li>Component Selection Patterns</li> </ul>"},{"location":"developer/backend_reference/","title":"Backend Reference","text":"<p>This document provides comprehensive technical reference for ROMPY's backend system, focusing on concepts, usage patterns, and advanced configuration techniques.</p> <p>[!NOTE] For getting started with backends, see backends. For complete API documentation, see api.</p>"},{"location":"developer/backend_reference/#backend-configuration-system","title":"Backend Configuration System","text":"<p>The backend system uses Pydantic models to provide type-safe, validated execution parameters. All configurations inherit from <code>rompy.backends.config.BaseBackendConfig</code>.</p>"},{"location":"developer/backend_reference/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<pre><code>BaseBackendConfig\n\u251c\u2500\u2500 LocalConfig          # Local system execution\n\u251c\u2500\u2500 DockerConfig         # Docker container execution\n\u2514\u2500\u2500 CustomConfig         # User-defined configurations\n</code></pre>"},{"location":"developer/backend_reference/#configuration-loading","title":"Configuration Loading","text":"<p>Configurations can be loaded from files or created programmatically:</p> <pre><code>import yaml\nfrom rompy.backends import LocalConfig, DockerConfig\n\n# From YAML file\nwith open(\"config.yml\") as f:\n    config_data = yaml.safe_load(f)\n    config = LocalConfig(**config_data)\n\n# Programmatically\nconfig = DockerConfig(\n    image=\"swan:latest\",\n    cpu=4,\n    memory=\"2g\"\n)\n</code></pre> <p>For complete configuration class documentation, see:</p> <ul> <li><code>rompy.backends.config.BaseBackendConfig</code></li> <li><code>rompy.backends.config.LocalConfig</code></li> <li><code>rompy.backends.config.DockerConfig</code></li> </ul>"},{"location":"developer/backend_reference/#configuration-file-formats","title":"Configuration File Formats","text":"<p>Backend configurations support YAML and JSON formats with a common structure.</p>"},{"location":"developer/backend_reference/#yaml-format","title":"YAML Format","text":"<pre><code># Local execution example\ntype: local\ntimeout: 3600\ncommand: \"python run_model.py\"\nenv_vars:\n  OMP_NUM_THREADS: \"4\"\n  MODEL_DEBUG: \"true\"\n\n---\n# Docker execution example\ntype: docker\nimage: \"swan:latest\"\ncpu: 8\nmemory: \"4g\"\ntimeout: 10800\nvolumes:\n  - \"/data/input:/app/input:ro\"\n  - \"/data/output:/app/output:rw\"\nenv_vars:\n  MODEL_THREADS: \"8\"\n</code></pre>"},{"location":"developer/backend_reference/#json-format","title":"JSON Format","text":"<pre><code>{\n  \"type\": \"local\",\n  \"timeout\": 3600,\n  \"command\": \"python run_model.py\",\n  \"env_vars\": {\n    \"OMP_NUM_THREADS\": \"4\"\n  }\n}\n</code></pre>"},{"location":"developer/backend_reference/#configuration-validation","title":"Configuration Validation","text":"<p>Pydantic provides comprehensive validation with descriptive error messages.</p>"},{"location":"developer/backend_reference/#validation-rules","title":"Validation Rules","text":"<p>Common Validation (BaseBackendConfig):</p> <ul> <li><code>timeout</code>: Must be between 60 and 86400 seconds</li> <li><code>env_vars</code>: Must be string key-value pairs</li> <li><code>working_dir</code>: Must exist if specified</li> </ul> <p>LocalConfig Validation:</p> <ul> <li><code>command</code>: Must be non-empty string if provided</li> <li><code>shell</code>: Must be boolean</li> <li><code>capture_output</code>: Must be boolean</li> </ul> <p>DockerConfig Validation:</p> <ul> <li>Either <code>image</code> or <code>dockerfile</code> must be provided (not both)</li> <li><code>cpu</code>: Must be between 1 and 128</li> <li><code>memory</code>: Must match pattern (e.g., \"2g\", \"512m\")</li> <li><code>volumes</code>: Must use \"host:container[:mode]\" format with existing host paths</li> </ul>"},{"location":"developer/backend_reference/#error-handling","title":"Error Handling","text":"<pre><code>from rompy.backends import DockerConfig\nfrom pydantic import ValidationError\n\ntry:\n    config = DockerConfig(cpu=200)  # Invalid - exceeds maximum\nexcept ValidationError as e:\n    for error in e.errors():\n        print(f\"Field {error['loc']}: {error['msg']}\")\n</code></pre>"},{"location":"developer/backend_reference/#schema-generation","title":"Schema Generation","text":"<p>Generate configuration schemas for validation and documentation:</p> <pre><code>from rompy.backends import LocalConfig\nimport json\n\n# Generate JSON schema\nschema = LocalConfig.model_json_schema()\n\n# Save for external validation\nwith open(\"local_schema.json\", \"w\") as f:\n    json.dump(schema, f, indent=2)\n</code></pre>"},{"location":"developer/backend_reference/#using-schemas","title":"Using Schemas","text":"<pre><code>import jsonschema\n\n# Validate configuration data against schema\nconfig_data = {\"timeout\": 3600, \"command\": \"python run.py\"}\nschema = LocalConfig.model_json_schema()\n\ntry:\n    jsonschema.validate(config_data, schema)\n    print(\"Configuration is valid\")\nexcept jsonschema.ValidationError as e:\n    print(f\"Validation error: {e.message}\")\n</code></pre>"},{"location":"developer/backend_reference/#advanced-configuration-patterns","title":"Advanced Configuration Patterns","text":""},{"location":"developer/backend_reference/#dynamic-configuration","title":"Dynamic Configuration","text":"<p>Create configurations based on runtime conditions:</p> <pre><code>import psutil\nfrom rompy.backends import LocalConfig, DockerConfig\n\ndef create_optimal_config():\n    \"\"\"Create configuration based on system resources.\"\"\"\n    cpu_count = psutil.cpu_count()\n    memory_gb = psutil.virtual_memory().total // (1024**3)\n\n    if memory_gb &gt; 16 and cpu_count &gt; 8:\n        return DockerConfig(\n            image=\"swan:hpc\",\n            cpu=cpu_count,\n            memory=f\"{memory_gb}g\",\n            mpiexec=f\"mpirun -np {cpu_count}\"\n        )\n    else:\n        return LocalConfig(\n            timeout=7200,\n            env_vars={\"OMP_NUM_THREADS\": str(min(cpu_count, 4))}\n        )\n</code></pre>"},{"location":"developer/backend_reference/#environment-based-configuration","title":"Environment-Based Configuration","text":"<p>Load different configurations based on environment:</p> <pre><code>import os\nfrom rompy.backends import LocalConfig, DockerConfig\n\ndef load_config_for_environment():\n    \"\"\"Load configuration based on ROMPY_ENV environment variable.\"\"\"\n    env = os.getenv(\"ROMPY_ENV\", \"development\")\n\n    configs = {\n        \"production\": DockerConfig(\n            image=\"swan:production\",\n            cpu=16,\n            memory=\"32g\",\n            timeout=21600\n        ),\n        \"staging\": DockerConfig(\n            image=\"swan:staging\",\n            cpu=8,\n            memory=\"16g\",\n            timeout=10800\n        ),\n        \"development\": LocalConfig(\n            timeout=3600,\n            env_vars={\"LOG_LEVEL\": \"DEBUG\"}\n        )\n    }\n\n    return configs.get(env, configs[\"development\"])\n</code></pre>"},{"location":"developer/backend_reference/#configuration-templates","title":"Configuration Templates","text":"<p>Create reusable configuration templates:</p> <pre><code>from rompy.backends import DockerConfig\n\n# Base template\nBASE_SWAN_CONFIG = {\n    \"image\": \"swan:latest\",\n    \"user\": \"modeluser\",\n    \"timeout\": 7200,\n    \"env_vars\": {\n        \"MODEL_DEBUG\": \"false\",\n        \"LOG_LEVEL\": \"INFO\"\n    }\n}\n\n# Specialized configurations\ndef create_hpc_config(**overrides):\n    \"\"\"Create HPC-optimized configuration.\"\"\"\n    config_data = {\n        **BASE_SWAN_CONFIG,\n        \"cpu\": 32,\n        \"memory\": \"64g\",\n        \"mpiexec\": \"mpirun -np 32\",\n        **overrides\n    }\n    return DockerConfig(**config_data)\n\ndef create_dev_config(**overrides):\n    \"\"\"Create development configuration.\"\"\"\n    config_data = {\n        **BASE_SWAN_CONFIG,\n        \"cpu\": 2,\n        \"memory\": \"2g\",\n        \"remove_container\": False,  # Keep for debugging\n        \"env_vars\": {\n            **BASE_SWAN_CONFIG[\"env_vars\"],\n            \"MODEL_DEBUG\": \"true\",\n            \"LOG_LEVEL\": \"DEBUG\"\n        },\n        **overrides\n    }\n    return DockerConfig(**config_data)\n</code></pre>"},{"location":"developer/backend_reference/#creating-custom-backends","title":"Creating Custom Backends","text":"<p>The backend system supports custom implementations through inheritance and entry points.</p>"},{"location":"developer/backend_reference/#custom-configuration-classes","title":"Custom Configuration Classes","text":"<p>Create custom configuration classes by inheriting from <code>rompy.backends.config.BaseBackendConfig</code>:</p> <pre><code>from rompy.backends.config import BaseBackendConfig\nfrom pydantic import Field, validator\nfrom typing import Optional\n\nclass SlurmConfig(BaseBackendConfig):\n    \"\"\"Configuration for SLURM cluster execution.\"\"\"\n\n    queue: str = Field(..., description=\"SLURM queue name\")\n    nodes: int = Field(1, ge=1, le=100, description=\"Number of nodes\")\n    partition: str = Field(\"compute\", description=\"Cluster partition\")\n    time_limit: str = Field(\"1:00:00\", description=\"Time limit (HH:MM:SS)\")\n    account: Optional[str] = Field(None, description=\"Account for billing\")\n\n    @validator('time_limit')\n    def validate_time_limit(cls, v):\n        import re\n        if not re.match(r'^\\d{1,2}:\\d{2}:\\d{2}$', v):\n            raise ValueError(\"Time limit must be in format HH:MM:SS\")\n        return v\n\n    def get_backend_class(self):\n        from mypackage.backends import SlurmRunBackend\n        return SlurmRunBackend\n</code></pre>"},{"location":"developer/backend_reference/#custom-backend-implementation","title":"Custom Backend Implementation","text":"<p>Implement backend classes that work with your custom configurations:</p> <pre><code>import logging\nfrom pathlib import Path\n\nclass SlurmRunBackend:\n    \"\"\"Execute models on SLURM clusters.\"\"\"\n\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n\n    def run(self, model_run, config: SlurmConfig) -&gt; bool:\n        \"\"\"Submit model run to SLURM queue.\"\"\"\n        try:\n            # Generate model input files\n            model_run.generate()\n\n            # Create and submit SLURM job\n            job_script = self._create_job_script(model_run, config)\n            job_id = self._submit_job(job_script)\n\n            if job_id:\n                return self._wait_for_completion(job_id, config)\n            return False\n\n        except Exception as e:\n            self.logger.error(f\"SLURM execution failed: {e}\")\n            return False\n\n    def _create_job_script(self, model_run, config):\n        \"\"\"Create SLURM job script.\"\"\"\n        # Implementation details...\n        pass\n\n    def _submit_job(self, job_script):\n        \"\"\"Submit job to SLURM.\"\"\"\n        # Implementation details...\n        pass\n\n    def _wait_for_completion(self, job_id, config):\n        \"\"\"Wait for job completion.\"\"\"\n        # Implementation details...\n        pass\n</code></pre>"},{"location":"developer/backend_reference/#entry-points-registration","title":"Entry Points Registration","text":"<p>Register custom backends in your package's <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"rompy.run\"]\nslurm = \"mypackage.backends:SlurmRunBackend\"\n\n[project.entry-points.\"rompy.config\"]\nslurm = \"mypackage.config:SlurmConfig\"\n</code></pre>"},{"location":"developer/backend_reference/#backend-discovery","title":"Backend Discovery","text":"<p>The system automatically discovers registered backends:</p> <pre><code>from rompy.backends import get_available_backends\n\n# Get all available backends\nbackends = get_available_backends()\nprint(\"Available backends:\", list(backends.keys()))\n\n# Use custom backend\nfrom mypackage.config import SlurmConfig\n\nconfig = SlurmConfig(\n    queue=\"gpu\",\n    nodes=2,\n    partition=\"compute\",\n    time_limit=\"2:00:00\"\n)\n\nsuccess = model_run.run(backend=config)\n</code></pre> <p>For complete backend discovery implementation, see <code>rompy.backends</code>.</p>"},{"location":"developer/backend_reference/#postprocessor-system","title":"Postprocessor System","text":"<p>Postprocessors handle model outputs after execution. The system supports built-in and custom postprocessors.</p>"},{"location":"developer/backend_reference/#built-in-postprocessors","title":"Built-in Postprocessors","text":"<p>Available postprocessors include:</p> <ul> <li>noop: No-operation processor (default)</li> <li>archive: Archive outputs to compressed files</li> <li>analyze: Analyze model results</li> <li>visualize: Generate visualization outputs</li> </ul> <p>For complete postprocessor documentation, see <code>rompy.backends.postprocessors</code>.</p>"},{"location":"developer/backend_reference/#usage-patterns","title":"Usage Patterns","text":"<pre><code># Basic postprocessing\nresults = model_run.postprocess(processor=\"archive\")\n\n# Custom postprocessing with options\nresults = model_run.postprocess(\n    processor=\"analyze\",\n    output_format=\"netcdf\",\n    compress=True,\n    analysis_type=\"spectral\"\n)\n</code></pre>"},{"location":"developer/backend_reference/#custom-postprocessors","title":"Custom Postprocessors","text":"<p>Create custom postprocessors by implementing the processor interface:</p> <pre><code>from typing import Dict, Any\n\nclass CustomPostprocessor:\n    \"\"\"Custom postprocessor example.\"\"\"\n\n    def process(self, model_run, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Process model outputs.\"\"\"\n        try:\n            # Custom processing logic here\n            output_dir = Path(model_run.output_dir) / model_run.run_id\n\n            # Process files in output_dir\n            processed_files = self._process_outputs(output_dir, **kwargs)\n\n            return {\n                \"success\": True,\n                \"processed_files\": processed_files,\n                \"message\": \"Custom processing completed\"\n            }\n\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e)\n            }\n\n    def _process_outputs(self, output_dir, **kwargs):\n        \"\"\"Implementation-specific processing.\"\"\"\n        # Custom processing logic\n        pass\n</code></pre>"},{"location":"developer/backend_reference/#best-practices","title":"Best Practices","text":""},{"location":"developer/backend_reference/#configuration-management","title":"Configuration Management","text":"<ol> <li>Use Version Control: Store configuration files in version control</li> <li>Environment Variables: Use environment variables for sensitive data</li> <li>Validation: Always validate configurations before production use</li> <li>Documentation: Document custom configurations thoroughly</li> <li>Testing: Test configurations with different scenarios</li> </ol> <pre><code># Good: Use environment variables for sensitive data\nconfig = LocalConfig(\n    env_vars={\"API_KEY\": os.environ.get(\"API_KEY\")}\n)\n\n# Avoid: Hardcoding sensitive data\nconfig = LocalConfig(\n    env_vars={\"API_KEY\": \"secret-key-123\"}\n)\n</code></pre>"},{"location":"developer/backend_reference/#security-considerations","title":"Security Considerations","text":"<ol> <li>Container Security: Use non-root users in containers</li> <li>Volume Mounts: Use read-only mounts when possible</li> <li>Resource Limits: Set appropriate CPU/memory limits</li> <li>Environment Variables: Never store secrets in configuration files</li> </ol> <pre><code># Secure Docker configuration\nconfig = DockerConfig(\n    image=\"swan:latest\",\n    user=\"appuser\",  # Non-root user\n    volumes=[\"/data:/app/data:ro\"],  # Read-only mount\n    cpu=4,  # Resource limit\n    memory=\"4g\"  # Memory limit\n)\n</code></pre>"},{"location":"developer/backend_reference/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Resource Allocation: Match resources to model requirements</li> <li>Parallel Execution: Use MPI for large models</li> <li>Image Optimization: Use optimized Docker images</li> <li>Configuration Caching: Cache validated configurations</li> <li>Monitoring: Track resource usage patterns</li> </ol> <pre><code># Performance-optimized configuration\nconfig = DockerConfig(\n    image=\"swan:hpc-optimized\",\n    cpu=16,\n    memory=\"32g\",\n    mpiexec=\"mpirun -np 16\",\n    env_vars={\n        \"OMP_NUM_THREADS\": \"1\",  # Avoid thread oversubscription\n        \"MODEL_PRECISION\": \"double\"\n    }\n)\n</code></pre>"},{"location":"developer/backend_reference/#error-handling_1","title":"Error Handling","text":"<ol> <li>Graceful Degradation: Handle errors gracefully</li> <li>Informative Messages: Provide clear error messages</li> <li>Logging: Log important events and errors</li> <li>Retry Logic: Implement retry mechanisms for transient failures</li> <li>Cleanup: Ensure proper cleanup on failure</li> </ol> <pre><code>def safe_model_execution(model_run, config):\n    \"\"\"Safely execute model with error handling.\"\"\"\n    try:\n        # Validate configuration\n        if not config.validate():\n            raise ValueError(\"Invalid configuration\")\n\n        # Execute model\n        success = model_run.run(backend=config)\n\n        if not success:\n            logger.error(\"Model execution failed\")\n            return False\n\n        return True\n\n    except Exception as e:\n        logger.error(f\"Execution error: {e}\")\n        # Cleanup logic here\n        return False\n</code></pre>"},{"location":"developer/backend_reference/#testing","title":"Testing","text":"<p>Backend configurations and implementations should be thoroughly tested.</p>"},{"location":"developer/backend_reference/#configuration-testing","title":"Configuration Testing","text":"<pre><code>import pytest\nfrom rompy.backends import LocalConfig\nfrom pydantic import ValidationError\n\ndef test_local_config_validation():\n    \"\"\"Test LocalConfig validation.\"\"\"\n    # Valid configuration\n    config = LocalConfig(timeout=3600, command=\"python test.py\")\n    assert config.timeout == 3600\n\n    # Invalid configuration\n    with pytest.raises(ValidationError):\n        LocalConfig(timeout=30)  # Too short\n</code></pre>"},{"location":"developer/backend_reference/#backend-testing","title":"Backend Testing","text":"<pre><code>def test_backend_execution():\n    \"\"\"Test backend execution.\"\"\"\n    config = LocalConfig(timeout=600, command=\"echo 'test'\")\n\n    # Mock model run\n    mock_model = create_mock_model()\n\n    # Test execution\n    backend = config.get_backend_class()()\n    success = backend.run(mock_model, config)\n\n    assert success is True\n</code></pre>"},{"location":"developer/backend_reference/#integration-testing","title":"Integration Testing","text":"<pre><code>def test_full_workflow():\n    \"\"\"Test complete workflow with backend.\"\"\"\n    model_run = ModelRun.from_file(\"test_model.yml\")\n    config = LocalConfig(timeout=1800)\n\n    # Test full workflow\n    success = model_run.run(backend=config)\n    results = model_run.postprocess(processor=\"archive\")\n\n    assert success is True\n    assert results[\"success\"] is True\n</code></pre> <p>For comprehensive testing examples, see the test suite in <code>tests/backends/</code>.</p>"},{"location":"developer/backend_reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/backend_reference/#common-issues","title":"Common Issues","text":"<p>Configuration Validation Errors : Use <code>rompy backends validate</code> to check configuration syntax and validate against schema.</p> <p>Docker Issues : Verify Docker installation, image availability, and volume mount permissions.</p> <p>Timeout Issues : Adjust timeout values based on model complexity and system performance.</p> <p>Memory Issues : Monitor memory usage and adjust allocation in Docker configurations.</p> <p>Permission Issues : Check file permissions for volume mounts and working directories.</p>"},{"location":"developer/backend_reference/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for detailed troubleshooting:</p> <pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Create debug configuration\nconfig = LocalConfig(\n    timeout=3600,\n    env_vars={\"LOG_LEVEL\": \"DEBUG\", \"MODEL_DEBUG\": \"true\"}\n)\n</code></pre>"},{"location":"developer/backend_reference/#getting-help","title":"Getting Help","text":"<ol> <li>Check Documentation: Review backends and api</li> <li>Validate Configuration: Use <code>rompy backends validate</code></li> <li>Check Logs: Review execution logs for error details</li> <li>Test Incrementally: Start with simple configurations</li> <li>Community Support: Check GitHub issues and discussions</li> </ol> <p>For additional help, see the troubleshooting section in backends or file an issue on GitHub.</p>"},{"location":"developer/backend_reference/#api-reference","title":"API Reference","text":"<p>For complete API documentation, see:</p> <ul> <li>api - Complete API documentation</li> <li><code>rompy.backends.config.BaseBackendConfig</code> - Base configuration class</li> <li><code>rompy.backends.config.LocalConfig</code> - Local execution configuration</li> <li><code>rompy.backends.config.DockerConfig</code> - Docker execution configuration</li> <li><code>rompy.run</code> - Backend implementation classes</li> <li><code>rompy.backends.postprocessors</code> - Postprocessor implementations</li> <li><code>rompy.backends</code> - Backend discovery and registry</li> </ul> <p>This reference covers the key concepts and patterns for working with ROMPY's backend system. For implementation details and complete parameter documentation, refer to the API documentation.</p>"},{"location":"developer/architecture/selection_patterns/","title":"Component Selection Patterns","text":"<p>One of the most important architectural decisions in rompy is the use of two different selection patterns for different types of functionality. Both patterns use entry points for discovery, but differ in when and how selection occurs. Understanding when and why to use each pattern is crucial for effective rompy development.</p>"},{"location":"developer/architecture/selection_patterns/#overview","title":"Overview","text":"<p>rompy uses two distinct approaches for component selection:</p> <ol> <li>Pydantic Discriminated Union Pattern for model configurations (<code>CONFIG_TYPES</code>)</li> <li>Runtime String Selection Pattern for execution backends (run, postprocess, pipeline)</li> </ol> <p>Both patterns use Python entry points for plugin discovery, but serve fundamentally different purposes. This document explains the rationale behind this dual approach and provides guidance on when to use each pattern.</p>"},{"location":"developer/architecture/selection_patterns/#the-two-patterns","title":"The Two Patterns","text":"<p>Both patterns use Python entry points for plugin discovery, but differ in when and how selection occurs.</p>"},{"location":"developer/architecture/selection_patterns/#pydantic-discriminated-union-pattern-config_types","title":"Pydantic Discriminated Union Pattern (CONFIG_TYPES)","text":"<p>Model configurations use entry points to build a discriminated union:</p> <pre><code>from typing import Union\nfrom pydantic import Field\nfrom rompy.utils import load_entry_points\n\n# Load config types from entry points at import time\nCONFIG_TYPES = load_entry_points(\"rompy.config\")\n\nclass ModelRun(RompyBaseModel):\n    config: Union[CONFIG_TYPES] = Field(\n        default_factory=BaseConfig,\n        description=\"The configuration object\",\n        discriminator=\"model_type\",  # Selection via discriminator field\n    )\n</code></pre> <p>Selection happens at model instantiation time via the <code>model_type</code> discriminator field in the configuration data.</p>"},{"location":"developer/architecture/selection_patterns/#runtime-string-selection-pattern-backends","title":"Runtime String Selection Pattern (Backends)","text":"<p>Execution backends use entry points for runtime selection:</p> <pre><code>from rompy.utils import load_entry_points\n\n# Load backends from entry points at import time\ndef _load_backends():\n    run_backends = {}\n    for backend in load_entry_points(\"rompy.run\"):\n        name = backend.__name__.lower().replace('runbackend', '')\n        run_backends[name] = backend\n    return run_backends\n\nRUN_BACKENDS = _load_backends()\n\ndef run(self, backend: str = \"local\", **kwargs) -&gt; bool:\n    # Selection happens at execution time via string parameter\n    backend_class = RUN_BACKENDS[backend]\n    backend_instance = backend_class()\n    return backend_instance.run(self, **kwargs)\n</code></pre> <p>Selection happens at execution time via string parameters passed to methods.</p>"},{"location":"developer/architecture/selection_patterns/#comparative-analysis","title":"Comparative Analysis","text":""},{"location":"developer/architecture/selection_patterns/#pydantic-discriminated-union-approach","title":"Pydantic Discriminated Union Approach","text":""},{"location":"developer/architecture/selection_patterns/#strengths","title":"\u2705 Strengths","text":"<p>Strong Type Safety : Full Pydantic validation happens at model instantiation time, catching configuration errors early in the workflow.</p> <pre><code># Validation happens here - invalid configs rejected immediately\nmodel = ModelRun(config={\"model_type\": \"swan\", \"grid\": {...}})\n</code></pre> <p>IDE Support &amp; Developer Experience : Excellent autocomplete, type checking, and refactoring support in modern IDEs.</p> <p>Serialization &amp; Reproducibility : Configuration is part of the model state and fully serializable, enabling reproducible science.</p> <pre><code># Complete model configuration saved as YAML\nconfig:\n  model_type: swan\n  grid:\n    x0: 115.68\n    y0: -32.76\n    # ... full configuration preserved\n</code></pre> <p>Schema Documentation : Clear, declarative schema with automatic documentation generation and validation rules.</p> <p>Immutability : Once instantiated, configurations are immutable, preventing accidental modification during execution.</p> <p>Plugin Support : Uses entry points for discovery, allowing third-party configuration types.</p> <pre><code># Third-party configs discovered via entry points\nCONFIG_TYPES = load_entry_points(\"rompy.config\")\n</code></pre>"},{"location":"developer/architecture/selection_patterns/#limitations","title":"\u274c Limitations","text":"<p>Selection Timing : Configuration type must be known at model instantiation time.</p> <p>State Coupling : Configuration choice becomes part of persistent model state.</p> <p>Validation Completeness : All possible configurations must be validated upfront, even if unused.</p>"},{"location":"developer/architecture/selection_patterns/#runtime-string-selection-approach","title":"Runtime String Selection Approach","text":""},{"location":"developer/architecture/selection_patterns/#strengths_1","title":"\u2705 Strengths","text":"<p>Execution-Time Flexibility : Backend choice can be made based on runtime conditions and environment.</p> <pre><code># Different backends for different environments\nbackend = \"docker\" if has_docker() else \"local\"\nmodel.run(backend=backend)\n</code></pre> <p>Operational Independence : Backend choice is independent of scientific configuration.</p> <p>Environment Adaptation : Same model configuration can use different backends based on deployment environment.</p> <pre><code># Same config, different execution strategies\nmodel.run(backend=\"local\")     # Development\nmodel.run(backend=\"slurm\")     # HPC cluster\nmodel.run(backend=\"k8s\")       # Cloud deployment\n</code></pre> <p>Plugin Support : Uses entry points for discovery, allowing third-party backends.</p> <pre><code># Third-party backends discovered via entry points\nRUN_BACKENDS = dict(load_entry_points(\"rompy.run\"))\n</code></pre> <p>Lazy Instantiation : Only instantiate backends when actually needed.</p> <p>Optional Dependencies : Graceful handling when optional backends aren't available.</p>"},{"location":"developer/architecture/selection_patterns/#limitations_1","title":"\u274c Limitations","text":"<p>Reduced Type Safety : Backend selection via strings means errors are only caught at execution time.</p> <pre><code># Error only discovered when run() is called\nmodel.run(backend=\"typo_backend\")  # ValueError at runtime\n</code></pre> <p>Late Validation : Backend availability and parameter validation happens during execution, not configuration.</p> <p>Non-Serializable Choice : Backend choice is not part of the serializable model configuration.</p> <p>Discovery Complexity : Harder to know what backends are available during development.</p>"},{"location":"developer/architecture/selection_patterns/#why-different-patterns-for-different-concerns","title":"Why Different Patterns for Different Concerns?","text":"<p>The architectural decision reflects the fundamental difference in purpose between these two types of selection, despite both using entry points:</p>"},{"location":"developer/architecture/selection_patterns/#state-vs-behavior-separation","title":"State vs Behavior Separation","text":""},{"location":"developer/architecture/selection_patterns/#configuration-represents-persistent-domain-state","title":"Configuration Represents Persistent Domain State","text":"<p>Model configurations encode scientific and mathematical knowledge that must be preserved:</p> <ul> <li>What physics to simulate (wave propagation, hydrodynamics)</li> <li>Where to simulate it (grid definition, boundaries)</li> <li>When to simulate it (time periods, forcing data)</li> </ul> <p>This domain state needs:</p> <p>Strong validation : (incorrect physics parameters = invalid science)</p> <p>Reproducibility : (same config = same results)</p> <p>Serialization : (configurations must be saveable and shareable)</p> <p>Immutability : (configurations shouldn't change during execution)</p> <p>Early validation : (catch errors before expensive computation starts)</p>"},{"location":"developer/architecture/selection_patterns/#execution-represents-runtime-behavior","title":"Execution Represents Runtime Behavior","text":"<p>Execution backends handle operational and deployment behavior:</p> <ul> <li>How to run the model (local process, container, HPC queue)</li> <li>Where to run it (laptop, cluster, cloud)</li> <li>With what resources (CPU cores, memory, time limits)</li> </ul> <p>This runtime behavior needs:</p> <p>Environment flexibility : (different options in different deployments)</p> <p>Late binding : (choose backend based on current conditions)</p> <p>Optional availability : (some backends may not be installed)</p> <p>Operational parameters : (that vary per execution, not per model)</p> <p>Ephemeral choice : (backend selection shouldn't be saved with scientific config)</p>"},{"location":"developer/architecture/selection_patterns/#practical-examples","title":"Practical Examples","text":""},{"location":"developer/architecture/selection_patterns/#configuration-example-discriminated-union","title":"Configuration Example (Discriminated Union)","text":"<p>Scientific parameters are validated, serialized, and preserved:</p> <pre><code># This represents scientific intent - must be validated and preserved\nconfig:\n  model_type: swan  # \u2190 Discriminator field for Pydantic union selection\n  grid:\n    x0: 115.68      # Geographic coordinate - must be valid\n    y0: -32.76      # Geographic coordinate - must be valid\n    dx: 0.001       # Grid resolution - affects numerical accuracy\n    dy: 0.001       # Grid resolution - affects numerical accuracy\n  physics:\n    friction: MAD   # Physics model choice - affects results\n    friction_coeff: 0.1  # Physics parameter - must be scientifically valid\n</code></pre> <p>The <code>model_type</code> field triggers Pydantic's discriminated union to select the correct configuration class. Any error in these parameters would produce scientifically invalid results, so they must be validated at instantiation time.</p>"},{"location":"developer/architecture/selection_patterns/#execution-example-runtime-string-selection","title":"Execution Example (Runtime String Selection)","text":"<p>Operational parameters vary by environment and are not serialized:</p> <pre><code># Same config object, different execution environments\nconfig_data = load_yaml(\"scientific_config.yaml\")  # Contains model_type discriminator\nmodel = ModelRun(**config_data)                     # Pydantic selects config class\n\n# Development environment - runtime string selection\nmodel.run(\n    backend=\"local\",        # \u2190 String parameter for runtime selection\n    timeout=600,\n    env_vars={\"OMP_NUM_THREADS\": \"2\"}\n)\n\n# Production HPC environment - same config, different backend\nmodel.run(\n    backend=\"slurm\",        # \u2190 Different string, same config\n    partition=\"compute\",\n    nodes=4,\n    time_limit=\"24:00:00\",\n    env_vars={\"OMP_NUM_THREADS\": \"16\"}\n)\n\n# Cloud deployment - same config, cloud backend\nmodel.run(\n    backend=\"kubernetes\",   # \u2190 Runtime choice, not saved\n    image=\"rompy/swan:v1.2.3\",\n    resources={\"cpu\": \"8\", \"memory\": \"32Gi\"}\n)\n</code></pre> <p>The same scientific configuration (with its <code>model_type</code> discriminator) runs in all environments, but with different runtime backend selections that are not part of the serializable state.</p>"},{"location":"developer/architecture/selection_patterns/#design-patterns-in-practice","title":"Design Patterns in Practice","text":""},{"location":"developer/architecture/selection_patterns/#when-to-use-discriminated-union-pattern","title":"When to Use Discriminated Union Pattern","text":"<p>Use the discriminated union pattern when extending rompy with components that need to be:</p> <p>\u2705 Part of Serializable State Components that must be saved, shared, and reproduced exactly.</p> <p>\u2705 Validated at Instantiation Components where early validation prevents expensive failures later.</p> <p>\u2705 Scientifically Critical Components where incorrect parameters lead to invalid scientific results.</p> <p>\u2705 Model Configuration Types New model types (SCHISM, XBeach, FVCOM) that define scientific computation.</p> <p>\u2705 Grid Definitions New grid types that define spatial discretization approaches.</p> <p>\u2705 Physics Parameterizations New physics options that require parameter validation and documentation.</p> <p>Example - Adding a new model type with entry point registration:</p> <pre><code>class XBeachConfig(BaseConfig):\n    \"\"\"XBeach model configuration.\"\"\"\n    model_type: Literal[\"xbeach\"] = \"xbeach\"  # Discriminator field\n\n    # Validated scientific parameters\n    grid: XBeachGrid\n    physics: XBeachPhysics\n    outputs: XBeachOutputs\n\n    # Strong validation rules\n    @validator('physics')\n    def validate_physics_consistency(cls, v, values):\n        # Ensure physics parameters are scientifically consistent\n        return v\n</code></pre> <pre><code># Register via entry points for discovery\n[project.entry-points.\"rompy.config\"]\nxbeach = \"mypackage.config:XBeachConfig\"\n</code></pre>"},{"location":"developer/architecture/selection_patterns/#when-to-use-runtime-string-selection-pattern","title":"When to Use Runtime String Selection Pattern","text":"<p>Use the runtime string selection pattern when extending rompy with components that are:</p> <p>\u2705 Environment-Specific Components that vary based on where the code is running.</p> <p>\u2705 Operationally Focused Components that handle execution, processing, or infrastructure concerns.</p> <p>\u2705 Optional Dependencies Components that may not be available in all environments.</p> <p>\u2705 Execution Environments New ways to run models (HPC schedulers, cloud platforms, containers).</p> <p>\u2705 Output Processing New analysis, visualization, or data transformation capabilities.</p> <p>\u2705 Workflow Orchestration New ways to coordinate multi-stage model workflows.</p> <p>Example - Adding a new execution backend with entry point registration:</p> <pre><code>class SlurmBackend:\n    \"\"\"Execute models via SLURM job scheduler.\"\"\"\n\n    def run(self, model_run, partition=\"compute\", nodes=1, **kwargs):\n        \"\"\"Submit model to SLURM queue.\"\"\"\n        # Generate SLURM job script\n        job_script = self._create_slurm_script(\n            model_run, partition, nodes, **kwargs\n        )\n\n        # Submit job and monitor execution\n        job_id = self._submit_job(job_script)\n        return self._wait_for_completion(job_id)\n</code></pre> <pre><code># Register via entry points for discovery\n[project.entry-points.\"rompy.run\"]\nslurm = \"rompy_hpc.backends:SlurmBackend\"\n</code></pre>"},{"location":"developer/architecture/selection_patterns/#best-practices","title":"Best Practices","text":""},{"location":"developer/architecture/selection_patterns/#for-discriminated-union-extensions-configuration","title":"For Discriminated Union Extensions (Configuration)","text":"<p>Comprehensive Validation Implement validators that check scientific and mathematical consistency.</p> <pre><code>@validator('grid_resolution')\ndef validate_resolution(cls, v):\n    if v &lt;= 0:\n        raise ValueError(\"Grid resolution must be positive\")\n    if v &gt; 0.1:\n        warnings.warn(\"Very coarse resolution may affect accuracy\")\n    return v\n</code></pre> <p>Clear Documentation Provide detailed docstrings explaining scientific meaning and valid ranges.</p> <p>Immutable Design Avoid mutable state that could change during model execution.</p> <p>Schema Versioning Plan for configuration schema evolution and backward compatibility.</p> <p>Entry Point Registration Register new configuration types via entry points for automatic discovery.</p> <pre><code># Register via entry points for discovery\n[project.entry-points.\"rompy.config\"]\nmymodel = \"mypackage.config:MyModelConfig\"\n</code></pre>"},{"location":"developer/architecture/selection_patterns/#for-runtime-string-selection-extensions-backends","title":"For Runtime String Selection Extensions (Backends)","text":"<p>Robust Error Handling Handle missing dependencies and environment issues gracefully.</p> <pre><code>def run(self, model_run, **kwargs):\n    try:\n        return self._execute_backend(model_run, **kwargs)\n    except ImportError as e:\n        raise RuntimeError(f\"Backend dependencies not available: {e}\")\n    except Exception as e:\n        logger.exception(f\"Backend execution failed: {e}\")\n        return False\n</code></pre> <p>Environment Detection Check if the backend can run in the current environment.</p> <p>Parameter Validation Validate backend-specific parameters at execution time.</p> <p>Resource Cleanup Ensure proper cleanup of resources on success and failure.</p> <p>Entry Point Registration Register new backends via entry points for automatic discovery.</p> <pre><code># Register via entry points for discovery\n[project.entry-points.\"rompy.run\"]\nmybackend = \"mypackage.backends:MyBackend\"\n</code></pre>"},{"location":"developer/architecture/selection_patterns/#conclusion","title":"Conclusion","text":"<p>The dual selection pattern in rompy reflects a sophisticated understanding of different types of component selection requirements:</p> <ul> <li>State-based selection (configurations) needs early validation, serialization, and reproducibility</li> <li>Behavior-based selection (backends) needs late binding, environment adaptation, and optional availability</li> </ul> <p>Both patterns use entry points for plugin discovery, but differ fundamentally in when selection occurs and what gets serialized:</p> <ul> <li>Configurations: Selected at instantiation time via discriminator fields, become part of persistent state</li> <li>Backends: Selected at execution time via string parameters, remain ephemeral operational choices</li> </ul> <p>This architectural decision enables rompy to maintain scientific rigor while supporting diverse computational environments. When extending rompy, carefully consider whether your extension represents:</p> <ul> <li>Persistent domain state \u2192 Use discriminated unions with entry point discovery</li> <li>Runtime behavior choice \u2192 Use string selection with entry point discovery</li> </ul> <p>The pattern demonstrates that the same plugin discovery mechanism can serve different selection patterns, and a well-designed system should choose the selection timing and state management approach that best fits the component's purpose.</p>"},{"location":"developer/architecture/selection_patterns/#further-reading","title":"Further Reading","text":"<ul> <li>Custom Backends - Practical guide to creating new backends</li> <li>Custom Models - Guide to adding new model configurations</li> <li>Entry Points - Technical details on the entry point system</li> <li>Configuration Patterns - Deep dive into configuration design patterns</li> </ul>"},{"location":"plugin_architecture/execution/","title":"Execution and Output Processing Plugin Architecture","text":"<p>Rompy features a flexible plugin-based architecture that allows for extensible model execution and output processing. The system uses Python entry points to automatically discover and load backends, making it easy to extend with custom implementations.</p>"},{"location":"plugin_architecture/execution/#overview","title":"Overview","text":"<p>The plugin architecture is built around three main categories:</p> <ol> <li>Run Backends (<code>rompy.run</code>): Handle model execution in different environments</li> <li>Postprocessors (<code>rompy.postprocess</code>): Handle model output analysis and transformation</li> <li>Pipeline Backends (<code>rompy.pipeline</code>): Orchestrate complete model workflows</li> </ol> <p>Each category uses Python entry points for automatic discovery and loading, allowing third-party packages to easily extend rompy's capabilities.</p>"},{"location":"plugin_architecture/execution/#run-backends","title":"Run Backends","text":"<p>Run backends are responsible for executing models in different environments. They all implement a common interface with a <code>run()</code> method.</p>"},{"location":"plugin_architecture/execution/#built-in-run-backends","title":"Built-in Run Backends","text":""},{"location":"plugin_architecture/execution/#local-backend","title":"Local Backend","text":"<p>The <code>local</code> backend executes models directly on the local system:</p> <pre><code># Basic local execution\nsuccess = model.run(backend=\"local\")\n\n# With custom command\nsuccess = model.run(\n    backend=\"local\",\n    command=\"./my_model_executable\",\n    env_vars={\"OMP_NUM_THREADS\": \"4\"},\n    timeout=3600\n)\n</code></pre>"},{"location":"plugin_architecture/execution/#docker-backend","title":"Docker Backend","text":"<p>The <code>docker</code> backend executes models inside Docker containers:</p> <pre><code># Using pre-built image\nsuccess = model.run(\n    backend=\"docker\",\n    image=\"rompy/schism:latest\",\n    executable=\"/usr/local/bin/schism\",\n    cpu=4,\n    volumes=[\"./data:/data:ro\"],\n    env_vars={\"MODEL_CONFIG\": \"production\"}\n)\n\n# Building from Dockerfile\nsuccess = model.run(\n    backend=\"docker\",\n    dockerfile=\"./docker/Dockerfile\",\n    build_args={\"MODEL_VERSION\": \"1.0.0\"},\n    executable=\"/usr/local/bin/model\",\n    mpiexec=\"mpiexec\",\n    cpu=8\n)\n</code></pre>"},{"location":"plugin_architecture/execution/#custom-run-backends","title":"Custom Run Backends","text":"<p>You can create custom run backends by implementing the run interface:</p> <pre><code>class CustomRunBackend:\n    \"\"\"Custom run backend example.\"\"\"\n\n    def run(self, model_run, **kwargs):\n        \"\"\"Execute the model run.\n\n        Args:\n            model_run: The ModelRun instance\n            **kwargs: Backend-specific parameters\n\n        Returns:\n            bool: True if successful, False otherwise\n        \"\"\"\n        try:\n            # Generate model inputs\n            model_run.generate()\n\n            # Custom execution logic here\n            return self._execute_custom_logic(model_run, **kwargs)\n\n        except Exception as e:\n            logger.exception(f\"Custom backend failed: {e}\")\n            return False\n</code></pre> <p>Register custom backends via entry points in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"rompy.run\"]\ncustom = \"mypackage.backends:CustomRunBackend\"\n</code></pre>"},{"location":"plugin_architecture/execution/#postprocessors","title":"Postprocessors","text":"<p>Postprocessors handle analysis and transformation of model outputs. They implement a <code>process()</code> method that returns a dictionary with results.</p>"},{"location":"plugin_architecture/execution/#built-in-postprocessors","title":"Built-in Postprocessors","text":""},{"location":"plugin_architecture/execution/#no-op-processor","title":"No-op Processor","text":"<p>The <code>noop</code> processor provides basic validation without processing:</p> <pre><code># Basic validation\nresults = model.postprocess(processor=\"noop\")\n\n# With custom validation\nresults = model.postprocess(\n    processor=\"noop\",\n    validate_outputs=True,\n    output_dir=\"./custom_output\"\n)\n</code></pre>"},{"location":"plugin_architecture/execution/#custom-postprocessors","title":"Custom Postprocessors","text":"<p>Create custom postprocessors by implementing the process interface:</p> <pre><code>class AnalysisPostprocessor:\n    \"\"\"Custom postprocessor for model analysis.\"\"\"\n\n    def process(self, model_run, **kwargs):\n        \"\"\"Process model outputs.\n\n        Args:\n            model_run: The ModelRun instance\n            **kwargs: Processor-specific parameters\n\n        Returns:\n            dict: Processing results\n        \"\"\"\n        try:\n            output_dir = Path(model_run.output_dir) / model_run.run_id\n\n            # Custom analysis logic\n            metrics = self._calculate_metrics(output_dir)\n            plots = self._generate_plots(output_dir)\n\n            return {\n                \"success\": True,\n                \"metrics\": metrics,\n                \"plots\": plots,\n                \"message\": \"Analysis completed successfully\"\n            }\n\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"message\": f\"Analysis failed: {e}\"\n            }\n</code></pre> <p>Register via entry points:</p> <pre><code>[project.entry-points.\"rompy.postprocess\"]\nanalysis = \"mypackage.processors:AnalysisPostprocessor\"\n</code></pre>"},{"location":"plugin_architecture/execution/#pipeline-backends","title":"Pipeline Backends","text":"<p>Pipeline backends orchestrate the complete model workflow from input generation through execution to output processing.</p>"},{"location":"plugin_architecture/execution/#built-in-pipeline-backends","title":"Built-in Pipeline Backends","text":""},{"location":"plugin_architecture/execution/#local-pipeline","title":"Local Pipeline","text":"<p>The <code>local</code> pipeline executes all stages locally:</p> <pre><code># Basic pipeline\nresults = model.pipeline(pipeline_backend=\"local\")\n\n# With custom backends\nresults = model.pipeline(\n    pipeline_backend=\"local\",\n    run_backend=\"docker\",\n    processor=\"analysis\",\n    run_kwargs={\"image\": \"rompy/model:latest\", \"cpu\": 4},\n    process_kwargs={\"create_plots\": True},\n    cleanup_on_failure=True\n)\n</code></pre>"},{"location":"plugin_architecture/execution/#custom-pipeline-backends","title":"Custom Pipeline Backends","text":"<p>Create custom pipeline backends for distributed or cloud execution:</p> <pre><code>class CloudPipelineBackend:\n    \"\"\"Pipeline backend for cloud execution.\"\"\"\n\n    def execute(self, model_run, **kwargs):\n        \"\"\"Execute the complete pipeline.\n\n        Args:\n            model_run: The ModelRun instance\n            **kwargs: Pipeline-specific parameters\n\n        Returns:\n            dict: Pipeline execution results\n        \"\"\"\n        results = {\n            \"success\": False,\n            \"run_id\": model_run.run_id,\n            \"stages_completed\": []\n        }\n\n        try:\n            # Stage 1: Generate inputs\n            model_run.generate()\n            results[\"stages_completed\"].append(\"generate\")\n\n            # Stage 2: Submit to cloud\n            job_id = self._submit_cloud_job(model_run, **kwargs)\n            results[\"job_id\"] = job_id\n            results[\"stages_completed\"].append(\"submit\")\n\n            # Stage 3: Wait for completion\n            self._wait_for_completion(job_id)\n            results[\"stages_completed\"].append(\"execute\")\n\n            # Stage 4: Download and process results\n            outputs = self._download_results(job_id)\n            processed = self._process_outputs(outputs, **kwargs)\n            results[\"outputs\"] = processed\n            results[\"stages_completed\"].append(\"postprocess\")\n\n            results[\"success\"] = True\n            return results\n\n        except Exception as e:\n            results[\"error\"] = str(e)\n            return results\n</code></pre>"},{"location":"plugin_architecture/execution/#best-practices","title":"Best Practices","text":""},{"location":"plugin_architecture/execution/#error-handling","title":"Error Handling","text":"<ul> <li>Always wrap main logic in try-catch blocks</li> <li>Return appropriate boolean/dict responses</li> <li>Log errors with sufficient detail for debugging</li> <li>Clean up resources on failure when possible</li> </ul>"},{"location":"plugin_architecture/execution/#parameter-validation","title":"Parameter Validation","text":"<ul> <li>Validate required parameters early</li> <li>Provide clear error messages for invalid inputs</li> <li>Use type hints for better IDE support</li> <li>Document all parameters in docstrings</li> </ul>"},{"location":"plugin_architecture/execution/#logging","title":"Logging","text":"<ul> <li>Use structured logging with appropriate levels</li> <li>Include run_id and context in log messages</li> <li>Log progress for long-running operations</li> <li>Avoid logging sensitive information</li> </ul>"},{"location":"plugin_architecture/execution/#resource-management","title":"Resource Management","text":"<ul> <li>Clean up temporary files and directories</li> <li>Handle timeouts gracefully</li> <li>Implement proper cancellation mechanisms</li> <li>Monitor resource usage for long-running processes</li> </ul>"},{"location":"plugin_architecture/execution/#testing","title":"Testing","text":"<ul> <li>Write unit tests for all backend methods</li> <li>Mock external dependencies (Docker, cloud APIs)</li> <li>Test error conditions and edge cases</li> <li>Include integration tests where appropriate</li> </ul>"},{"location":"plugin_architecture/execution/#examples","title":"Examples","text":"<p>Complete examples demonstrating the plugin architecture can be found in the <code>examples/backends/</code> directory:</p> <ul> <li><code>01_basic_local_run.py</code>: Simple local execution</li> <li><code>02_docker_run.py</code>: Docker container execution</li> <li><code>03_custom_postprocessor.py</code>: Custom output processing</li> <li><code>04_complete_workflow.py</code>: End-to-end custom workflow</li> </ul> <p>For interactive examples, see the <code>notebooks/backend_examples.ipynb</code> notebook.</p>"},{"location":"plugin_architecture/execution/#api-reference","title":"API Reference","text":""},{"location":"plugin_architecture/execution/#rompy.run","title":"run","text":"<p>Local execution backend for model runs.</p> <p>This module provides the local run backend implementation.</p>"},{"location":"plugin_architecture/execution/#rompy.run-attributes","title":"Attributes","text":""},{"location":"plugin_architecture/execution/#rompy.run.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"plugin_architecture/execution/#rompy.run-classes","title":"Classes","text":""},{"location":"plugin_architecture/execution/#rompy.run.LocalRunBackend","title":"LocalRunBackend","text":"<p>Execute models locally using the system's Python interpreter.</p> <p>This is the simplest backend that just runs the model directly on the local system.</p> Source code in <code>rompy/run/__init__.py</code> <pre><code>class LocalRunBackend:\n    \"\"\"Execute models locally using the system's Python interpreter.\n\n    This is the simplest backend that just runs the model directly\n    on the local system.\n    \"\"\"\n\n    def run(\n        self, model_run, config: \"LocalConfig\", workspace_dir: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Run the model locally.\n\n        Args:\n            model_run: The ModelRun instance to execute\n            config: LocalConfig instance with execution parameters\n            workspace_dir: Path to the generated workspace directory (if None, will generate)\n\n        Returns:\n            True if execution was successful, False otherwise\n\n        Raises:\n            ValueError: If model_run is invalid\n            TimeoutError: If execution exceeds timeout\n        \"\"\"\n        # Validate input parameters\n        if not model_run:\n            raise ValueError(\"model_run cannot be None\")\n\n        if not hasattr(model_run, \"run_id\"):\n            raise ValueError(\"model_run must have a run_id attribute\")\n\n        # Use config parameters\n        exec_command = config.command\n        exec_working_dir = config.working_dir\n        exec_env_vars = config.env_vars\n        exec_timeout = config.timeout\n        exec_stream_output = getattr(config, \"stream_output\", False)\n\n        logger.debug(\n            f\"Using LocalConfig: timeout={exec_timeout}, env_vars={list(exec_env_vars.keys())}\"\n        )\n\n        logger.info(f\"Starting local execution for run_id: {model_run.run_id}\")\n\n        try:\n            # Use provided workspace or generate if not provided (for backwards compatibility)\n            if workspace_dir is None:\n                logger.warning(\n                    \"No workspace_dir provided, generating files (this may cause double generation in pipeline)\"\n                )\n                staging_dir = model_run.generate()\n                logger.info(f\"Model inputs generated in: {staging_dir}\")\n            else:\n                logger.info(f\"Using provided workspace directory: {workspace_dir}\")\n                staging_dir = workspace_dir\n\n            # Set working directory\n            if exec_working_dir:\n                work_dir = Path(exec_working_dir)\n            else:\n                work_dir = (\n                    Path(staging_dir)\n                    if staging_dir\n                    else Path(model_run.output_dir) / model_run.run_id\n                )\n\n            if not work_dir.exists():\n                logger.error(f\"Working directory does not exist: {work_dir}\")\n                return False\n\n            # Prepare environment\n            env = os.environ.copy()\n            if exec_env_vars:\n                env.update(exec_env_vars)\n                logger.debug(\n                    f\"Added environment variables: {list(exec_env_vars.keys())}\"\n                )\n\n            # Execute command or config.run()\n            if exec_command:\n                success = self._execute_command(\n                    exec_command, work_dir, env, exec_timeout, exec_stream_output\n                )\n            else:\n                success = self._execute_config_run(model_run, work_dir, env)\n\n            if success:\n                logger.info(\n                    f\"Local execution completed successfully for run_id: {model_run.run_id}\"\n                )\n            else:\n                logger.error(f\"Local execution failed for run_id: {model_run.run_id}\")\n\n            return success\n\n        except TimeoutError:\n            logger.error(f\"Model execution timed out after {exec_timeout} seconds\")\n            raise\n        except Exception as e:\n            logger.exception(f\"Failed to run model locally: {e}\")\n            return False\n\n    def _execute_command(\n        self,\n        command: str,\n        work_dir: Path,\n        env: Dict[str, str],\n        timeout: Optional[int],\n        stream_output: bool = False,\n    ) -&gt; bool:\n        \"\"\"Execute a shell command.\n\n        Args:\n            command: Command to execute\n            work_dir: Working directory\n            env: Environment variables\n            timeout: Execution timeout\n            stream_output: Whether to stream output in real-time\n\n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        if stream_output:\n            return self._execute_command_streaming(command, work_dir, env, timeout)\n        else:\n            return self._execute_command_buffered(command, work_dir, env, timeout)\n\n    def _execute_command_buffered(\n        self, command: str, work_dir: Path, env: Dict[str, str], timeout: Optional[int]\n    ) -&gt; bool:\n        \"\"\"Execute a shell command with buffered output.\"\"\"\n        logger.info(f\"Executing command: {command}\")\n        logger.debug(f\"Working directory: {work_dir}\")\n\n        try:\n            result = subprocess.run(\n                command,\n                shell=True,\n                cwd=work_dir,\n                env=env,\n                timeout=timeout,\n                capture_output=True,\n                text=True,\n                check=False,\n            )\n\n            if result.stdout:\n                logger.info(f\"Command stdout:\\n{result.stdout}\")\n            if result.stderr:\n                if result.returncode == 0:\n                    logger.warning(f\"Command stderr:\\n{result.stderr}\")\n                else:\n                    logger.error(f\"Command stderr:\\n{result.stderr}\")\n\n            if result.returncode == 0:\n                logger.debug(\"Command completed successfully\")\n                return True\n            else:\n                logger.error(f\"Command failed with return code: {result.returncode}\")\n                return False\n\n        except subprocess.TimeoutExpired:\n            logger.error(f\"Command timed out after {timeout} seconds\")\n            raise TimeoutError(f\"Command execution timed out after {timeout} seconds\")\n        except Exception as e:\n            logger.exception(f\"Error executing command: {e}\")\n            return False\n\n    def _execute_command_streaming(\n        self, command: str, work_dir: Path, env: Dict[str, str], timeout: Optional[int]\n    ) -&gt; bool:\n        \"\"\"Execute a shell command with streaming output.\"\"\"\n        logger.info(f\"Executing command: {command}\")\n        logger.debug(f\"Working directory: {work_dir}\")\n\n        try:\n            process = subprocess.Popen(\n                command,\n                shell=True,\n                cwd=work_dir,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                bufsize=1,\n            )\n\n            # Capture output while streaming\n            stdout_lines = []\n            stderr_lines = []\n\n            def read_stream(stream, lines, log_func):\n                \"\"\"Read from a stream and log each line.\"\"\"\n                for line in stream:\n                    line = line.rstrip()\n                    lines.append(line)\n                    log_func(line)\n\n            # Start threads to read stdout and stderr concurrently\n            stdout_thread = threading.Thread(\n                target=read_stream,\n                args=(process.stdout, stdout_lines, logger.info),\n            )\n            stderr_thread = threading.Thread(\n                target=read_stream,\n                args=(process.stderr, stderr_lines, lambda msg: logger.warning(msg)),\n            )\n\n            stdout_thread.start()\n            stderr_thread.start()\n\n            # Wait for process to complete with timeout\n            try:\n                returncode = process.wait(timeout=timeout)\n            except subprocess.TimeoutExpired:\n                process.kill()\n                process.wait()\n                logger.error(f\"Command timed out after {timeout} seconds\")\n                raise TimeoutError(\n                    f\"Command execution timed out after {timeout} seconds\"\n                )\n\n            # Wait for reader threads to finish\n            stdout_thread.join()\n            stderr_thread.join()\n\n            # Log remaining stderr if any (after process completed)\n            # (Thread already handled most output, but capture any final lines)\n\n            if returncode == 0:\n                logger.debug(\"Command completed successfully\")\n                return True\n            else:\n                logger.error(f\"Command failed with return code: {returncode}\")\n                if stderr_lines:\n                    logger.error(\"Command stderr:\\n\" + \"\\n\".join(stderr_lines))\n                return False\n\n        except Exception as e:\n            logger.exception(f\"Error executing command: {e}\")\n            return False\n\n    def _execute_config_run(\n        self, model_run, work_dir: Path, env: Dict[str, str]\n    ) -&gt; bool:\n        \"\"\"Execute the model using config.run() method.\n\n        Args:\n            model_run: The ModelRun instance\n            work_dir: Working directory\n            env: Environment variables\n\n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        # Check if config has a run method\n        if not hasattr(model_run.config, \"run\") or not callable(model_run.config.run):\n            logger.warning(\n                \"Model config does not have a run method. Nothing to execute.\"\n            )\n            return True\n\n        logger.info(\"Executing model using config.run() method\")\n\n        try:\n            # Set working directory in environment for config.run()\n            original_cwd = os.getcwd()\n            os.chdir(work_dir)\n\n            # Update environment\n            original_env = {}\n            for key, value in env.items():\n                if key in os.environ:\n                    original_env[key] = os.environ[key]\n                os.environ[key] = value\n\n            try:\n                # Execute the config run method\n                result = model_run.config.run(model_run)\n\n                if isinstance(result, bool):\n                    return result\n                else:\n                    logger.warning(f\"config.run() returned non-boolean value: {result}\")\n                    return True\n\n            finally:\n                # Restore original environment and directory\n                os.chdir(original_cwd)\n                for key, value in env.items():\n                    if key in original_env:\n                        os.environ[key] = original_env[key]\n                    else:\n                        os.environ.pop(key, None)\n\n        except Exception as e:\n            logger.exception(f\"Error in config.run(): {e}\")\n            return False\n</code></pre>"},{"location":"plugin_architecture/execution/#rompy.run.LocalRunBackend-functions","title":"Functions","text":""},{"location":"plugin_architecture/execution/#rompy.run.LocalRunBackend.run","title":"run","text":"<pre><code>run(model_run, config: LocalConfig, workspace_dir: Optional[str] = None) -&gt; bool\n</code></pre> <p>Run the model locally.</p> <p>Parameters:</p> Name Type Description Default <code>model_run</code> <p>The ModelRun instance to execute</p> required <code>config</code> <code>LocalConfig</code> <p>LocalConfig instance with execution parameters</p> required <code>workspace_dir</code> <code>Optional[str]</code> <p>Path to the generated workspace directory (if None, will generate)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if execution was successful, False otherwise</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_run is invalid</p> <code>TimeoutError</code> <p>If execution exceeds timeout</p> Source code in <code>rompy/run/__init__.py</code> <pre><code>def run(\n    self, model_run, config: \"LocalConfig\", workspace_dir: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Run the model locally.\n\n    Args:\n        model_run: The ModelRun instance to execute\n        config: LocalConfig instance with execution parameters\n        workspace_dir: Path to the generated workspace directory (if None, will generate)\n\n    Returns:\n        True if execution was successful, False otherwise\n\n    Raises:\n        ValueError: If model_run is invalid\n        TimeoutError: If execution exceeds timeout\n    \"\"\"\n    # Validate input parameters\n    if not model_run:\n        raise ValueError(\"model_run cannot be None\")\n\n    if not hasattr(model_run, \"run_id\"):\n        raise ValueError(\"model_run must have a run_id attribute\")\n\n    # Use config parameters\n    exec_command = config.command\n    exec_working_dir = config.working_dir\n    exec_env_vars = config.env_vars\n    exec_timeout = config.timeout\n    exec_stream_output = getattr(config, \"stream_output\", False)\n\n    logger.debug(\n        f\"Using LocalConfig: timeout={exec_timeout}, env_vars={list(exec_env_vars.keys())}\"\n    )\n\n    logger.info(f\"Starting local execution for run_id: {model_run.run_id}\")\n\n    try:\n        # Use provided workspace or generate if not provided (for backwards compatibility)\n        if workspace_dir is None:\n            logger.warning(\n                \"No workspace_dir provided, generating files (this may cause double generation in pipeline)\"\n            )\n            staging_dir = model_run.generate()\n            logger.info(f\"Model inputs generated in: {staging_dir}\")\n        else:\n            logger.info(f\"Using provided workspace directory: {workspace_dir}\")\n            staging_dir = workspace_dir\n\n        # Set working directory\n        if exec_working_dir:\n            work_dir = Path(exec_working_dir)\n        else:\n            work_dir = (\n                Path(staging_dir)\n                if staging_dir\n                else Path(model_run.output_dir) / model_run.run_id\n            )\n\n        if not work_dir.exists():\n            logger.error(f\"Working directory does not exist: {work_dir}\")\n            return False\n\n        # Prepare environment\n        env = os.environ.copy()\n        if exec_env_vars:\n            env.update(exec_env_vars)\n            logger.debug(\n                f\"Added environment variables: {list(exec_env_vars.keys())}\"\n            )\n\n        # Execute command or config.run()\n        if exec_command:\n            success = self._execute_command(\n                exec_command, work_dir, env, exec_timeout, exec_stream_output\n            )\n        else:\n            success = self._execute_config_run(model_run, work_dir, env)\n\n        if success:\n            logger.info(\n                f\"Local execution completed successfully for run_id: {model_run.run_id}\"\n            )\n        else:\n            logger.error(f\"Local execution failed for run_id: {model_run.run_id}\")\n\n        return success\n\n    except TimeoutError:\n        logger.error(f\"Model execution timed out after {exec_timeout} seconds\")\n        raise\n    except Exception as e:\n        logger.exception(f\"Failed to run model locally: {e}\")\n        return False\n</code></pre>"},{"location":"plugin_architecture/execution/#rompy.postprocess","title":"postprocess","text":"<p>No-op postprocessor for model outputs.</p> <p>This module provides a basic postprocessor that does nothing.</p>"},{"location":"plugin_architecture/execution/#rompy.postprocess-attributes","title":"Attributes","text":""},{"location":"plugin_architecture/execution/#rompy.postprocess.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"plugin_architecture/execution/#rompy.postprocess-classes","title":"Classes","text":""},{"location":"plugin_architecture/execution/#rompy.postprocess.NoopPostprocessor","title":"NoopPostprocessor","text":"<p>A postprocessor that does nothing.</p> <p>This is a placeholder implementation that simply returns a success message. It's useful as a base class or for testing.</p> Source code in <code>rompy/postprocess/__init__.py</code> <pre><code>class NoopPostprocessor:\n    \"\"\"A postprocessor that does nothing.\n\n    This is a placeholder implementation that simply returns a success message.\n    It's useful as a base class or for testing.\n    \"\"\"\n\n    def process(\n        self,\n        model_run,\n        validate_outputs: bool = True,\n        output_dir: Optional[Union[str, Path]] = None,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Process the output of a model run (does nothing).\n\n        Args:\n            model_run: The ModelRun instance whose outputs to process\n            validate_outputs: Whether to validate that output directory exists\n            output_dir: Override output directory to check (defaults to model_run output)\n            **kwargs: Additional parameters (unused)\n\n        Returns:\n            Dictionary with processing results\n\n        Raises:\n            ValueError: If model_run is invalid\n        \"\"\"\n        # Validate input parameters\n        if not model_run:\n            raise ValueError(\"model_run cannot be None\")\n\n        if not hasattr(model_run, \"run_id\"):\n            raise ValueError(\"model_run must have a run_id attribute\")\n\n        logger.info(f\"Starting no-op postprocessing for run_id: {model_run.run_id}\")\n\n        try:\n            # Determine output directory\n            if output_dir:\n                check_dir = Path(output_dir)\n            else:\n                check_dir = Path(model_run.output_dir) / model_run.run_id\n\n            # Validate outputs if requested\n            if validate_outputs:\n                if not check_dir.exists():\n                    logger.warning(f\"Output directory does not exist: {check_dir}\")\n                    return {\n                        \"success\": False,\n                        \"message\": f\"Output directory not found: {check_dir}\",\n                        \"run_id\": model_run.run_id,\n                        \"output_dir\": str(check_dir),\n                    }\n                else:\n                    # Count files in output directory\n                    file_count = sum(1 for f in check_dir.rglob(\"*\") if f.is_file())\n                    logger.info(f\"Found {file_count} output files in {check_dir}\")\n\n            logger.info(\n                f\"No-op postprocessing completed for run_id: {model_run.run_id}\"\n            )\n\n            return {\n                \"success\": True,\n                \"message\": \"No postprocessing requested - validation only\",\n                \"run_id\": model_run.run_id,\n                \"output_dir\": str(check_dir),\n                \"validated\": validate_outputs,\n            }\n\n        except Exception as e:\n            logger.exception(f\"Error in no-op postprocessor: {e}\")\n            return {\n                \"success\": False,\n                \"message\": f\"Error in postprocessor: {str(e)}\",\n                \"run_id\": getattr(model_run, \"run_id\", \"unknown\"),\n                \"error\": str(e),\n            }\n</code></pre>"},{"location":"plugin_architecture/execution/#rompy.postprocess.NoopPostprocessor-functions","title":"Functions","text":""},{"location":"plugin_architecture/execution/#rompy.postprocess.NoopPostprocessor.process","title":"process","text":"<pre><code>process(model_run, validate_outputs: bool = True, output_dir: Optional[Union[str, Path]] = None, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Process the output of a model run (does nothing).</p> <p>Parameters:</p> Name Type Description Default <code>model_run</code> <p>The ModelRun instance whose outputs to process</p> required <code>validate_outputs</code> <code>bool</code> <p>Whether to validate that output directory exists</p> <code>True</code> <code>output_dir</code> <code>Optional[Union[str, Path]]</code> <p>Override output directory to check (defaults to model_run output)</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters (unused)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with processing results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_run is invalid</p> Source code in <code>rompy/postprocess/__init__.py</code> <pre><code>def process(\n    self,\n    model_run,\n    validate_outputs: bool = True,\n    output_dir: Optional[Union[str, Path]] = None,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"Process the output of a model run (does nothing).\n\n    Args:\n        model_run: The ModelRun instance whose outputs to process\n        validate_outputs: Whether to validate that output directory exists\n        output_dir: Override output directory to check (defaults to model_run output)\n        **kwargs: Additional parameters (unused)\n\n    Returns:\n        Dictionary with processing results\n\n    Raises:\n        ValueError: If model_run is invalid\n    \"\"\"\n    # Validate input parameters\n    if not model_run:\n        raise ValueError(\"model_run cannot be None\")\n\n    if not hasattr(model_run, \"run_id\"):\n        raise ValueError(\"model_run must have a run_id attribute\")\n\n    logger.info(f\"Starting no-op postprocessing for run_id: {model_run.run_id}\")\n\n    try:\n        # Determine output directory\n        if output_dir:\n            check_dir = Path(output_dir)\n        else:\n            check_dir = Path(model_run.output_dir) / model_run.run_id\n\n        # Validate outputs if requested\n        if validate_outputs:\n            if not check_dir.exists():\n                logger.warning(f\"Output directory does not exist: {check_dir}\")\n                return {\n                    \"success\": False,\n                    \"message\": f\"Output directory not found: {check_dir}\",\n                    \"run_id\": model_run.run_id,\n                    \"output_dir\": str(check_dir),\n                }\n            else:\n                # Count files in output directory\n                file_count = sum(1 for f in check_dir.rglob(\"*\") if f.is_file())\n                logger.info(f\"Found {file_count} output files in {check_dir}\")\n\n        logger.info(\n            f\"No-op postprocessing completed for run_id: {model_run.run_id}\"\n        )\n\n        return {\n            \"success\": True,\n            \"message\": \"No postprocessing requested - validation only\",\n            \"run_id\": model_run.run_id,\n            \"output_dir\": str(check_dir),\n            \"validated\": validate_outputs,\n        }\n\n    except Exception as e:\n        logger.exception(f\"Error in no-op postprocessor: {e}\")\n        return {\n            \"success\": False,\n            \"message\": f\"Error in postprocessor: {str(e)}\",\n            \"run_id\": getattr(model_run, \"run_id\", \"unknown\"),\n            \"error\": str(e),\n        }\n</code></pre>"},{"location":"plugin_architecture/execution/#rompy.pipeline","title":"pipeline","text":"<p>Local pipeline backend for model execution.</p> <p>This module provides the local pipeline backend implementation.</p>"},{"location":"plugin_architecture/execution/#rompy.pipeline-attributes","title":"Attributes","text":""},{"location":"plugin_architecture/execution/#rompy.pipeline.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"plugin_architecture/execution/#rompy.pipeline-classes","title":"Classes","text":""},{"location":"plugin_architecture/execution/#rompy.pipeline.LocalPipelineBackend","title":"LocalPipelineBackend","text":"<p>Local pipeline backend that executes the full workflow locally.</p> <p>This backend uses the existing generate(), run() and postprocess() methods to execute the complete pipeline locally.</p> Source code in <code>rompy/pipeline/__init__.py</code> <pre><code>class LocalPipelineBackend:\n    \"\"\"Local pipeline backend that executes the full workflow locally.\n\n    This backend uses the existing generate(), run() and postprocess() methods\n    to execute the complete pipeline locally.\n    \"\"\"\n\n    def execute(\n        self,\n        model_run,\n        run_backend: str = \"local\",\n        processor: str = \"noop\",\n        run_kwargs: Optional[Dict[str, Any]] = None,\n        process_kwargs: Optional[Dict[str, Any]] = None,\n        cleanup_on_failure: bool = False,\n        validate_stages: bool = True,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the model pipeline locally.\n\n        Args:\n            model_run: The ModelRun instance to execute\n            run_backend: Backend to use for the run stage (\"local\" or \"docker\")\n            processor: Processor to use for the postprocess stage\n            run_kwargs: Additional parameters for the run stage\n            process_kwargs: Additional parameters for the postprocess stage\n            cleanup_on_failure: Whether to cleanup outputs on pipeline failure\n            validate_stages: Whether to validate each stage before proceeding\n            **kwargs: Additional parameters (unused)\n\n        Returns:\n            Combined results from the pipeline execution\n\n        Raises:\n            ValueError: If model_run is invalid or parameters are invalid\n        \"\"\"\n        # Validate input parameters\n        if not model_run:\n            raise ValueError(\"model_run cannot be None\")\n\n        if not hasattr(model_run, \"run_id\"):\n            raise ValueError(\"model_run must have a run_id attribute\")\n\n        if not isinstance(run_backend, str) or not run_backend.strip():\n            raise ValueError(\"run_backend must be a non-empty string\")\n\n        if not isinstance(processor, str) or not processor.strip():\n            raise ValueError(\"processor must be a non-empty string\")\n\n        # Initialize parameters\n        run_kwargs = run_kwargs or {}\n        process_kwargs = process_kwargs or {}\n\n        logger.info(f\"Starting pipeline execution for run_id: {model_run.run_id}\")\n        logger.info(\n            f\"Pipeline configuration: run_backend='{run_backend}', processor='{processor}'\"\n        )\n\n        pipeline_results = {\n            \"success\": False,\n            \"run_id\": model_run.run_id,\n            \"stages_completed\": [],\n            \"run_backend\": run_backend,\n            \"processor\": processor,\n        }\n\n        try:\n            # Stage 1: Generate input files\n            logger.info(f\"Stage 1: Generating input files for {model_run.run_id}\")\n\n            try:\n                staging_dir = model_run.generate()\n                pipeline_results[\"staging_dir\"] = (\n                    str(staging_dir) if staging_dir else None\n                )\n                pipeline_results[\"stages_completed\"].append(\"generate\")\n                logger.info(f\"Input files generated successfully in: {staging_dir}\")\n            except Exception as e:\n                logger.exception(f\"Failed to generate input files: {e}\")\n                return {\n                    **pipeline_results,\n                    \"stage\": \"generate\",\n                    \"message\": f\"Input file generation failed: {str(e)}\",\n                    \"error\": str(e),\n                }\n\n            # Validate generation stage\n            if validate_stages:\n                output_dir = Path(model_run.output_dir) / model_run.run_id\n                if not output_dir.exists():\n                    logger.error(f\"Output directory was not created: {output_dir}\")\n                    return {\n                        **pipeline_results,\n                        \"stage\": \"generate\",\n                        \"message\": f\"Output directory not found after generation: {output_dir}\",\n                    }\n\n            # Stage 2: Run the model\n            logger.info(f\"Stage 2: Running model using {run_backend} backend\")\n\n            try:\n                # Create appropriate backend configuration\n                backend_config = self._create_backend_config(run_backend, run_kwargs)\n\n                # Pass the generated workspace directory to avoid duplicate generation\n                run_success = model_run.run(\n                    backend=backend_config, workspace_dir=staging_dir\n                )\n                pipeline_results[\"run_success\"] = run_success\n\n                if not run_success:\n                    logger.error(\"Model run failed\")\n                    if cleanup_on_failure:\n                        self._cleanup_outputs(model_run)\n                    return {\n                        **pipeline_results,\n                        \"stage\": \"run\",\n                        \"message\": \"Model run failed\",\n                    }\n\n                pipeline_results[\"stages_completed\"].append(\"run\")\n                logger.info(\"Model run completed successfully\")\n\n            except Exception as e:\n                logger.exception(f\"Error during model run: {e}\")\n                if cleanup_on_failure:\n                    self._cleanup_outputs(model_run)\n                return {\n                    **pipeline_results,\n                    \"stage\": \"run\",\n                    \"message\": f\"Model run error: {str(e)}\",\n                    \"error\": str(e),\n                }\n\n            # Stage 3: Postprocess outputs\n            logger.info(f\"Stage 3: Postprocessing with {processor}\")\n\n            try:\n                postprocess_results = model_run.postprocess(\n                    processor=processor, **process_kwargs\n                )\n                pipeline_results[\"postprocess_results\"] = postprocess_results\n                pipeline_results[\"stages_completed\"].append(\"postprocess\")\n\n                # Check if postprocessing was successful\n                if isinstance(\n                    postprocess_results, dict\n                ) and not postprocess_results.get(\"success\", True):\n                    logger.warning(\n                        \"Postprocessing reported failure but pipeline will continue\"\n                    )\n\n                logger.info(\"Postprocessing completed\")\n\n            except Exception as e:\n                logger.exception(f\"Error during postprocessing: {e}\")\n                return {\n                    **pipeline_results,\n                    \"stage\": \"postprocess\",\n                    \"message\": f\"Postprocessing error: {str(e)}\",\n                    \"error\": str(e),\n                }\n\n            # Pipeline completed successfully\n            pipeline_results[\"success\"] = True\n            pipeline_results[\"message\"] = \"Pipeline completed successfully\"\n\n            logger.info(\n                f\"Pipeline execution completed successfully for run_id: {model_run.run_id}\"\n            )\n            return pipeline_results\n\n        except Exception as e:\n            logger.exception(f\"Unexpected error in pipeline execution: {e}\")\n            if cleanup_on_failure:\n                self._cleanup_outputs(model_run)\n            return {\n                **pipeline_results,\n                \"stage\": \"pipeline\",\n                \"message\": f\"Pipeline error: {str(e)}\",\n                \"error\": str(e),\n            }\n\n    def _cleanup_outputs(self, model_run) -&gt; None:\n        \"\"\"Clean up output files on pipeline failure.\n\n        Args:\n            model_run: The ModelRun instance\n        \"\"\"\n        try:\n            output_dir = Path(model_run.output_dir) / model_run.run_id\n            if output_dir.exists():\n                logger.info(f\"Cleaning up output directory: {output_dir}\")\n                import shutil\n\n                shutil.rmtree(output_dir)\n                logger.info(\"Cleanup completed\")\n        except Exception as e:\n            logger.warning(f\"Failed to cleanup output directory: {e}\")\n\n    def _create_backend_config(self, run_backend: str, run_kwargs: Dict[str, Any]):\n        \"\"\"Create appropriate backend configuration from string name and kwargs.\n\n        Args:\n            run_backend: Backend name (\"local\" or \"docker\")\n            run_kwargs: Additional configuration parameters\n\n        Returns:\n            Backend configuration object\n\n        Raises:\n            ValueError: If backend name is not supported\n        \"\"\"\n        if run_backend == \"local\":\n            # Filter kwargs to only include valid LocalConfig fields\n            valid_fields = set(LocalConfig.model_fields.keys())\n            filtered_kwargs = {k: v for k, v in run_kwargs.items() if k in valid_fields}\n            if filtered_kwargs != run_kwargs:\n                invalid_fields = set(run_kwargs.keys()) - valid_fields\n                logger.warning(f\"Ignoring invalid LocalConfig fields: {invalid_fields}\")\n            return LocalConfig(**filtered_kwargs)\n        elif run_backend == \"docker\":\n            # Filter kwargs to only include valid DockerConfig fields\n            valid_fields = set(DockerConfig.model_fields.keys())\n            filtered_kwargs = {k: v for k, v in run_kwargs.items() if k in valid_fields}\n            if filtered_kwargs != run_kwargs:\n                invalid_fields = set(run_kwargs.keys()) - valid_fields\n                logger.warning(\n                    f\"Ignoring invalid DockerConfig fields: {invalid_fields}\"\n                )\n            return DockerConfig(**filtered_kwargs)\n        else:\n            raise ValueError(\n                f\"Unsupported backend: {run_backend}. Supported: local, docker\"\n            )\n</code></pre>"},{"location":"plugin_architecture/execution/#rompy.pipeline.LocalPipelineBackend-functions","title":"Functions","text":""},{"location":"plugin_architecture/execution/#rompy.pipeline.LocalPipelineBackend.execute","title":"execute","text":"<pre><code>execute(model_run, run_backend: str = 'local', processor: str = 'noop', run_kwargs: Optional[Dict[str, Any]] = None, process_kwargs: Optional[Dict[str, Any]] = None, cleanup_on_failure: bool = False, validate_stages: bool = True, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Execute the model pipeline locally.</p> <p>Parameters:</p> Name Type Description Default <code>model_run</code> <p>The ModelRun instance to execute</p> required <code>run_backend</code> <code>str</code> <p>Backend to use for the run stage (\"local\" or \"docker\")</p> <code>'local'</code> <code>processor</code> <code>str</code> <p>Processor to use for the postprocess stage</p> <code>'noop'</code> <code>run_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters for the run stage</p> <code>None</code> <code>process_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters for the postprocess stage</p> <code>None</code> <code>cleanup_on_failure</code> <code>bool</code> <p>Whether to cleanup outputs on pipeline failure</p> <code>False</code> <code>validate_stages</code> <code>bool</code> <p>Whether to validate each stage before proceeding</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters (unused)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Combined results from the pipeline execution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_run is invalid or parameters are invalid</p> Source code in <code>rompy/pipeline/__init__.py</code> <pre><code>def execute(\n    self,\n    model_run,\n    run_backend: str = \"local\",\n    processor: str = \"noop\",\n    run_kwargs: Optional[Dict[str, Any]] = None,\n    process_kwargs: Optional[Dict[str, Any]] = None,\n    cleanup_on_failure: bool = False,\n    validate_stages: bool = True,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the model pipeline locally.\n\n    Args:\n        model_run: The ModelRun instance to execute\n        run_backend: Backend to use for the run stage (\"local\" or \"docker\")\n        processor: Processor to use for the postprocess stage\n        run_kwargs: Additional parameters for the run stage\n        process_kwargs: Additional parameters for the postprocess stage\n        cleanup_on_failure: Whether to cleanup outputs on pipeline failure\n        validate_stages: Whether to validate each stage before proceeding\n        **kwargs: Additional parameters (unused)\n\n    Returns:\n        Combined results from the pipeline execution\n\n    Raises:\n        ValueError: If model_run is invalid or parameters are invalid\n    \"\"\"\n    # Validate input parameters\n    if not model_run:\n        raise ValueError(\"model_run cannot be None\")\n\n    if not hasattr(model_run, \"run_id\"):\n        raise ValueError(\"model_run must have a run_id attribute\")\n\n    if not isinstance(run_backend, str) or not run_backend.strip():\n        raise ValueError(\"run_backend must be a non-empty string\")\n\n    if not isinstance(processor, str) or not processor.strip():\n        raise ValueError(\"processor must be a non-empty string\")\n\n    # Initialize parameters\n    run_kwargs = run_kwargs or {}\n    process_kwargs = process_kwargs or {}\n\n    logger.info(f\"Starting pipeline execution for run_id: {model_run.run_id}\")\n    logger.info(\n        f\"Pipeline configuration: run_backend='{run_backend}', processor='{processor}'\"\n    )\n\n    pipeline_results = {\n        \"success\": False,\n        \"run_id\": model_run.run_id,\n        \"stages_completed\": [],\n        \"run_backend\": run_backend,\n        \"processor\": processor,\n    }\n\n    try:\n        # Stage 1: Generate input files\n        logger.info(f\"Stage 1: Generating input files for {model_run.run_id}\")\n\n        try:\n            staging_dir = model_run.generate()\n            pipeline_results[\"staging_dir\"] = (\n                str(staging_dir) if staging_dir else None\n            )\n            pipeline_results[\"stages_completed\"].append(\"generate\")\n            logger.info(f\"Input files generated successfully in: {staging_dir}\")\n        except Exception as e:\n            logger.exception(f\"Failed to generate input files: {e}\")\n            return {\n                **pipeline_results,\n                \"stage\": \"generate\",\n                \"message\": f\"Input file generation failed: {str(e)}\",\n                \"error\": str(e),\n            }\n\n        # Validate generation stage\n        if validate_stages:\n            output_dir = Path(model_run.output_dir) / model_run.run_id\n            if not output_dir.exists():\n                logger.error(f\"Output directory was not created: {output_dir}\")\n                return {\n                    **pipeline_results,\n                    \"stage\": \"generate\",\n                    \"message\": f\"Output directory not found after generation: {output_dir}\",\n                }\n\n        # Stage 2: Run the model\n        logger.info(f\"Stage 2: Running model using {run_backend} backend\")\n\n        try:\n            # Create appropriate backend configuration\n            backend_config = self._create_backend_config(run_backend, run_kwargs)\n\n            # Pass the generated workspace directory to avoid duplicate generation\n            run_success = model_run.run(\n                backend=backend_config, workspace_dir=staging_dir\n            )\n            pipeline_results[\"run_success\"] = run_success\n\n            if not run_success:\n                logger.error(\"Model run failed\")\n                if cleanup_on_failure:\n                    self._cleanup_outputs(model_run)\n                return {\n                    **pipeline_results,\n                    \"stage\": \"run\",\n                    \"message\": \"Model run failed\",\n                }\n\n            pipeline_results[\"stages_completed\"].append(\"run\")\n            logger.info(\"Model run completed successfully\")\n\n        except Exception as e:\n            logger.exception(f\"Error during model run: {e}\")\n            if cleanup_on_failure:\n                self._cleanup_outputs(model_run)\n            return {\n                **pipeline_results,\n                \"stage\": \"run\",\n                \"message\": f\"Model run error: {str(e)}\",\n                \"error\": str(e),\n            }\n\n        # Stage 3: Postprocess outputs\n        logger.info(f\"Stage 3: Postprocessing with {processor}\")\n\n        try:\n            postprocess_results = model_run.postprocess(\n                processor=processor, **process_kwargs\n            )\n            pipeline_results[\"postprocess_results\"] = postprocess_results\n            pipeline_results[\"stages_completed\"].append(\"postprocess\")\n\n            # Check if postprocessing was successful\n            if isinstance(\n                postprocess_results, dict\n            ) and not postprocess_results.get(\"success\", True):\n                logger.warning(\n                    \"Postprocessing reported failure but pipeline will continue\"\n                )\n\n            logger.info(\"Postprocessing completed\")\n\n        except Exception as e:\n            logger.exception(f\"Error during postprocessing: {e}\")\n            return {\n                **pipeline_results,\n                \"stage\": \"postprocess\",\n                \"message\": f\"Postprocessing error: {str(e)}\",\n                \"error\": str(e),\n            }\n\n        # Pipeline completed successfully\n        pipeline_results[\"success\"] = True\n        pipeline_results[\"message\"] = \"Pipeline completed successfully\"\n\n        logger.info(\n            f\"Pipeline execution completed successfully for run_id: {model_run.run_id}\"\n        )\n        return pipeline_results\n\n    except Exception as e:\n        logger.exception(f\"Unexpected error in pipeline execution: {e}\")\n        if cleanup_on_failure:\n            self._cleanup_outputs(model_run)\n        return {\n            **pipeline_results,\n            \"stage\": \"pipeline\",\n            \"message\": f\"Pipeline error: {str(e)}\",\n            \"error\": str(e),\n        }\n</code></pre>"},{"location":"reference/cli/","title":"CLI","text":""},{"location":"reference/cli/#rompy.cli","title":"cli","text":"<p>ROMPY Command Line Interface</p> <p>This module provides the command-line interface for ROMPY.</p>"},{"location":"reference/cli/#rompy.cli-attributes","title":"Attributes","text":""},{"location":"reference/cli/#rompy.cli.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = get_logger(__name__)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.installed","title":"installed  <code>module-attribute</code>","text":"<pre><code>installed = names\n</code></pre>"},{"location":"reference/cli/#rompy.cli.common_options","title":"common_options  <code>module-attribute</code>","text":"<pre><code>common_options = [option('-v', '--verbose', count=True, help='Increase verbosity (can be used multiple times)'), option('--log-dir', envvar='ROMPY_LOG_DIR', help='Directory to save log files'), option('--show-warnings/--hide-warnings', default=False, help='Show Python warnings'), option('--ascii-only/--unicode', default=False, help='Use ASCII-only characters in output', envvar='ROMPY_ASCII_ONLY'), option('--simple-logs/--detailed-logs', default=False, help='Use simple log format without timestamps and module names', envvar='ROMPY_SIMPLE_LOGS'), option('--config-from-env', is_flag=True, help='Load configuration from ROMPY_CONFIG environment variable instead of file')]\n</code></pre>"},{"location":"reference/cli/#rompy.cli-classes","title":"Classes","text":""},{"location":"reference/cli/#rompy.cli-functions","title":"Functions","text":""},{"location":"reference/cli/#rompy.cli.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(verbosity: int = 0, log_dir: Optional[str] = None, simple_logs: bool = False, ascii_only: bool = False, show_warnings: bool = False) -&gt; None\n</code></pre> <p>Configure logging based on verbosity level and other options.</p> <p>This function configures the logging system using the LoggingConfig class. It sets up the log level, format, and output destinations based on the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>verbosity</code> <code>int</code> <p>Level of verbosity (0 = warning, 1 = info, 2+ = debug)</p> <code>0</code> <code>log_dir</code> <code>Optional[str]</code> <p>Directory to save log files (optional)</p> <code>None</code> <code>simple_logs</code> <code>bool</code> <p>Use simple log format without timestamps and module names</p> <code>False</code> <code>ascii_only</code> <code>bool</code> <p>Use ASCII-only characters in output</p> <code>False</code> <code>show_warnings</code> <code>bool</code> <p>Show Python warnings</p> <code>False</code> Source code in <code>rompy/cli.py</code> <pre><code>def configure_logging(\n    verbosity: int = 0,\n    log_dir: Optional[str] = None,\n    simple_logs: bool = False,\n    ascii_only: bool = False,\n    show_warnings: bool = False,\n) -&gt; None:\n    \"\"\"Configure logging based on verbosity level and other options.\n\n    This function configures the logging system using the LoggingConfig class.\n    It sets up the log level, format, and output destinations based on the\n    provided parameters.\n\n    Args:\n        verbosity: Level of verbosity (0 = warning, 1 = info, 2+ = debug)\n        log_dir: Directory to save log files (optional)\n        simple_logs: Use simple log format without timestamps and module names\n        ascii_only: Use ASCII-only characters in output\n        show_warnings: Show Python warnings\n    \"\"\"\n    # Determine log level based on verbosity\n    if verbosity == 0:\n        log_level = LogLevel.WARNING\n    elif verbosity == 1:\n        log_level = LogLevel.INFO\n    else:\n        log_level = LogLevel.DEBUG\n\n    # Determine log format\n    log_format = LogFormat.SIMPLE if simple_logs else LogFormat.VERBOSE\n\n    # Configure logging\n    logging_config = LoggingConfig(\n        level=log_level,\n        format=log_format,\n        log_dir=Path(log_dir) if log_dir else None,\n        use_ascii=ascii_only,\n    )\n\n    # Apply configuration\n    logging_config.configure_logging()\n\n    # Handle warnings\n    if show_warnings:\n        # Show deprecation warnings\n        warnings.filterwarnings(\"default\", category=DeprecationWarning)\n\n    # Log configuration\n    logger.debug(\"Logging configured with level: %s\", log_level.value)\n    if log_dir:\n        logger.info(\"Log directory: %s\", log_dir)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.add_common_options","title":"add_common_options","text":"<pre><code>add_common_options(f)\n</code></pre> <p>Decorator to add common CLI options to commands.</p> Source code in <code>rompy/cli.py</code> <pre><code>def add_common_options(f):\n    \"\"\"Decorator to add common CLI options to commands.\"\"\"\n    for option in reversed(common_options):\n        f = option(f)\n    return f\n</code></pre>"},{"location":"reference/cli/#rompy.cli.load_config","title":"load_config","text":"<pre><code>load_config(config_path: str, from_env: bool = False, env_var: str = 'ROMPY_CONFIG') -&gt; Dict[str, Any]\n</code></pre> <p>Load configuration from file, string, or environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to config file or raw config string</p> required <code>from_env</code> <code>bool</code> <p>If True, load from environment variable instead of config_path</p> <code>False</code> <code>env_var</code> <code>str</code> <p>Environment variable name to load from when from_env=True</p> <code>'ROMPY_CONFIG'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing parsed configuration</p> <p>Raises:</p> Type Description <code>UsageError</code> <p>If config cannot be loaded or parsed</p> Source code in <code>rompy/cli.py</code> <pre><code>def load_config(\n    config_path: str, from_env: bool = False, env_var: str = \"ROMPY_CONFIG\"\n) -&gt; Dict[str, Any]:\n    \"\"\"Load configuration from file, string, or environment variable.\n\n    Args:\n        config_path: Path to config file or raw config string\n        from_env: If True, load from environment variable instead of config_path\n        env_var: Environment variable name to load from when from_env=True\n\n    Returns:\n        Dict containing parsed configuration\n\n    Raises:\n        click.UsageError: If config cannot be loaded or parsed\n    \"\"\"\n    if from_env:\n        content = os.environ.get(env_var)\n        if content is None:\n            raise click.UsageError(f\"Environment variable {env_var} is not set\")\n        logger.info(f\"Loading config from environment variable: {env_var}\")\n    else:\n        try:\n            with open(config_path, \"r\") as f:\n                content = f.read()\n        except (FileNotFoundError, IsADirectoryError, OSError):\n            # Not a file, treat as raw string\n            content = config_path\n        logger.info(f\"Loading config from: {config_path}\")\n\n    # Try JSON first\n    try:\n        config = json.loads(content)\n        logger.info(\"Parsed config as JSON\")\n        return config\n    except json.JSONDecodeError:\n        pass\n\n    # If JSON failed, try YAML\n    try:\n        config = yaml.safe_load(content)\n        logger.info(\"Parsed config as YAML\")\n    except yaml.YAMLError as e:\n        logger.error(f\"Failed to parse config as JSON or YAML: {e}\")\n        raise click.UsageError(\"Config file is not valid JSON or YAML\")\n\n    # Render template variables in config\n    try:\n        config = render_templates(config, context=dict(os.environ), strict=True)\n        logger.debug(\"Template variables rendered successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to render template variables: {e}\")\n        raise click.UsageError(f\"Template rendering error: {e}\")\n\n    return config\n</code></pre>"},{"location":"reference/cli/#rompy.cli.print_version","title":"print_version","text":"<pre><code>print_version(ctx, param, value)\n</code></pre> <p>Callback to print version and exit.</p> Source code in <code>rompy/cli.py</code> <pre><code>def print_version(ctx, param, value):\n    \"\"\"Callback to print version and exit.\"\"\"\n    if not value or ctx.resilient_parsing:\n        return\n\n    # Import here to avoid circular imports\n    import rompy\n\n    click.echo(f\"ROMPY Version: {rompy.__version__}\")\n    click.echo(f\"Available models: {', '.join(installed)}\")\n    ctx.exit(0)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.cli","title":"cli","text":"<pre><code>cli(ctx)\n</code></pre> <p>ROMPY (Regional Ocean Modeling PYthon) - Ocean Model Configuration and Execution Tool.</p> <p>ROMPY provides tools for generating, running, and processing ocean, wave, and hydrodynamic model configurations with support for multiple execution backends.</p> Source code in <code>rompy/cli.py</code> <pre><code>@click.group(context_settings=dict(help_option_names=[\"-h\", \"--help\"]))\n@click.option(\n    \"--version\",\n    is_flag=True,\n    expose_value=False,\n    is_eager=True,\n    callback=print_version,\n    help=\"Show version information and exit\",\n)\n@click.pass_context\ndef cli(ctx):\n    \"\"\"ROMPY (Regional Ocean Modeling PYthon) - Ocean Model Configuration and Execution Tool.\n\n    ROMPY provides tools for generating, running, and processing ocean, wave, and\n    hydrodynamic model configurations with support for multiple execution backends.\n    \"\"\"\n    # Ensure that ctx.obj exists and is a dict\n    ctx.ensure_object(dict)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.run","title":"run","text":"<pre><code>run(config, backend_config, dry_run, skip_generate, verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env)\n</code></pre> <p>Run a model configuration using Pydantic backend configuration.</p> <p>Examples:</p>"},{"location":"reference/cli/#rompy.cli.run--run-with-local-backend-configuration","title":"Run with local backend configuration","text":"<p>rompy run config.yml --backend-config unified_local_single.yml</p>"},{"location":"reference/cli/#rompy.cli.run--run-with-docker-backend-configuration","title":"Run with Docker backend configuration","text":"<p>rompy run config.yml --backend-config unified_docker_single.yml</p>"},{"location":"reference/cli/#rompy.cli.run--run-with-config-from-environment-variable","title":"Run with config from environment variable","text":"<p>rompy run --config-from-env --backend-config unified_local_single.yml</p>"},{"location":"reference/cli/#rompy.cli.run--use-pre-generated-workspace-two-step-workflow","title":"Use pre-generated workspace (two-step workflow)","text":"<p>rompy generate config.yml rompy run config.yml --backend-config backend.yml --skip-generate</p> Source code in <code>rompy/cli.py</code> <pre><code>@cli.command()\n@click.argument(\"config\", type=click.Path(exists=True), required=False)\n@click.option(\n    \"--backend-config\",\n    type=click.Path(exists=True),\n    required=True,\n    help=\"YAML/JSON file with backend configuration\",\n)\n@click.option(\"--dry-run\", is_flag=True, help=\"Generate inputs only, don't run\")\n@click.option(\n    \"--skip-generate\",\n    is_flag=True,\n    help=\"Skip generation step, use existing workspace (must already exist)\",\n)\n@add_common_options\ndef run(\n    config,\n    backend_config,\n    dry_run,\n    skip_generate,\n    verbose,\n    log_dir,\n    show_warnings,\n    ascii_only,\n    simple_logs,\n    config_from_env,\n):\n    \"\"\"Run a model configuration using Pydantic backend configuration.\n\n    Examples:\n        # Run with local backend configuration\n        rompy run config.yml --backend-config unified_local_single.yml\n\n        # Run with Docker backend configuration\n        rompy run config.yml --backend-config unified_docker_single.yml\n\n        # Run with config from environment variable\n        rompy run --config-from-env --backend-config unified_local_single.yml\n\n        # Use pre-generated workspace (two-step workflow)\n        rompy generate config.yml\n        rompy run config.yml --backend-config backend.yml --skip-generate\n    \"\"\"\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    if skip_generate and dry_run:\n        raise click.UsageError(\"Cannot use --skip-generate with --dry-run\")\n\n    if config_from_env and config:\n        raise click.UsageError(\"Cannot specify both config file and --config-from-env\")\n    if not config_from_env and not config:\n        raise click.UsageError(\"Must specify either config file or --config-from-env\")\n\n    try:\n        config_data = load_config(config, from_env=config_from_env)\n        model_run = ModelRun(**config_data)\n\n        logger.info(f\"Running model: {model_run.config.model_type}\")\n        logger.info(f\"Run ID: {model_run.run_id}\")\n\n        backend_cfg = _load_backend_config(backend_config)\n\n        start_time = datetime.now()\n\n        if skip_generate:\n            staging_dir = str(model_run.staging_dir)\n            staging_path = Path(staging_dir)\n            if not staging_path.exists():\n                raise click.UsageError(\n                    f\"Workspace does not exist: {staging_dir}\\n\"\n                    f\"Run 'rompy generate {config or '&lt;config&gt;'}' first or remove --skip-generate\"\n                )\n            if not list(staging_path.glob(\"*\")):\n                raise click.UsageError(\n                    f\"Workspace exists but is empty: {staging_dir}\\n\"\n                    f\"Run 'rompy generate {config or '&lt;config&gt;'}' first or remove --skip-generate\"\n                )\n            logger.info(f\"Using existing workspace: {staging_dir}\")\n        else:\n            staging_dir = model_run.generate()\n            logger.info(f\"Inputs generated in: {staging_dir}\")\n\n        if dry_run:\n            logger.info(\"Dry run mode - skipping model execution\")\n            return\n\n        success = model_run.run(backend=backend_cfg, workspace_dir=staging_dir)\n\n        elapsed = datetime.now() - start_time\n        if success:\n            logger.info(\n                f\"\u2705 Model completed successfully in {elapsed.total_seconds():.2f}s\"\n            )\n        else:\n            logger.error(\n                f\"\u274c Model execution failed after {elapsed.total_seconds():.2f}s\"\n            )\n            sys.exit(1)\n\n    except Exception as e:\n        logger.error(f\"Error running model: {e}\")\n        if verbose &gt; 0:\n            logger.exception(\"Full traceback:\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.pipeline","title":"pipeline","text":"<pre><code>pipeline(config, run_backend, processor, cleanup_on_failure, validate_stages, verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env)\n</code></pre> <p>Run full model pipeline: generate \u2192 run \u2192 postprocess.</p> Source code in <code>rompy/cli.py</code> <pre><code>@cli.command()\n@click.argument(\"config\", type=click.Path(exists=True), required=False)\n@click.option(\"--run-backend\", default=\"local\", help=\"Execution backend for run stage\")\n@click.option(\"--processor\", default=\"noop\", help=\"Postprocessor to use\")\n@click.option(\n    \"--cleanup-on-failure/--no-cleanup\", default=False, help=\"Clean up on failure\"\n)\n@click.option(\n    \"--validate-stages/--no-validate\", default=True, help=\"Validate each stage\"\n)\n@add_common_options\ndef pipeline(\n    config,\n    run_backend,\n    processor,\n    cleanup_on_failure,\n    validate_stages,\n    verbose,\n    log_dir,\n    show_warnings,\n    ascii_only,\n    simple_logs,\n    config_from_env,\n):\n    \"\"\"Run full model pipeline: generate \u2192 run \u2192 postprocess.\"\"\"\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    # Validate config source\n    if config_from_env and config:\n        raise click.UsageError(\"Cannot specify both config file and --config-from-env\")\n    if not config_from_env and not config:\n        raise click.UsageError(\"Must specify either config file or --config-from-env\")\n\n    try:\n        # Load configuration\n        config_data = load_config(config, from_env=config_from_env)\n        model_run = ModelRun(**config_data)\n\n        logger.info(f\"Running pipeline for: {model_run.config.model_type}\")\n        logger.info(f\"Run ID: {model_run.run_id}\")\n        logger.info(\n            f\"Pipeline: generate \u2192 run({run_backend}) \u2192 postprocess({processor})\"\n        )\n\n        start_time = datetime.now()\n\n        # Execute pipeline\n        results = model_run.pipeline(\n            pipeline_backend=\"local\",\n            run_backend=run_backend,\n            processor=processor,\n            cleanup_on_failure=cleanup_on_failure,\n            validate_stages=validate_stages,\n        )\n\n        elapsed = datetime.now() - start_time\n\n        # Report results\n        success = results.get(\"success\", False)\n        stages = results.get(\"stages_completed\", [])\n\n        logger.info(f\"Pipeline completed in {elapsed.total_seconds():.2f}s\")\n        logger.info(f\"Stages completed: {', '.join(stages)}\")\n\n        if success:\n            logger.info(\"\u2705 Pipeline completed successfully\")\n        else:\n            logger.error(\n                f\"\u274c Pipeline failed: {results.get('message', 'Unknown error')}\"\n            )\n            sys.exit(1)\n\n    except Exception as e:\n        logger.error(f\"Error running pipeline: {e}\")\n        if verbose &gt; 0:\n            logger.exception(\"Full traceback:\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.generate","title":"generate","text":"<pre><code>generate(config, output_dir, verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env)\n</code></pre> <p>Generate model input files only.</p> Source code in <code>rompy/cli.py</code> <pre><code>@cli.command()\n@click.argument(\"config\", type=click.Path(exists=True), required=False)\n@click.option(\"--output-dir\", help=\"Override output directory\")\n@add_common_options\ndef generate(\n    config,\n    output_dir,\n    verbose,\n    log_dir,\n    show_warnings,\n    ascii_only,\n    simple_logs,\n    config_from_env,\n):\n    \"\"\"Generate model input files only.\"\"\"\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    # Validate config source\n    if config_from_env and config:\n        raise click.UsageError(\"Cannot specify both config file and --config-from-env\")\n    if not config_from_env and not config:\n        raise click.UsageError(\"Must specify either config file or --config-from-env\")\n\n    try:\n        # Load configuration\n        config_data = load_config(config, from_env=config_from_env)\n        if output_dir:\n            config_data[\"output_dir\"] = output_dir\n\n        model_run = ModelRun(**config_data)\n\n        logger.info(f\"Generating inputs for: {model_run.config.model_type}\")\n        logger.info(f\"Run ID: {model_run.run_id}\")\n\n        start_time = datetime.now()\n        staging_dir = model_run.generate()\n        elapsed = datetime.now() - start_time\n\n        logger.info(f\"\u2705 Inputs generated in {elapsed.total_seconds():.2f}s\")\n        logger.info(f\"\ud83d\udcc1 Staging directory: {staging_dir}\")\n\n        # List generated files\n        if Path(staging_dir).exists():\n            files = list(Path(staging_dir).glob(\"*\"))\n            logger.info(f\"Generated {len(files)} files\")\n\n    except Exception as e:\n        logger.error(f\"Error generating inputs: {e}\")\n        if verbose &gt; 0:\n            logger.exception(\"Full traceback:\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.postprocess","title":"postprocess","text":"<pre><code>postprocess(config, processor, output_dir, validate_outputs, verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env)\n</code></pre> <p>Run postprocessing on model outputs using the specified postprocessor.</p> Source code in <code>rompy/cli.py</code> <pre><code>@cli.command()\n@click.argument(\"config\", type=click.Path(exists=True), required=False)\n@click.option(\n    \"--processor\", default=\"noop\", help=\"Postprocessor to use (default: noop)\"\n)\n@click.option(\"--output-dir\", help=\"Override output directory for postprocessing\")\n@click.option(\n    \"--validate-outputs/--no-validate\",\n    default=True,\n    help=\"Validate outputs exist (default: True)\",\n)\n@add_common_options\ndef postprocess(\n    config,\n    processor,\n    output_dir,\n    validate_outputs,\n    verbose,\n    log_dir,\n    show_warnings,\n    ascii_only,\n    simple_logs,\n    config_from_env,\n):\n    \"\"\"Run postprocessing on model outputs using the specified postprocessor.\"\"\"\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    # Validate config source\n    if config_from_env and config:\n        raise click.UsageError(\"Cannot specify both config file and --config-from-env\")\n    if not config_from_env and not config:\n        raise click.UsageError(\"Must specify either config file or --config-from-env\")\n\n    try:\n        # Load configuration\n        config_data = load_config(config, from_env=config_from_env)\n        model_run = ModelRun(**config_data)\n\n        logger.info(f\"Running postprocessing for: {model_run.config.model_type}\")\n        logger.info(f\"Run ID: {model_run.run_id}\")\n        logger.info(f\"Postprocessor: {processor}\")\n\n        # Run postprocessing\n        start_time = datetime.now()\n        results = model_run.postprocess(\n            processor=processor,\n            output_dir=output_dir,\n            validate_outputs=validate_outputs,\n        )\n        elapsed = datetime.now() - start_time\n\n        logger.info(f\"\u2705 Postprocessing completed in {elapsed.total_seconds():.2f}s\")\n        logger.info(f\"Results: {results}\")\n\n    except Exception as e:\n        logger.error(f\"\u274c Postprocessing failed: {e}\")\n        if verbose &gt; 0:\n            logger.exception(\"Full traceback:\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.validate","title":"validate","text":"<pre><code>validate(config, verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env)\n</code></pre> <p>Validate model configuration.</p> Source code in <code>rompy/cli.py</code> <pre><code>@cli.command()\n@click.argument(\"config\", type=click.Path(exists=True), required=False)\n@add_common_options\ndef validate(\n    config, verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env\n):\n    \"\"\"Validate model configuration.\"\"\"\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    # Validate config source\n    if config_from_env and config:\n        raise click.UsageError(\"Cannot specify both config file and --config-from-env\")\n    if not config_from_env and not config:\n        raise click.UsageError(\"Must specify either config file or --config-from-env\")\n\n    try:\n        # Load and validate configuration\n        config_data = load_config(config, from_env=config_from_env)\n        model_run = ModelRun(**config_data)\n\n        logger.info(\"\u2705 Configuration is valid\")\n        logger.info(f\"Model type: {model_run.config.model_type}\")\n        logger.info(f\"Run ID: {model_run.run_id}\")\n        logger.info(f\"Period: {model_run.period}\")\n        logger.info(f\"Output directory: {model_run.output_dir}\")\n\n    except Exception as e:\n        logger.error(f\"\u274c Configuration validation failed: {e}\")\n        if verbose &gt; 0:\n            logger.exception(\"Full traceback:\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.backends","title":"backends","text":"<pre><code>backends()\n</code></pre> <p>Manage execution backends.</p> Source code in <code>rompy/cli.py</code> <pre><code>@cli.group()\ndef backends():\n    \"\"\"Manage execution backends.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cli/#rompy.cli.list_backends","title":"list_backends","text":"<pre><code>list_backends(verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env)\n</code></pre> <p>List available backends.</p> Source code in <code>rompy/cli.py</code> <pre><code>@backends.command(\"list\")\n@add_common_options\ndef list_backends(\n    verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env\n):\n    \"\"\"List available backends.\"\"\"\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    logger.info(\"Available Backends:\")\n\n    logger.info(\"\\n\ud83c\udfc3 Run Backends:\")\n    for name, backend_class in RUN_BACKENDS.items():\n        logger.info(f\"  - {name}: {backend_class.__name__}\")\n        if hasattr(backend_class, \"__doc__\") and backend_class.__doc__:\n            doc = backend_class.__doc__.strip().split(\"\\n\")[0]\n            logger.info(f\"    {doc}\")\n\n    logger.info(\"\\n\ud83d\udd04 Postprocessors:\")\n    for name, proc_class in POSTPROCESSORS.items():\n        logger.info(f\"  - {name}: {proc_class.__name__}\")\n        if hasattr(proc_class, \"__doc__\") and proc_class.__doc__:\n            doc = proc_class.__doc__.strip().split(\"\\n\")[0]\n            logger.info(f\"    {doc}\")\n\n    logger.info(\"\\n\ud83d\udd17 Pipeline Backends:\")\n    for name, pipeline_class in PIPELINE_BACKENDS.items():\n        logger.info(f\"  - {name}: {pipeline_class.__name__}\")\n        if hasattr(pipeline_class, \"__doc__\") and pipeline_class.__doc__:\n            doc = pipeline_class.__doc__.strip().split(\"\\n\")[0]\n            logger.info(f\"    {doc}\")\n\n    # Show Pydantic backend configurations\n    logger.info(\"\\n\u2699\ufe0f  Backend Configurations:\")\n    logger.info(\"  - LocalConfig \u2192 LocalRunBackend\")\n    logger.info(\"  - DockerConfig \u2192 DockerRunBackend\")\n</code></pre>"},{"location":"reference/cli/#rompy.cli.validate_backend_config","title":"validate_backend_config","text":"<pre><code>validate_backend_config(config_file, backend_type, verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env)\n</code></pre> <p>Validate a backend configuration file.</p> Source code in <code>rompy/cli.py</code> <pre><code>@backends.command(\"validate\")\n@click.argument(\"config_file\", type=click.Path(exists=True))\n@click.option(\n    \"--backend-type\",\n    type=click.Choice([\"local\", \"docker\"]),\n    help=\"Backend type to validate as\",\n)\n@add_common_options\ndef validate_backend_config(\n    config_file,\n    backend_type,\n    verbose,\n    log_dir,\n    show_warnings,\n    ascii_only,\n    simple_logs,\n    config_from_env,\n):\n    \"\"\"Validate a backend configuration file.\"\"\"\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    try:\n        # Load configuration\n        config_data = load_config(config_file)\n\n        # Determine backend type and extract config parameters\n        if backend_type:\n            config_type = backend_type\n        elif \"backend_type\" in config_data:\n            config_type = config_data.pop(\"backend_type\")\n        elif \"type\" in config_data:\n            config_type = config_data.pop(\"type\")\n        else:\n            raise click.UsageError(\n                \"Backend type must be specified via --backend-type or 'type' field in config\"\n            )\n\n        # Validate configuration\n        if config_type == \"local\":\n            config = LocalConfig(**config_data)\n            logger.info(\"\u2705 Local backend configuration is valid\")\n        elif config_type == \"docker\":\n            config = DockerConfig(**config_data)\n            logger.info(\"\u2705 Docker backend configuration is valid\")\n        else:\n            raise click.UsageError(f\"Unknown backend type: {config_type}\")\n\n        # Show configuration details\n        logger.info(f\"Backend type: {config_type}\")\n        logger.info(f\"Timeout: {config.timeout}s\")\n        if config.env_vars:\n            logger.info(f\"Environment variables: {list(config.env_vars.keys())}\")\n        if config.working_dir:\n            logger.info(f\"Working directory: {config.working_dir}\")\n\n        # Type-specific details\n        if isinstance(config, LocalConfig):\n            if config.command:\n                logger.info(f\"Command: {config.command}\")\n        elif isinstance(config, DockerConfig):\n            if config.image:\n                logger.info(f\"Image: {config.image}\")\n            if config.dockerfile:\n                logger.info(f\"Dockerfile: {config.dockerfile}\")\n            logger.info(f\"CPU: {config.cpu}\")\n            if config.memory:\n                logger.info(f\"Memory: {config.memory}\")\n            if config.volumes:\n                logger.info(f\"Volumes: {len(config.volumes)} mounts\")\n\n    except Exception as e:\n        logger.error(f\"\u274c Backend configuration validation failed: {e}\")\n        if verbose &gt; 0:\n            logger.exception(\"Full traceback:\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.show_backend_schema","title":"show_backend_schema","text":"<pre><code>show_backend_schema(backend_type, output_format, examples, verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env)\n</code></pre> <p>Show JSON schema for backend configurations.</p> Source code in <code>rompy/cli.py</code> <pre><code>@backends.command(\"schema\")\n@click.option(\n    \"--backend-type\",\n    type=click.Choice([\"local\", \"docker\"]),\n    required=True,\n    help=\"Backend type to show schema for\",\n)\n@click.option(\n    \"--format\",\n    \"output_format\",\n    type=click.Choice([\"json\", \"yaml\"]),\n    default=\"json\",\n    help=\"Output format\",\n)\n@click.option(\"--examples\", is_flag=True, help=\"Include examples in output\")\n@add_common_options\ndef show_backend_schema(\n    backend_type,\n    output_format,\n    examples,\n    verbose,\n    log_dir,\n    show_warnings,\n    ascii_only,\n    simple_logs,\n    config_from_env,\n):\n    \"\"\"Show JSON schema for backend configurations.\"\"\"\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    try:\n        # Get the appropriate configuration class\n        if backend_type == \"local\":\n            config_class = LocalConfig\n        elif backend_type == \"docker\":\n            config_class = DockerConfig\n        else:\n            raise click.UsageError(f\"Unknown backend type: {backend_type}\")\n\n        # Generate schema\n        schema = config_class.model_json_schema()\n\n        if not examples:\n            # Remove examples from schema\n            schema.pop(\"examples\", None)\n            for prop in schema.get(\"properties\", {}).values():\n                prop.pop(\"examples\", None)\n\n        # Output schema\n        if output_format == \"json\":\n            import json\n\n            output = json.dumps(schema, indent=2)\n        else:  # yaml\n            import yaml\n\n            output = yaml.dump(schema, default_flow_style=False)\n\n        click.echo(output)\n\n    except Exception as e:\n        logger.error(f\"Error generating schema: {e}\")\n        if verbose &gt; 0:\n            logger.exception(\"Full traceback:\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.create_backend_config","title":"create_backend_config","text":"<pre><code>create_backend_config(backend_type, output, output_format, with_examples, verbose, log_dir, show_warnings, ascii_only, simple_logs, config_from_env)\n</code></pre> <p>Create a template backend configuration file.</p> Source code in <code>rompy/cli.py</code> <pre><code>@backends.command(\"create\")\n@click.option(\n    \"--backend-type\",\n    type=click.Choice([\"local\", \"docker\"]),\n    required=True,\n    help=\"Backend type to create\",\n)\n@click.option(\"--output\", type=click.Path(), help=\"Output file (default: stdout)\")\n@click.option(\n    \"--format\",\n    \"output_format\",\n    type=click.Choice([\"json\", \"yaml\"]),\n    default=\"yaml\",\n    help=\"Output format\",\n)\n@click.option(\"--with-examples\", is_flag=True, help=\"Include example values\")\n@add_common_options\ndef create_backend_config(\n    backend_type,\n    output,\n    output_format,\n    with_examples,\n    verbose,\n    log_dir,\n    show_warnings,\n    ascii_only,\n    simple_logs,\n    config_from_env,\n):\n    \"\"\"Create a template backend configuration file.\"\"\"\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    try:\n        # Create template configuration\n        config_data = {}\n        if backend_type == \"local\":\n            if with_examples:\n                config_data = {\n                    \"type\": \"local\",\n                    \"timeout\": 7200,\n                    \"env_vars\": {\"OMP_NUM_THREADS\": \"4\"},\n                    \"command\": \"python run_model.py\",\n                    \"shell\": True,\n                    \"capture_output\": True,\n                }\n            else:\n                config_data = {\n                    \"type\": \"local\",\n                    \"timeout\": 3600,\n                    \"env_vars\": {},\n                    \"command\": None,\n                    \"shell\": True,\n                    \"capture_output\": True,\n                }\n        elif backend_type == \"docker\":\n            if with_examples:\n                config_data = {\n                    \"type\": \"docker\",\n                    \"image\": \"swan:latest\",\n                    \"timeout\": 7200,\n                    \"cpu\": 4,\n                    \"memory\": \"2g\",\n                    \"env_vars\": {\"DOCKER_ENV\": \"value\"},\n                    \"volumes\": [\"/data:/app/data\"],\n                    \"executable\": \"/usr/local/bin/run.sh\",\n                }\n            else:\n                config_data = {\n                    \"type\": \"docker\",\n                    \"image\": \"your-image:latest\",\n                    \"timeout\": 3600,\n                    \"cpu\": 1,\n                    \"env_vars\": {},\n                    \"volumes\": [],\n                    \"executable\": \"/usr/local/bin/run.sh\",\n                }\n\n        # Format output\n        if output_format == \"json\":\n            import json\n\n            content = json.dumps(config_data, indent=2)\n        else:  # yaml\n            import yaml\n\n            content = yaml.dump(config_data, default_flow_style=False)\n\n        # Write to file or stdout\n        if output:\n            with open(output, \"w\") as f:\n                f.write(content)\n            logger.info(f\"\u2705 Backend configuration template created: {output}\")\n        else:\n            click.echo(content)\n\n    except Exception as e:\n        logger.error(f\"Error creating backend configuration: {e}\")\n        if verbose &gt; 0:\n            logger.exception(\"Full traceback:\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.schema","title":"schema","text":"<pre><code>schema(model_type: str, output: Optional[str] = None, output_format: str = 'json', verbose: int = 0, log_dir: Optional[str] = None, show_warnings: bool = False, ascii_only: bool = False, simple_logs: bool = False, config_from_env: bool = False) -&gt; None\n</code></pre> <p>Show JSON schema for a rompy model.</p> <p>Examples:</p>"},{"location":"reference/cli/#rompy.cli.schema--show-schema-for-modelrun-default","title":"Show schema for ModelRun (default)","text":"<p>rompy schema</p>"},{"location":"reference/cli/#rompy.cli.schema--show-schema-for-a-specific-model-type","title":"Show schema for a specific model type","text":"<p>rompy schema \"rompy.model.ModelRun\" rompy schema \"rompy.swan.SWAN\"</p>"},{"location":"reference/cli/#rompy.cli.schema--save-schema-to-a-file","title":"Save schema to a file","text":"<p>rompy schema --output=model_schema.json rompy schema --output=model_schema.yaml --format=yaml</p> Source code in <code>rompy/cli.py</code> <pre><code>@cli.command(name=\"schema\")\n@click.argument(\"model_type\", default=\"ModelRun\", type=str, required=False)\n@click.option(\n    \"--output\",\n    \"-o\",\n    type=click.Path(dir_okay=False, writable=True),\n    help=\"Output file to save the schema (default: print to stdout)\",\n)\n@click.option(\n    \"--format\",\n    \"output_format\",\n    type=click.Choice([\"json\", \"yaml\"], case_sensitive=False),\n    default=\"json\",\n    help=\"Output format (default: json)\",\n)\n@add_common_options\ndef schema(\n    model_type: str,\n    output: Optional[str] = None,\n    output_format: str = \"json\",\n    verbose: int = 0,\n    log_dir: Optional[str] = None,\n    show_warnings: bool = False,\n    ascii_only: bool = False,\n    simple_logs: bool = False,\n    config_from_env: bool = False,\n) -&gt; None:\n    \"\"\"Show JSON schema for a rompy model.\n\n    Examples:\n        # Show schema for ModelRun (default)\n        rompy schema\n\n        # Show schema for a specific model type\n        rompy schema \"rompy.model.ModelRun\"\n        rompy schema \"rompy.swan.SWAN\"\n\n        # Save schema to a file\n        rompy schema --output=model_schema.json\n        rompy schema --output=model_schema.yaml --format=yaml\n    \"\"\"\n    # Configure logging\n    configure_logging(\n        verbosity=verbose,\n        log_dir=log_dir,\n        simple_logs=simple_logs,\n        ascii_only=ascii_only,\n        show_warnings=show_warnings,\n    )\n\n    # Get logger for this module\n    logger = get_logger(__name__)\n\n    try:\n        logger.debug(f\"Showing schema for model: {model_type}\")\n\n        # Import the model class\n        try:\n            if \".\" in model_type:\n                # Full module path provided (e.g., \"rompy.model.ModelRun\")\n                module_path, class_name = model_type.rsplit(\".\", 1)\n                logger.debug(f\"Importing {class_name} from {module_path}\")\n                module = importlib.import_module(module_path)\n                model_class = getattr(module, class_name)\n            else:\n                # Try to import from rompy.model first\n                try:\n                    logger.debug(f\"Trying to import {model_type} from rompy.model\")\n                    model_class = getattr(rompy.model, model_type)\n                except AttributeError:\n                    logger.debug(\n                        f\"{model_type} not found in rompy.model, checking entry points\"\n                    )\n                    # Try to find the model in entry points with different approaches\n                    try:\n                        # Python 3.10+ style\n                        model_eps = importlib.metadata.entry_points()\n                        if hasattr(model_eps, \"select\"):\n                            # Python 3.10+\n                            model_entries = model_eps.select(group=\"rompy.model\")\n                        elif hasattr(model_eps, \"get\"):\n                            # Python 3.8-3.9\n                            model_entries = model_eps.get(\"rompy.model\", [])\n                        else:\n                            # Fallback for older Python versions\n                            model_entries = []\n                            if hasattr(model_eps, \"items\"):\n                                for group, entries in model_eps.items():\n                                    if group == \"rompy.model\":\n                                        model_entries = entries\n                                        break\n                    except Exception as e:\n                        logger.debug(f\"Error getting entry points: {e}\")\n                        model_entries = []\n\n                    # Try to find the model in entry points\n                    found = False\n                    for entry_point in model_entries:\n                        if entry_point.name.lower() == model_type.lower():\n                            logger.debug(\n                                f\"Found {model_type} in entry points, loading...\"\n                            )\n                            model_class = entry_point.load()\n                            found = True\n                            break\n\n                    if not found:\n                        # Try direct import as a last resort\n                        try:\n                            model_class = importlib.import_module(\n                                f\"rompy.{model_type.lower()}\"\n                            )\n                            found = True\n                        except ImportError:\n                            raise ImportError(\n                                f\"No model found with name '{model_type}'. \"\n                                \"Please provide the full module path (e.g., 'rompy.swan.SWAN')\"\n                            )\n\n            logger.debug(\n                f\"Successfully imported model class: {model_class.__module__}.{model_class.__name__}\"\n            )\n\n        except (ImportError, AttributeError) as e:\n            # Initialize error message\n            error_msg = [f\"Could not import model class '{model_type}'\"]\n\n            # Get available models from rompy.model\n            try:\n                from rompy.model import __all__ as model_classes\n\n                error_msg.append(\n                    f\"Available models in rompy.model: {', '.join(model_classes)}\"\n                )\n            except Exception as e:\n                logger.debug(f\"Could not get models from rompy.model: {e}\")\n\n            # Try to get available models from entry points\n            try:\n                model_eps = importlib.metadata.entry_points()\n                if hasattr(model_eps, \"select\"):\n                    # Python 3.10+\n                    model_entries = model_eps.select(group=\"rompy.model\")\n                elif hasattr(model_eps, \"get\"):\n                    # Python 3.8-3.9\n                    model_entries = model_eps.get(\"rompy.model\", [])\n                else:\n                    # Fallback for older Python versions\n                    model_entries = []\n                    if hasattr(model_eps, \"items\"):\n                        for group, entries in model_eps.items():\n                            if group == \"rompy.model\":\n                                model_entries = entries\n                                break\n\n                available_models = [ep.name for ep in model_entries]\n                if available_models:\n                    error_msg.append(\n                        f\"Available models from entry points: {', '.join(available_models)}\"\n                    )\n                else:\n                    error_msg.append(\"No models found in entry points\")\n\n            except Exception as ep_error:\n                error_msg.append(f\"Error checking entry points: {ep_error}\")\n\n            # Log all error messages\n            for msg in error_msg:\n                logger.error(msg)\n\n            # Add more detailed error info if verbose\n            if verbose &gt; 0:\n                logger.error(f\"Error details: {str(e)}\")\n                import traceback\n\n                logger.error(traceback.format_exc())\n\n            # Suggest using full module path\n            logger.error(\"\\nTry using the full module path, e.g., 'rompy.swan.SWAN'\")\n            logger.error(\n                \"For a list of available models, run: python -m rompy.cli list-models\"\n            )\n\n            sys.exit(1)\n\n        # Generate the schema\n        schema = model_class.model_json_schema()\n\n        # Output the schema\n        if output:\n            output_path = Path(output)\n            output_path.parent.mkdir(parents=True, exist_ok=True)\n            if output_format == \"json\":\n                with open(output_path, \"w\") as f:\n                    json.dump(schema, f, indent=2)\n            else:  # yaml\n                with open(output_path, \"w\") as f:\n                    yaml.dump(schema, f, default_flow_style=False)\n            logger.info(f\"Schema written to {output_path}\")\n        else:\n            if output_format == \"json\":\n                print(json.dumps(schema, indent=2))\n            else:  # yaml\n                print(yaml.dump(schema, default_flow_style=False))\n\n    except Exception as e:\n        logger.error(f\"Error generating schema: {e}\")\n        if verbose &gt; 0:\n            import traceback\n\n            logger.error(traceback.format_exc())\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.legacy_main","title":"legacy_main","text":"<pre><code>legacy_main(model, config, zip, verbose, log_dir, show_warnings, ascii_only, simple_logs)\n</code></pre> <p>Legacy command for backward compatibility (DEPRECATED).</p> <p>Use 'rompy run' instead for new functionality.</p> Source code in <code>rompy/cli.py</code> <pre><code>@cli.command(name=\"legacy\", hidden=True)\n@click.argument(\n    \"model\", type=click.Choice(installed), envvar=\"ROMPY_MODEL\", required=False\n)\n@click.argument(\"config\", envvar=\"ROMPY_CONFIG\", required=False)\n@click.option(\"zip\", \"--zip/--no-zip\", default=False, envvar=\"ROMPY_ZIP\")\n@click.option(\n    \"-v\",\n    \"--verbose\",\n    count=True,\n    help=\"Increase verbosity (can be used multiple times)\",\n)\n@click.option(\"--log-dir\", envvar=\"ROMPY_LOG_DIR\", help=\"Directory to save log files\")\n@click.option(\n    \"--show-warnings/--hide-warnings\", default=False, help=\"Show Python warnings\"\n)\n@click.option(\n    \"--ascii-only/--unicode\",\n    default=False,\n    help=\"Use ASCII-only characters in output\",\n    envvar=\"ROMPY_ASCII_ONLY\",\n)\n@click.option(\n    \"--simple-logs/--detailed-logs\",\n    default=False,\n    help=\"Use simple log format without timestamps and module names\",\n    envvar=\"ROMPY_SIMPLE_LOGS\",\n)\ndef legacy_main(\n    model,\n    config,\n    zip,\n    verbose,\n    log_dir,\n    show_warnings,\n    ascii_only,\n    simple_logs,\n):\n    \"\"\"Legacy command for backward compatibility (DEPRECATED).\n\n    Use 'rompy run' instead for new functionality.\n    \"\"\"\n    # Configure logging\n    configure_logging(verbose, log_dir, simple_logs, ascii_only, show_warnings)\n\n    # Import here to avoid circular imports\n    import rompy\n\n    # If no model or config is provided, show help and available models\n    if not model or not config:\n        logger.info(f\"ROMPY Version: {rompy.__version__}\")\n        logger.info(f\"Available models: {', '.join(installed)}\")\n        logger.info(\"Run 'rompy --help' for usage information\")\n        ctx = click.get_current_context()\n        click.echo(ctx.get_help())\n        ctx.exit()\n\n    try:\n        # Load configuration\n        config_data = load_config(config)\n\n        # Log version and execution information\n        logger.info(f\"ROMPY Version: {rompy.__version__}\")\n        logger.info(f\"Running model: {model}\")\n        logger.info(f\"Configuration: {config}\")\n\n        # Create and run the model\n        start_time = datetime.now()\n        logger.info(\"Running model...\")\n        model_run = ModelRun(**config_data)\n        model_run()\n\n        if zip:\n            logger.info(\"Zipping model outputs...\")\n            zip_file = model_run.zip()\n            logger.info(f\"Model archive created: {zip_file}\")\n\n        # Log completion time\n        elapsed = datetime.now() - start_time\n        logger.info(f\"Model run completed in {elapsed.total_seconds():.2f} seconds\")\n\n        if log_dir:\n            logger.info(f\"Log directory: {log_dir}\")\n    except TypeError as e:\n        if \"unsupported format string\" in str(e) and \"timedelta\" in str(e):\n            logger.error(f\"Error with time format: {str(e)}\")\n            logger.error(\n                \"This is likely due to formatting issues with time duration values\"\n            )\n            if verbose &gt; 0:\n                logger.error(\"\", exc_info=True)\n        else:\n            logger.error(f\"Type error in model: {str(e)}\", exc_info=verbose &gt; 0)\n        sys.exit(1)\n    except Exception as e:\n        logger.error(f\"Error running model: {str(e)}\", exc_info=verbose &gt; 0)\n        sys.exit(1)\n</code></pre>"},{"location":"reference/cli/#rompy.cli.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Entry point for the rompy CLI.</p> <p>This function is used by the console script entry point.</p> Source code in <code>rompy/cli.py</code> <pre><code>def main():\n    \"\"\"Entry point for the rompy CLI.\n\n    This function is used by the console script entry point.\n    \"\"\"\n    cli()\n</code></pre>"},{"location":"reference/config/","title":"Config","text":""},{"location":"reference/config/#rompy.core.config","title":"config","text":""},{"location":"reference/config/#rompy.core.config-attributes","title":"Attributes","text":""},{"location":"reference/config/#rompy.core.config.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/config/#rompy.core.config.DEFAULT_TEMPLATE","title":"DEFAULT_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TEMPLATE = str(parent / 'templates' / 'base')\n</code></pre>"},{"location":"reference/config/#rompy.core.config-classes","title":"Classes","text":""},{"location":"reference/config/#rompy.core.config.BaseConfig","title":"BaseConfig","text":"<p>               Bases: <code>RompyBaseModel</code></p> <p>Base class for model templates.</p> <p>The template class provides the object that is used to set up the model configuration. When implemented for a given model, can move along a scale of complexity to suit the application.</p> <p>In its most basic form, as implemented in this base object, it consists of path to a cookiecutter template with the class providing the context for the {{config}} values in that template. Note that any {{runtime}} values are filled from the ModelRun object.</p> <p>If the template is a git repo, the checkout parameter can be used to specify a branch or tag and it will be cloned and used.</p> <p>If the object is callable, it will be colled prior to rendering the template. This mechanism can be used to perform tasks such as fetching exteral data, or providing additional context to the template beyond the arguments provided by the user..</p> Source code in <code>rompy/core/config.py</code> <pre><code>class BaseConfig(RompyBaseModel):\n    \"\"\"Base class for model templates.\n\n    The template class provides the object that is used to set up the model configuration.\n    When implemented for a given model, can move along a scale of complexity\n    to suit the application.\n\n    In its most basic form, as implemented in this base object, it consists of path to a cookiecutter template\n    with the class providing the context for the {{config}} values in that template. Note that any\n    {{runtime}} values are filled from the ModelRun object.\n\n    If the template is a git repo, the checkout parameter can be used to specify a branch or tag and it\n    will be cloned and used.\n\n    If the object is callable, it will be colled prior to rendering the template. This mechanism can be\n    used to perform tasks such as fetching exteral data, or providing additional context to the template\n    beyond the arguments provided by the user..\n    \"\"\"\n\n    model_type: Literal[\"base\"] = \"base\"\n    template: Optional[str] = Field(\n        description=\"The path to the model template\",\n        default=DEFAULT_TEMPLATE,\n    )\n    checkout: Optional[str] = Field(\n        description=\"The git branch to use if the template is a git repo\",\n        default=\"main\",\n    )\n\n    # noop call for config objects\n    def __call__(self, *args, **kwargs):\n        return self\n</code></pre>"},{"location":"reference/config/#rompy.core.config.BaseConfig-attributes","title":"Attributes","text":""},{"location":"reference/config/#rompy.core.config.BaseConfig.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['base'] = 'base'\n</code></pre>"},{"location":"reference/config/#rompy.core.config.BaseConfig.template","title":"template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>template: Optional[str] = Field(description='The path to the model template', default=DEFAULT_TEMPLATE)\n</code></pre>"},{"location":"reference/config/#rompy.core.config.BaseConfig.checkout","title":"checkout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>checkout: Optional[str] = Field(description='The git branch to use if the template is a git repo', default='main')\n</code></pre>"},{"location":"reference/source/","title":"Source","text":""},{"location":"reference/source/#rompy.core.source","title":"source","text":"<p>Rompy source objects.</p>"},{"location":"reference/source/#rompy.core.source-attributes","title":"Attributes","text":""},{"location":"reference/source/#rompy.core.source.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = get_logger(__name__)\n</code></pre>"},{"location":"reference/source/#rompy.core.source-classes","title":"Classes","text":""},{"location":"reference/source/#rompy.core.source.SourceBase","title":"SourceBase","text":"<p>               Bases: <code>RompyBaseModel</code>, <code>ABC</code></p> <p>Abstract base class for a source dataset.</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceBase(RompyBaseModel, ABC):\n    \"\"\"Abstract base class for a source dataset.\"\"\"\n\n    model_type: Literal[\"base_source\"] = Field(\n        description=\"Model type discriminator, must be overriden by a subclass\",\n    )\n\n    @abstractmethod\n    def _open(self) -&gt; xr.Dataset:\n        \"\"\"This abstract private method should return a xarray dataset object.\"\"\"\n        pass\n\n    @cached_property\n    def coordinates(self) -&gt; xr.Dataset:\n        \"\"\"Return the coordinates of the datasource.\"\"\"\n        return self.open().coords\n\n    def open(self, variables: list = [], filters: Filter = {}, **kwargs) -&gt; xr.Dataset:\n        \"\"\"Return the filtered dataset object.\n\n        Parameters\n        ----------\n        variables : list, optional\n            List of variables to select from the dataset.\n        filters : Filter, optional\n            Filters to apply to the dataset.\n\n        Notes\n        -----\n        The kwargs are only a placeholder in case a subclass needs to pass additional\n        arguments to the open method.\n\n        \"\"\"\n        ds = self._open()\n        if variables:\n            try:\n                ds = ds[variables]\n            except KeyError as e:\n                dataset_variables = list(ds.data_vars.keys())\n                missing_variables = list(set(variables) - set(dataset_variables))\n                raise ValueError(\n                    f\"Cannot find requested variables in dataset.\\n\\n\"\n                    f\"Requested variables in the Data object: {variables}\\n\"\n                    f\"Available variables in source dataset: {dataset_variables}\\n\"\n                    f\"Missing variables: {missing_variables}\\n\\n\"\n                    f\"Please check:\\n\"\n                    f\"1. The variable names in your Data object, make sure you check for default values\\n\"\n                    f\"2. The data source contains the expected variables\\n\"\n                    f\"3. If using a custom data source, ensure it creates variables with the correct names\"\n                ) from e\n        if filters:\n            ds = filters(ds)\n        return ds\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceBase-attributes","title":"Attributes","text":""},{"location":"reference/source/#rompy.core.source.SourceBase.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['base_source'] = Field(description='Model type discriminator, must be overriden by a subclass')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceBase.coordinates","title":"coordinates  <code>cached</code> <code>property</code>","text":"<pre><code>coordinates: Dataset\n</code></pre> <p>Return the coordinates of the datasource.</p>"},{"location":"reference/source/#rompy.core.source.SourceBase-functions","title":"Functions","text":""},{"location":"reference/source/#rompy.core.source.SourceBase.open","title":"open","text":"<pre><code>open(variables: list = [], filters: Filter = {}, **kwargs) -&gt; Dataset\n</code></pre> <p>Return the filtered dataset object.</p>"},{"location":"reference/source/#rompy.core.source.SourceBase.open--parameters","title":"Parameters","text":"<p>variables : list, optional     List of variables to select from the dataset. filters : Filter, optional     Filters to apply to the dataset.</p>"},{"location":"reference/source/#rompy.core.source.SourceBase.open--notes","title":"Notes","text":"<p>The kwargs are only a placeholder in case a subclass needs to pass additional arguments to the open method.</p> Source code in <code>rompy/core/source.py</code> <pre><code>def open(self, variables: list = [], filters: Filter = {}, **kwargs) -&gt; xr.Dataset:\n    \"\"\"Return the filtered dataset object.\n\n    Parameters\n    ----------\n    variables : list, optional\n        List of variables to select from the dataset.\n    filters : Filter, optional\n        Filters to apply to the dataset.\n\n    Notes\n    -----\n    The kwargs are only a placeholder in case a subclass needs to pass additional\n    arguments to the open method.\n\n    \"\"\"\n    ds = self._open()\n    if variables:\n        try:\n            ds = ds[variables]\n        except KeyError as e:\n            dataset_variables = list(ds.data_vars.keys())\n            missing_variables = list(set(variables) - set(dataset_variables))\n            raise ValueError(\n                f\"Cannot find requested variables in dataset.\\n\\n\"\n                f\"Requested variables in the Data object: {variables}\\n\"\n                f\"Available variables in source dataset: {dataset_variables}\\n\"\n                f\"Missing variables: {missing_variables}\\n\\n\"\n                f\"Please check:\\n\"\n                f\"1. The variable names in your Data object, make sure you check for default values\\n\"\n                f\"2. The data source contains the expected variables\\n\"\n                f\"3. If using a custom data source, ensure it creates variables with the correct names\"\n            ) from e\n    if filters:\n        ds = filters(ds)\n    return ds\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceFile","title":"SourceFile","text":"<p>               Bases: <code>SourceBase</code></p> <p>Source dataset from file to open with xarray.open_dataset.</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceFile(SourceBase):\n    \"\"\"Source dataset from file to open with xarray.open_dataset.\"\"\"\n\n    model_type: Literal[\"file\"] = Field(\n        default=\"file\",\n        description=\"Model type discriminator\",\n    )\n    uri: Union[str, Path] = Field(description=\"Path to the dataset\")\n    kwargs: dict = Field(\n        default={},\n        description=\"Keyword arguments to pass to xarray.open_dataset\",\n    )\n\n    variable: Optional[str] = Field(\n        default=None,\n        description=\"Variable to select from the dataset\",\n    )\n\n    # Enable arbitrary types for Path objects\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __str__(self) -&gt; str:\n        return f\"SourceFile(uri={self.uri})\"\n\n    def _open(self) -&gt; Union[xr.Dataset, xr.DataArray]:\n        # Handle Path objects by using str() to ensure compatibility\n        uri_str = str(self.uri) if isinstance(self.uri, Path) else self.uri\n        if self.variable:\n            # If a variable is specified, open the dataset and select the variable\n            return xr.open_dataset(uri_str, **self.kwargs)[self.variable]\n        else:\n            return xr.open_dataset(uri_str, **self.kwargs)\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceFile-attributes","title":"Attributes","text":""},{"location":"reference/source/#rompy.core.source.SourceFile.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['file'] = Field(default='file', description='Model type discriminator')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceFile.uri","title":"uri  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uri: Union[str, Path] = Field(description='Path to the dataset')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceFile.kwargs","title":"kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kwargs: dict = Field(default={}, description='Keyword arguments to pass to xarray.open_dataset')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceFile.variable","title":"variable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>variable: Optional[str] = Field(default=None, description='Variable to select from the dataset')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceFile.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceIntake","title":"SourceIntake","text":"<p>               Bases: <code>SourceBase</code></p> <p>Source dataset from intake catalog.</p>"},{"location":"reference/source/#rompy.core.source.SourceIntake--note","title":"note","text":"<p>The intake catalog can be prescribed either by the URI of an existing catalog file or by a YAML string defining the catalog. The YAML string can be obtained from calling the <code>yaml()</code> method on an intake dataset instance.</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceIntake(SourceBase):\n    \"\"\"Source dataset from intake catalog.\n\n    note\n    ----\n    The intake catalog can be prescribed either by the URI of an existing catalog file\n    or by a YAML string defining the catalog. The YAML string can be obtained from\n    calling the `yaml()` method on an intake dataset instance.\n\n    \"\"\"\n\n    model_type: Literal[\"intake\"] = Field(\n        default=\"intake\",\n        description=\"Model type discriminator\",\n    )\n    dataset_id: str = Field(description=\"The id of the dataset to read in the catalog\")\n    catalog_uri: Optional[str | Path] = Field(\n        default=None,\n        description=\"The URI of the catalog to read from\",\n    )\n    catalog_yaml: Optional[str] = Field(\n        default=None,\n        description=\"The YAML string of the catalog to read from\",\n    )\n    kwargs: dict = Field(\n        default={},\n        description=\"Keyword arguments to define intake dataset parameters\",\n    )\n\n    @model_validator(mode=\"after\")\n    def check_catalog(self) -&gt; \"SourceIntake\":\n        if self.catalog_uri is None and self.catalog_yaml is None:\n            raise ValueError(\"Either catalog_uri or catalog_yaml must be provided\")\n        elif self.catalog_uri is not None and self.catalog_yaml is not None:\n            raise ValueError(\"Only one of catalog_uri or catalog_yaml can be provided\")\n        return self\n\n    def __str__(self) -&gt; str:\n        return f\"SourceIntake(catalog_uri={self.catalog_uri}, dataset_id={self.dataset_id})\"\n\n    @property\n    def catalog(self) -&gt; Catalog:\n        \"\"\"The intake catalog instance.\"\"\"\n        if self.catalog_uri:\n            return intake.open_catalog(self.catalog_uri)\n        else:\n            fs = fsspec.filesystem(\"memory\")\n            fs_map = fs.get_mapper()\n            fs_map[\"/temp.yaml\"] = self.catalog_yaml.encode(\"utf-8\")\n            return YAMLFileCatalog(\"temp.yaml\", fs=fs)\n\n    def _open(self) -&gt; xr.Dataset:\n        return self.catalog[self.dataset_id](**self.kwargs).to_dask()\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceIntake-attributes","title":"Attributes","text":""},{"location":"reference/source/#rompy.core.source.SourceIntake.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['intake'] = Field(default='intake', description='Model type discriminator')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceIntake.dataset_id","title":"dataset_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dataset_id: str = Field(description='The id of the dataset to read in the catalog')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceIntake.catalog_uri","title":"catalog_uri  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>catalog_uri: Optional[str | Path] = Field(default=None, description='The URI of the catalog to read from')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceIntake.catalog_yaml","title":"catalog_yaml  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>catalog_yaml: Optional[str] = Field(default=None, description='The YAML string of the catalog to read from')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceIntake.kwargs","title":"kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kwargs: dict = Field(default={}, description='Keyword arguments to define intake dataset parameters')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceIntake.catalog","title":"catalog  <code>property</code>","text":"<pre><code>catalog: Catalog\n</code></pre> <p>The intake catalog instance.</p>"},{"location":"reference/source/#rompy.core.source.SourceIntake-functions","title":"Functions","text":""},{"location":"reference/source/#rompy.core.source.SourceIntake.check_catalog","title":"check_catalog","text":"<pre><code>check_catalog() -&gt; SourceIntake\n</code></pre> Source code in <code>rompy/core/source.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_catalog(self) -&gt; \"SourceIntake\":\n    if self.catalog_uri is None and self.catalog_yaml is None:\n        raise ValueError(\"Either catalog_uri or catalog_yaml must be provided\")\n    elif self.catalog_uri is not None and self.catalog_yaml is not None:\n        raise ValueError(\"Only one of catalog_uri or catalog_yaml can be provided\")\n    return self\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceDatamesh","title":"SourceDatamesh","text":"<p>               Bases: <code>SourceBase</code></p> <p>Source dataset from Datamesh.</p> <p>Datamesh documentation: https://docs.oceanum.io/datamesh/index.html</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceDatamesh(SourceBase):\n    \"\"\"Source dataset from Datamesh.\n\n    Datamesh documentation: https://docs.oceanum.io/datamesh/index.html\n\n    \"\"\"\n\n    model_type: Literal[\"datamesh\"] = Field(\n        default=\"datamesh\",\n        description=\"Model type discriminator\",\n    )\n    datasource: str = Field(\n        description=\"The id of the datasource on Datamesh\",\n    )\n    token: Optional[str] = Field(\n        default=None,\n        description=\"Datamesh API token, taken from the environment if not provided\",\n    )\n    kwargs: dict = Field(\n        default={},\n        description=\"Keyword arguments to pass to `oceanum.datamesh.Connector`\",\n    )\n\n    _query_cache: dict = PrivateAttr(default_factory=dict)\n\n    def __str__(self) -&gt; str:\n        return f\"SourceDatamesh(datasource={self.datasource})\"\n\n    @cached_property\n    def connector(self) -&gt; Connector:\n        \"\"\"The Datamesh connector instance.\"\"\"\n        return Connector(token=self.token, **self.kwargs)\n\n    @cached_property\n    def coordinates(self) -&gt; xr.Dataset:\n        \"\"\"Return the coordinates of the datasource.\"\"\"\n        return self._open(variables=[], geofilter=None, timefilter=None).coords\n\n    def _geofilter(self, filters: Filter, coords: DatasetCoords) -&gt; dict:\n        \"\"\"The Datamesh geofilter.\"\"\"\n        xslice = filters.crop.get(coords.x)\n        yslice = filters.crop.get(coords.y)\n        if xslice is None or yslice is None:\n            logger.warning(\n                f\"No slices found for x={coords.x} and/or y={coords.y} in the crop \"\n                f\"filter {filters.crop}, cannot define a geofilter for querying\"\n            )\n            return None\n\n        x0 = min(xslice.start, xslice.stop)\n        x1 = max(xslice.start, xslice.stop)\n        y0 = min(yslice.start, yslice.stop)\n        y1 = max(yslice.start, yslice.stop)\n        return dict(type=\"bbox\", geom=[x0, y0, x1, y1])\n\n    def _timefilter(self, filters: Filter, coords: DatasetCoords) -&gt; dict:\n        \"\"\"The Datamesh timefilter.\"\"\"\n        tslice = filters.crop.get(coords.t)\n        if tslice is None:\n            logger.info(\n                f\"No time slice found in the crop filter {filters.crop}, \"\n                \"cannot define a timefilter for querying datamesh\"\n            )\n            return None\n        return dict(type=\"range\", times=[tslice.start, tslice.stop])\n\n    def _open(self, variables: list, geofilter: dict, timefilter: dict) -&gt; xr.Dataset:\n        query = dict(\n            datasource=self.datasource,\n            variables=variables,\n            geofilter=geofilter,\n            timefilter=timefilter,\n        )\n        query_key = dict_to_key(query)\n        if query_key in self._query_cache:\n            logger.info(\"Using cached Datamesh query result\")\n        else:\n            logger.info(f\"Querying Datamesh datasource '{self.datasource}'\")\n            logger.debug(f\"Datamesh query: {query}\")\n            self._query_cache[query_key] = self.connector.query(query)\n        return self._query_cache[query_key]\n\n    def open(\n        self, filters: Filter, coords: DatasetCoords, variables: list = []\n    ) -&gt; xr.Dataset:\n        \"\"\"Returns the filtered dataset object.\n\n        This method is overriden from the base class because the crop filters need to\n        be converted to a geofilter and timefilter for querying Datamesh.\n\n        \"\"\"\n        ds = self._open(\n            variables=variables,\n            geofilter=self._geofilter(filters, coords),\n            timefilter=self._timefilter(filters, coords),\n        )\n        if filters:\n            ds = filters(ds)\n        return ds\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceDatamesh-attributes","title":"Attributes","text":""},{"location":"reference/source/#rompy.core.source.SourceDatamesh.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['datamesh'] = Field(default='datamesh', description='Model type discriminator')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceDatamesh.datasource","title":"datasource  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>datasource: str = Field(description='The id of the datasource on Datamesh')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceDatamesh.token","title":"token  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>token: Optional[str] = Field(default=None, description='Datamesh API token, taken from the environment if not provided')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceDatamesh.kwargs","title":"kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kwargs: dict = Field(default={}, description='Keyword arguments to pass to `oceanum.datamesh.Connector`')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceDatamesh.connector","title":"connector  <code>cached</code> <code>property</code>","text":"<pre><code>connector: Connector\n</code></pre> <p>The Datamesh connector instance.</p>"},{"location":"reference/source/#rompy.core.source.SourceDatamesh.coordinates","title":"coordinates  <code>cached</code> <code>property</code>","text":"<pre><code>coordinates: Dataset\n</code></pre> <p>Return the coordinates of the datasource.</p>"},{"location":"reference/source/#rompy.core.source.SourceDatamesh-functions","title":"Functions","text":""},{"location":"reference/source/#rompy.core.source.SourceDatamesh.open","title":"open","text":"<pre><code>open(filters: Filter, coords: DatasetCoords, variables: list = []) -&gt; Dataset\n</code></pre> <p>Returns the filtered dataset object.</p> <p>This method is overriden from the base class because the crop filters need to be converted to a geofilter and timefilter for querying Datamesh.</p> Source code in <code>rompy/core/source.py</code> <pre><code>def open(\n    self, filters: Filter, coords: DatasetCoords, variables: list = []\n) -&gt; xr.Dataset:\n    \"\"\"Returns the filtered dataset object.\n\n    This method is overriden from the base class because the crop filters need to\n    be converted to a geofilter and timefilter for querying Datamesh.\n\n    \"\"\"\n    ds = self._open(\n        variables=variables,\n        geofilter=self._geofilter(filters, coords),\n        timefilter=self._timefilter(filters, coords),\n    )\n    if filters:\n        ds = filters(ds)\n    return ds\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceWavespectra","title":"SourceWavespectra","text":"<p>               Bases: <code>SourceBase</code></p> <p>Wavespectra dataset from wavespectra reader.</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceWavespectra(SourceBase):\n    \"\"\"Wavespectra dataset from wavespectra reader.\"\"\"\n\n    model_type: Literal[\"wavespectra\"] = Field(\n        default=\"wavespectra\",\n        description=\"Model type discriminator\",\n    )\n    uri: str | Path = Field(description=\"Path to the dataset\")\n    reader: str = Field(\n        description=\"Name of the wavespectra reader to use, e.g., read_swan\",\n    )\n    kwargs: dict = Field(\n        default={},\n        description=\"Keyword arguments to pass to the wavespectra reader\",\n    )\n\n    def __str__(self) -&gt; str:\n        return f\"SourceWavespectra(uri={self.uri}, reader={self.reader})\"\n\n    def _open(self):\n        return getattr(wavespectra, self.reader)(self.uri, **self.kwargs)\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceWavespectra-attributes","title":"Attributes","text":""},{"location":"reference/source/#rompy.core.source.SourceWavespectra.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['wavespectra'] = Field(default='wavespectra', description='Model type discriminator')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceWavespectra.uri","title":"uri  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uri: str | Path = Field(description='Path to the dataset')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceWavespectra.reader","title":"reader  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reader: str = Field(description='Name of the wavespectra reader to use, e.g., read_swan')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceWavespectra.kwargs","title":"kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kwargs: dict = Field(default={}, description='Keyword arguments to pass to the wavespectra reader')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceTimeseriesCSV","title":"SourceTimeseriesCSV","text":"<p>               Bases: <code>SourceBase</code></p> <p>Timeseries source class from CSV file.</p> <p>This class should return a timeseries from a CSV file. The dataset variables are defined from the column headers, therefore the appropriate read_csv kwargs must be passed to allow defining the columns. The time index is defined from column name identified by the tcol field.</p> Source code in <code>rompy/core/source.py</code> <pre><code>class SourceTimeseriesCSV(SourceBase):\n    \"\"\"Timeseries source class from CSV file.\n\n    This class should return a timeseries from a CSV file. The dataset variables are\n    defined from the column headers, therefore the appropriate read_csv kwargs must be\n    passed to allow defining the columns. The time index is defined from column name\n    identified by the tcol field.\n\n    \"\"\"\n\n    model_type: Literal[\"csv\"] = Field(\n        default=\"csv\",\n        description=\"Model type discriminator\",\n    )\n    filename: str | Path = Field(description=\"Path to the csv file\")\n    tcol: str = Field(\n        default=\"time\",\n        description=\"Name of the column containing the time data\",\n    )\n    read_csv_kwargs: dict = Field(\n        default={},\n        description=\"Keyword arguments to pass to pandas.read_csv\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_kwargs(self) -&gt; \"SourceTimeseriesCSV\":\n        \"\"\"Validate the keyword arguments.\"\"\"\n        if \"parse_dates\" not in self.read_csv_kwargs:\n            self.read_csv_kwargs[\"parse_dates\"] = [self.tcol]\n        if \"index_col\" not in self.read_csv_kwargs:\n            self.read_csv_kwargs[\"index_col\"] = self.tcol\n        return self\n\n    def _open_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Read the data from the csv file.\"\"\"\n        return pd.read_csv(self.filename, **self.read_csv_kwargs)\n\n    def _open(self) -&gt; xr.Dataset:\n        \"\"\"Interpolate the xyz data onto a regular grid.\"\"\"\n        df = self._open_dataframe()\n        ds = xr.Dataset.from_dataframe(df).rename({self.tcol: \"time\"})\n        return ds\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceTimeseriesCSV-attributes","title":"Attributes","text":""},{"location":"reference/source/#rompy.core.source.SourceTimeseriesCSV.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: Literal['csv'] = Field(default='csv', description='Model type discriminator')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceTimeseriesCSV.filename","title":"filename  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filename: str | Path = Field(description='Path to the csv file')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceTimeseriesCSV.tcol","title":"tcol  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcol: str = Field(default='time', description='Name of the column containing the time data')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceTimeseriesCSV.read_csv_kwargs","title":"read_csv_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>read_csv_kwargs: dict = Field(default={}, description='Keyword arguments to pass to pandas.read_csv')\n</code></pre>"},{"location":"reference/source/#rompy.core.source.SourceTimeseriesCSV-functions","title":"Functions","text":""},{"location":"reference/source/#rompy.core.source.SourceTimeseriesCSV.validate_kwargs","title":"validate_kwargs","text":"<pre><code>validate_kwargs() -&gt; SourceTimeseriesCSV\n</code></pre> <p>Validate the keyword arguments.</p> Source code in <code>rompy/core/source.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_kwargs(self) -&gt; \"SourceTimeseriesCSV\":\n    \"\"\"Validate the keyword arguments.\"\"\"\n    if \"parse_dates\" not in self.read_csv_kwargs:\n        self.read_csv_kwargs[\"parse_dates\"] = [self.tcol]\n    if \"index_col\" not in self.read_csv_kwargs:\n        self.read_csv_kwargs[\"index_col\"] = self.tcol\n    return self\n</code></pre>"},{"location":"reference/source/#rompy.core.source-functions","title":"Functions","text":""},{"location":"reference/source/#rompy.core.source.render_datetimes","title":"render_datetimes","text":"<pre><code>render_datetimes(dict_obj: dict) -&gt; dict\n</code></pre> <p>Convert datetime objects in a dictionary to ISO format strings.</p>"},{"location":"reference/source/#rompy.core.source.render_datetimes--parameters","title":"Parameters","text":"<p>dict_obj : dict     The dictionary to process.</p>"},{"location":"reference/source/#rompy.core.source.render_datetimes--returns","title":"Returns","text":"<p>dict     The processed dictionary with datetime objects converted to strings.</p> Source code in <code>rompy/core/source.py</code> <pre><code>def render_datetimes(dict_obj: dict) -&gt; dict:\n    \"\"\"Convert datetime objects in a dictionary to ISO format strings.\n\n    Parameters\n    ----------\n    dict_obj : dict\n        The dictionary to process.\n\n    Returns\n    -------\n    dict\n        The processed dictionary with datetime objects converted to strings.\n    \"\"\"\n    for key, value in dict_obj.items():\n        if isinstance(value, dict):\n            dict_obj[key] = render_datetimes(value)\n        if isinstance(value, list):\n            for i, item in enumerate(value):\n                if isinstance(item, dict):\n                    value[i] = render_datetimes(item)\n                elif hasattr(item, \"isoformat\"):\n                    value[i] = item.isoformat()\n        elif hasattr(value, \"isoformat\"):\n            dict_obj[key] = value.isoformat()\n    return dict_obj\n</code></pre>"},{"location":"reference/source/#rompy.core.source.dict_to_key","title":"dict_to_key","text":"<pre><code>dict_to_key(data: dict) -&gt; str\n</code></pre> <p>Convert a dictionary to a hashable key string.</p> <p>This function converts a dictionary uniquely to a string which can be used as a hashable key.</p>"},{"location":"reference/source/#rompy.core.source.dict_to_key--parameters","title":"Parameters","text":"<p>data : dict     The dictionary to convert.</p>"},{"location":"reference/source/#rompy.core.source.dict_to_key--returns","title":"Returns","text":"<p>str     A string representation of the sorted dictionary items.</p> Source code in <code>rompy/core/source.py</code> <pre><code>def dict_to_key(data: dict) -&gt; str:\n    \"\"\"Convert a dictionary to a hashable key string.\n\n    This function converts a dictionary uniquely to a string\n    which can be used as a hashable key.\n\n    Parameters\n    ----------\n    data : dict\n        The dictionary to convert.\n\n    Returns\n    -------\n    str\n        A string representation of the sorted dictionary items.\n    \"\"\"\n    data = render_datetimes(data)\n    unique_str = json.dumps(data, sort_keys=True)\n    hash_str = hashlib.sha256(unique_str.encode()).hexdigest()\n    return hash_str\n</code></pre>"}]}