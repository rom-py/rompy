"""
SLURM backend for running models.

This module provides a SLURM-based execution backend for rompy models.
"""

import logging
import os
import subprocess
import tempfile
import time
from pathlib import Path
from typing import TYPE_CHECKING, Dict, List, Optional, Union

if TYPE_CHECKING:
    from rompy.backends import SlurmConfig

logger = logging.getLogger(__name__)


class SlurmRunBackend:
    """Execute models on SLURM clusters.

    This backend submits model runs to a SLURM-managed HPC cluster
    for execution.
    """

    def run(
        self, model_run, config: "SlurmConfig", workspace_dir: Optional[str] = None
    ) -> bool:
        """Submit model run to SLURM queue.

        Args:
            model_run: The ModelRun instance to execute
            config: SlurmConfig instance with execution parameters
            workspace_dir: Path to the generated workspace directory (if None, will generate)

        Returns:
            True if execution was successful, False otherwise
        """
        logger.debug(f"Using SlurmConfig: nodes={config.nodes}, ntasks={config.ntasks}")

        # Use provided workspace or generate if not provided (for backwards compatibility)
        if workspace_dir is None:
            logger.warning(
                "No workspace_dir provided, generating files (this may cause double generation in pipeline)"
            )
            staging_dir = model_run.generate()
            logger.info(f"Model inputs generated in: {staging_dir}")
        else:
            logger.info(f"Using provided workspace directory: {workspace_dir}")
            staging_dir = workspace_dir

        try:
            # Create and submit SLURM job script
            job_script = self._create_job_script(model_run, config, staging_dir)
            job_id = self._submit_job(job_script)

            if job_id:
                logger.info(f"SLURM job submitted successfully with ID: {job_id}")
                return self._wait_for_completion(job_id, config)
            else:
                logger.error("Failed to submit SLURM job")
                return False

        except Exception as e:
            logger.exception(f"SLURM execution failed: {e}")
            return False

    def _create_job_script(
        self, model_run, config: "SlurmConfig", staging_dir: str
    ) -> str:
        """Create SLURM job script.

        Args:
            model_run: The ModelRun instance
            config: SlurmConfig with execution parameters
            staging_dir: Path to workspace directory

        Returns:
            Path to the created job script
        """
        # Determine the working directory for the job
        work_dir = config.working_dir if config.working_dir else staging_dir
        
        # Create the job script content
        script_lines = [
            "#!/bin/bash",
            "# SLURM job script generated by rompy",
        ]

        # Add SBATCH directives from configuration
        if config.job_name:
            script_lines.append(f"#SBATCH --job-name={config.job_name}")
        
        if config.output_file:
            script_lines.append(f"#SBATCH --output={config.output_file}")
        else:
            # Default output file with job ID
            script_lines.append(f"#SBATCH --output={work_dir}/slurm-%j.out")
        
        if config.error_file:
            script_lines.append(f"#SBATCH --error={config.error_file}")
        else:
            # Default error file with job ID
            script_lines.append(f"#SBATCH --error={work_dir}/slurm-%j.err")

        if config.queue:
            script_lines.append(f"#SBATCH --partition={config.queue}")
        
        script_lines.append(f"#SBATCH --nodes={config.nodes}")
        script_lines.append(f"#SBATCH --ntasks={config.ntasks}")
        script_lines.append(f"#SBATCH --cpus-per-task={config.cpus_per_task}")
        script_lines.append(f"#SBATCH --time={config.time_limit}")
        
        if config.account:
            script_lines.append(f"#SBATCH --account={config.account}")
        
        if config.qos:
            script_lines.append(f"#SBATCH --qos={config.qos}")
        
        if config.reservation:
            script_lines.append(f"#SBATCH --reservation={config.reservation}")
        
        if config.mail_type and config.mail_user:
            script_lines.append(f"#SBATCH --mail-type={config.mail_type}")
            script_lines.append(f"#SBATCH --mail-user={config.mail_user}")

        # Add additional options
        for option in config.additional_options:
            script_lines.append(f"#SBATCH {option}")

        script_lines.extend([
            "",
            "# Change to working directory",
            f"cd {work_dir}",
            "",
            "# Set environment variables",
        ])

        # Add environment variables
        for key, value in config.env_vars.items():
            script_lines.append(f"export {key}={value}")

        # Add the actual command to run the model\n        # First, check if there's a specific command in config, otherwise use the model's run method\n        if hasattr(config, 'command') and config.command:\n            script_lines.extend([\n                \"\",\n                \"# Execute custom command\",\n                config.command,\n            ])\n        else:\n            script_lines.extend([\n                \"\",\n                \"# Execute model using model_run.config.run() method\",\n                \"python -c \\\"\",\n                \"import sys\",\n                \"import os\",\n                \"sys.path.insert(0, os.getcwd())\",\n                \"from rompy.model import ModelRun\",\n                f\"model_run = ModelRun.from_dict({model_run.model_dump()})\",\n                \"model_run.config.run(model_run)\",\n                \"\\\"\",\n            ])

        # Create temporary job script file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False) as f:
            f.write('\n'.join(script_lines))
            script_path = f.name

        logger.debug(f"SLURM job script created at: {script_path}")
        logger.debug(f"Job script content:\n{'\n'.join(script_lines)}")

        return script_path

    def _submit_job(self, job_script: str) -> Optional[str]:
        """Submit job to SLURM.

        Args:
            job_script: Path to the job script to submit

        Returns:
            Job ID if submission successful, None otherwise
        """
        try:
            # Submit the job using sbatch
            result = subprocess.run(
                ["sbatch", job_script],
                capture_output=True,
                text=True,
                check=True
            )

            # Extract job ID from sbatch output (format: "Submitted batch job <ID>")
            output = result.stdout.strip()
            if "Submitted batch job" in output:
                job_id = output.split()[-1]
                logger.info(f"Submitted SLURM job with ID: {job_id}")
                return job_id
            else:
                logger.error(f"Unexpected sbatch output format: {output}")
                return None

        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to submit SLURM job: {e.stderr}")
            return None
        except Exception as e:
            logger.error(f"Error submitting SLURM job: {e}")
            return None
        finally:
            # Clean up the temporary job script
            try:
                os.remove(job_script)
                logger.debug(f"Cleaned up temporary job script: {job_script}")
            except OSError:
                logger.warning(f"Could not remove temporary job script: {job_script}")

    def _wait_for_completion(self, job_id: str, config: "SlurmConfig") -> bool:
        """Wait for job completion.

        Args:
            job_id: SLURM job ID to monitor
            config: SlurmConfig with timeout parameters

        Returns:
            True if job completed successfully, False otherwise
        """
        logger.info(f"Waiting for SLURM job {job_id} to complete...")

        # Terminal states that indicate job completion (successful or failed)
        terminal_states = {'CD', 'CA', 'F', 'TO', 'NF', 'OOM', 'BF', 'DL', 'PR'}

        # Start time for timeout check
        start_time = time.time()

        while True:
            # Check if we've exceeded the timeout
            elapsed_time = time.time() - start_time
            if elapsed_time > config.timeout:
                logger.error(f"Timeout waiting for job {job_id} after {config.timeout} seconds")
                
                # Try to cancel the job
                try:
                    subprocess.run(['scancel', job_id], check=True, capture_output=True)
                    logger.info(f"Cancelled job {job_id} due to timeout")
                except subprocess.CalledProcessError:
                    logger.warning(f"Could not cancel job {job_id} due to timeout")
                
                return False

            # Get job status
            try:
                result = subprocess.run(
                    ['squeue', '-j', job_id, '-h', '-o', '%T'],
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                state = result.stdout.strip()
                
                if not state:  # If job is not found, it may have completed and been purged
                    logger.info(f"Job {job_id} not found in queue - likely completed")
                    return True  # Assume successful completion if not in queue
                
                if state in terminal_states:
                    if state == 'CD':  # Completed
                        logger.info(f"SLURM job {job_id} completed successfully")
                        return True
                    elif state == 'CA':  # Cancelled
                        logger.warning(f"SLURM job {job_id} was cancelled")
                        return False
                    elif state == 'F':  # Failed
                        logger.error(f"SLURM job {job_id} failed")
                        return False
                    elif state == 'TO':  # Timeout
                        logger.error(f"SLURM job {job_id} timed out")
                        return False
                    else:
                        logger.error(f"SLURM job {job_id} ended with state: {state}")
                        return False

                # Job is still running or pending, wait before checking again
                logger.debug(f"Job {job_id} still in state: {state}, waiting...")
                time.sleep(30)  # Wait 30 seconds before next check

            except subprocess.CalledProcessError as e:
                logger.error(f"Error checking job status for {job_id}: {e.stderr}")
                # If we can't check the status, we consider it a failure
                return False
            except Exception as e:
                logger.error(f"Unexpected error while monitoring job {job_id}: {e}")
                return False