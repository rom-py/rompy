"""
SLURM backend for running models.

This module provides a SLURM-based execution backend for rompy models.
"""

import logging
import os
import subprocess
import tempfile
import time
from pathlib import Path
from typing import TYPE_CHECKING, Dict, List, Optional, Union

if TYPE_CHECKING:
    from rompy.backends import SlurmConfig

logger = logging.getLogger(__name__)


class SlurmRunBackend:
    """Execute models on SLURM clusters.

    This backend submits model runs to a SLURM-managed HPC cluster
    for execution.
    """

    def run(
        self, model_run, config: "SlurmConfig", workspace_dir: Optional[str] = None
    ) -> bool:
        """Submit model run to SLURM queue.

        Args:
            model_run: The ModelRun instance to execute
            config: SlurmConfig instance with execution parameters
            workspace_dir: Path to the generated workspace directory (if None, will generate)

        Returns:
            True if execution was successful, False otherwise
        """
        logger.debug(f"Using SlurmConfig: nodes={config.nodes}, ntasks={config.ntasks}")

        # Use provided workspace or generate if not provided (for backwards compatibility)
        if workspace_dir is None:
            logger.warning(
                "No workspace_dir provided, generating files (this may cause double generation in pipeline)"
            )
            staging_dir = model_run.generate()
            logger.info(f"Model inputs generated in: {staging_dir}")
        else:
            logger.info(f"Using provided workspace directory: {workspace_dir}")
            staging_dir = workspace_dir

        try:
            # Create and submit SLURM job script
            job_script = self._create_job_script(model_run, config, staging_dir)
            job_id = self._submit_job(job_script)

            if job_id:
                logger.info(f"SLURM job submitted successfully with ID: {job_id}")
                return self._wait_for_completion(job_id, config)
            else:
                logger.error("Failed to submit SLURM job")
                return False

        except Exception as e:
            logger.exception(f"SLURM execution failed: {e}")
            return False

    def _create_job_script(
        self, model_run, config: "SlurmConfig", staging_dir: str
    ) -> str:
        """Create SLURM job script.

        Args:
            model_run: The ModelRun instance
            config: SlurmConfig with execution parameters
            staging_dir: Path to workspace directory

        Returns:
            Path to the created job script
        """
        # Determine the working directory for the job
        work_dir = config.working_dir if config.working_dir else staging_dir
        
        # Create the job script content
        script_lines = [
            "#!/bin/bash",
            "# SLURM job script generated by rompy",
        ]

        # Add SBATCH directives from configuration
        if config.job_name:
            script_lines.append(f"#SBATCH --job-name={config.job_name}")
        
        if config.output_file:
            script_lines.append(f"#SBATCH --output={config.output_file}")
        else:
            # Default output file with job ID
            script_lines.append(f"#SBATCH --output={work_dir}/slurm-%j.out")
        
        if config.error_file:
            script_lines.append(f"#SBATCH --error={config.error_file}")
        else:
            # Default error file with job ID
            script_lines.append(f"#SBATCH --error={work_dir}/slurm-%j.err")

        if config.queue:
            script_lines.append(f"#SBATCH --partition={config.queue}")
        
        script_lines.append(f"#SBATCH --nodes={config.nodes}")
        script_lines.append(f"#SBATCH --ntasks={config.ntasks}")
        script_lines.append(f"#SBATCH --cpus-per-task={config.cpus_per_task}")
        script_lines.append(f"#SBATCH --time={config.time_limit}")
        
        if config.account:
            script_lines.append(f"#SBATCH --account={config.account}")
        
        if config.qos:
            script_lines.append(f"#SBATCH --qos={config.qos}")
        
        if config.reservation:
            script_lines.append(f"#SBATCH --reservation={config.reservation}")
        
        if config.mail_type and config.mail_user:
            script_lines.append(f"#SBATCH --mail-type={config.mail_type}")
            script_lines.append(f"#SBATCH --mail-user={config.mail_user}")

        # Add additional options
        for option in config.additional_options:
            script_lines.append(f"#SBATCH {option}")

        script_lines.extend([
            "",
            "# Change to working directory",
            f"cd {work_dir}",
            "",
            "# Set environment variables",
        ])

        # Add environment variables
        for key, value in config.env_vars.items():
            script_lines.append(f"export {key}={value}")

        # Add the actual command to run the model
        script_lines.extend([
            "",
            "# Execute command in the workspace",
            config.command,
        ])

        # Create temporary job script file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False) as f:
            f.write('\n'.join(script_lines))
            script_path = f.name

        logger.debug(f"SLURM job script created at: {script_path}")
        logger.debug("Job script content:\n%s", "\n".join(script_lines))

        return script_path

    def _submit_job(self, job_script: str) -> Optional[str]:
        """Submit job to SLURM.

        Args:
            job_script: Path to the job script to submit

        Returns:
            Job ID if submission successful, None otherwise
        """
        try:
            # Check if sbatch command is available
            result = subprocess.run(
                ["which", "sbatch"],
                capture_output=True,
                text=True
            )
            if result.returncode != 0 or not result.stdout.strip():
                logger.error("sbatch command not found. SLURM may not be installed or in PATH.")
                return None

            # Check if SLURM controller is responsive
            result = subprocess.run(
                ["scontrol", "--help"],
                capture_output=True,
                text=True,
                timeout=10  # Don't wait too long
            )
            if result.returncode != 0:
                logger.error("SLURM controller is not responsive. scontrol command failed.")
                return None

            # Submit the job using sbatch
            result = subprocess.run(
                ["sbatch", job_script],
                capture_output=True,
                text=True,
                check=True
            )

            # Extract job ID from sbatch output (format: "Submitted batch job <ID>")
            output = result.stdout.strip()
            if "Submitted batch job" in output:
                job_id = output.split()[-1]
                logger.info(f"Submitted SLURM job with ID: {job_id}")
                return job_id
            else:
                logger.error(f"Unexpected sbatch output format: {output}")
                return None

        except subprocess.TimeoutExpired:
            logger.error("SLURM controller check timed out. SLURM may not be properly configured.")
            return None
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to submit SLURM job: {e.stderr}")
            return None
        except Exception as e:
            logger.error(f"Error submitting SLURM job: {e}")
            return None
        finally:
            # Clean up the temporary job script
            try:
                os.remove(job_script)
                logger.debug(f"Cleaned up temporary job script: {job_script}")
            except OSError:
                logger.warning(f"Could not remove temporary job script: {job_script}")

    def _wait_for_completion(self, job_id: str, config: "SlurmConfig") -> bool:
        """Wait for job completion.

        Args:
            job_id: SLURM job ID to monitor
            config: SlurmConfig with timeout parameters

        Returns:
            True if job completed successfully, False otherwise
        """
        logger.info(f"Waiting for SLURM job {job_id} to complete...")

        # Terminal states that indicate job completion (successful or failed)
        # Using SLURM job states: https://slurm.schedmd.com/squeue.html#SECTION_JOB-STATE-CODES
        terminal_states = {'BOOT_FAIL', 'CANCELLED', 'COMPLETED', 'DEADLINE', 'FAILED', 
                          'NODE_FAIL', 'OUT_OF_MEMORY', 'PREEMPTED', 'TIMEOUT'}

        # Start time for timeout check
        start_time = time.time()

        while True:
            # Check if we've exceeded the timeout
            elapsed_time = time.time() - start_time
            if elapsed_time > config.timeout:
                logger.error(f"Timeout waiting for job {job_id} after {config.timeout} seconds")
                
                # Let SLURM handle job cancellation according to its configured policies
                return False

            # Get job status using scontrol for more reliable detection
            try:
                result = subprocess.run(
                    ['scontrol', 'show', 'job', job_id],
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                # Parse the output to get the job state
                output = result.stdout
                if 'JobState=' in output:
                    state = output.split('JobState=')[1].split()[0].split('_')[0]  # Extract state like 'RUNNING', 'COMPLETED', etc.
                else:
                    # If JobState is not found, we might have an issue with parsing
                    logger.warning(f"Could not determine job state from output for job {job_id}")
                    state = None
                
                if state is None:  # If job state can't be determined, check if job is not found
                    if 'slurm_load_jobs error' in output or 'Invalid job id' in output.lower():
                        logger.info(f"Job {job_id} not found - likely completed")
                        return True  # Assume successful completion if job ID is invalid
                
                if state in terminal_states:
                    if state == 'COMPLETED':  # Completed successfully
                        logger.info(f"SLURM job {job_id} completed successfully")
                        return True
                    elif state == 'CANCELLED':  # Cancelled
                        logger.warning(f"SLURM job {job_id} was cancelled")
                        return False
                    elif state == 'FAILED':  # Failed
                        logger.error(f"SLURM job {job_id} failed")
                        return False
                    elif state == 'TIMEOUT':  # Timeout
                        logger.error(f"SLURM job {job_id} timed out")
                        return False
                    elif state == 'BOOT_FAIL':  # Boot failure
                        logger.error(f"SLURM job {job_id} failed to boot")
                        return False
                    elif state == 'NODE_FAIL':  # Node failure
                        logger.error(f"SLURM job {job_id} failed due to node failure")
                        return False
                    elif state == 'OUT_OF_MEMORY':  # Out of memory
                        logger.error(f"SLURM job {job_id} ran out of memory")
                        return False
                    elif state == 'PREEMPTED':  # Preempted
                        logger.error(f"SLURM job {job_id} was preempted")
                        return False
                    else:
                        logger.error(f"SLURM job {job_id} ended with state: {state}")
                        return False

                # Job is still running or pending, wait before checking again
                logger.debug(f"Job {job_id} still in state: {state}, waiting...")
                time.sleep(30)  # Wait 30 seconds before next check

            except subprocess.CalledProcessError as e:
                logger.error(f"Error checking job status for {job_id}: {e.stderr}")
                # If we can't check the status, we consider it a failure
                return False
            except Exception as e:
                logger.error(f"Unexpected error while monitoring job {job_id}: {e}")
                return False
